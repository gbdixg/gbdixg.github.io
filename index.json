[{"content":"Read-IMELog - A PowerShell script to read Intune Management Extension (IME) logs\nBackground The Intune Management Extension is a Windows client component responsible for running Intune scripts and installing Win32 apps. It creates log files in the following folder:\nC:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\nIME Log entries are in the CMTrace format used by System Center Configuration Manager e.g.\n\u0026lt;![LOG[[Win32App] Checking ESP status and phase for sessionId: 0]LOG]!\u0026gt;\u0026lt;time=\u0026#34;23:05:44.2390147\u0026#34; date=\u0026#34;4-16-2023\u0026#34; component=\u0026#34;IntuneManagementExtension\u0026#34; context=\u0026#34;\u0026#34; type=\u0026#34;1\u0026#34; thread=\u0026#34;65\u0026#34; file=\u0026#34;\u0026#34;\u0026gt; \u0026lt;![LOG[[Proxy Poller] Processing session id 2 starts]LOG]!\u0026gt;\u0026lt;time=\u0026#34;23:05:47.8411669\u0026#34; date=\u0026#34;4-16-2023\u0026#34; component=\u0026#34;IntuneManagementExtension\u0026#34; context=\u0026#34;\u0026#34; type=\u0026#34;1\u0026#34; thread=\u0026#34;25\u0026#34; file=\u0026#34;\u0026#34;\u0026gt; They are best viewed using the CMTrace tool, but this won\u0026rsquo;t be readily available for a company that doesn\u0026rsquo;t use System Center Configuration Manager. There is no official download for cloud-only businesses using Intune.\nRead-IMELog is a PowerShell script that converts IME logs to PowerShell objects, allowing flexible filtering and sorting. It can also be used for Config Manager logs.\nExample usage 1 - filter on message text $IMELog = \u0026#39;C:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\\IntuneManagementExtension.log\u0026#39; Read-IMELog -Path $IMELog | where-object{$_.message -like \u0026#34;*ProcessDetection*\u0026#34;} | Select-object -first 2 Example usage 2 - read all IME logs $IMELogFolder = \u0026#39;C:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\u0026#39; Get-Childitem -path $IMELogFolder | Read-IMELog | Out-Gridview Read-IMELog.ps1 See below for the PowerShell script:\n","permalink":"https://write-verbose.com/2023/04/17/readimelog/","summary":"\u003cp\u003eRead-IMELog - A PowerShell script to read Intune Management Extension (IME) logs\u003c/p\u003e","title":"Read Intune Logs with PowerShell (CMTrace format)"},{"content":"More things I learned creating a GUI tool with PowerShell and WPF.\nThe snippets in this article are based on the Show-Win32AppUI tool available on GitHub.\nPart2 - Creating a multi-page WPF app in PowerShell (See part1 of this series for information on using PowerShell Runspaces with WPF).\nWPF uses .XAML text files to define the layout and properties of supported controls such as Textboxes, Buttons, Status bars etc. XAML is a form is XML, with opening and closing tags that create a hierarchy of controls. A child control such as a Combobox is displayed within a parent container such as a Window.\nVery simple apps may be able to fit controls in a single Window, but a wizard-driven interface will usually need multiple Pages to guide the user through selections. There are many ways to create a multi-page WPF app. The method below is the one I prefer:\nMain window and child pages The starting point is a WPF Window control defined in it\u0026rsquo;s own .XAML file. A Window is a container for other WPF controls, most importantly in this case, a Frame that can load Pages. The main Window can show a header, footer and sidebar that is always visible while the Page within the Frame control changes as the user navigates the app.\nEach page can be defined in a separate .XAML file. A Page is also a container so each page can host controls such as Textboxes, Textblocks, Comboboxes etc. The Frame content is updated at runtime to show a new Page when an event occurs such as clicking navigation buttons.\nA Frame does have a built-in navigation control, but it isn\u0026rsquo;t pretty. I turn it off and use Button controls in the main Window.\n\u0026lt;!--Partial XAML for a Main Window with a Frame and navigation Buttons--\u0026gt; \u0026lt;Window xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Height=\u0026#34;850\u0026#34; Width=\u0026#34;450\u0026#34; ...\u0026gt; \u0026lt;!--Pages will be loaded into this frame at runtime--\u0026gt; \u0026lt;Frame x:Name=\u0026#34;frame_Pages\u0026#34; Grid.Row=\u0026#34;2\u0026#34; Grid.Column=\u0026#34;1\u0026#34; Grid.ColumnSpan=\u0026#34;3\u0026#34; NavigationUIVisibility=\u0026#34;Hidden\u0026#34; # Built-in navigation disabled Margin=\u0026#34;0,10,0,0\u0026#34; /\u0026gt; \u0026lt;!--Navigation Buttons below the pages--\u0026gt; \u0026lt;Button x:Name = \u0026#34;Btn_Previous\u0026#34; Content=\u0026#34;Previous\u0026#34; Height=\u0026#34;35\u0026#34; Width=\u0026#34;60\u0026#34; Grid.Row=\u0026#34;3\u0026#34; Grid.Column=\u0026#34;2\u0026#34; BorderThickness=\u0026#34;0\u0026#34;/\u0026gt; \u0026lt;Button x:Name = \u0026#34;Btn_Next\u0026#34; Content=\u0026#34;Next\u0026#34; Margin=\u0026#34;3,0,0,0\u0026#34; Height=\u0026#34;35\u0026#34; Width=\u0026#34;60\u0026#34; Grid.Row=\u0026#34;3\u0026#34; Grid.Column=\u0026#34;3\u0026#34; BorderThickness=\u0026#34;0\u0026#34;/\u0026gt; See the Show-Win32UI tool for an example of separate XAML files per Page and main Window.\nLoad XAML controls into PowerShell variables A thread safe Hashtable collection allows WPF controls to be referenced in PowerShell at runtime. Grouping the controls in a collection is convenient as it simplifies passing them into Runspaces.\n### Example of reading controls from XAML files and assigning them to variables function LoadXml ($filename) { # Convert a .XAML file to an XMLDocument $XmlLoader = (New-Object System.Xml.XmlDocument) $XmlLoader.Load($filename) return $XmlLoader } # Load the XAML files $xmlMainWindow = LoadXml(\u0026#34;$PSScriptRoot\\Xaml\\MainWindow.xaml\u0026#34;) $xmlPage1 = LoadXml(\u0026#34;$PScriptRoot\\Xaml\\Page1.xaml\u0026#34;) $xmlPage2 = LoadXml(\u0026#34;$PScriptRoot\\Xaml\\Page2.xaml\u0026#34;) # Collection storing references to all named WPF controls in the UI $UIControls=[hashtable]::Synchronized(@{}) # Convert Windows and Pages to a XAML object graph $UIControls.MainWindow = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlMainWindow)) $UIControls.Page1 = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlPage1)) $UIControls.Page2 = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlPage2)) # Add each named control to the $UIControls hashtable (repeat for each Window / Page) # This allows key controls to be referenced directly at runtime, rather than through a parent-child hierarchy of Page\u0026gt;Control $XmlMainWindow.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.MainWindow.FindName($_.Name) } $xmlPage1.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.Page1.FindName($_.Name) } $xmlPage2.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.Page2.FindName($_.Name) } # Example of loading first Page into the Frame # \u0026#34;frame_Pages\u0026#34; in the name of the Frame defined in the XAML file $UIControls.frame_Pages.Content = $UIControls.Page1 # Show the user interface $UIControls.MainWindow.ShowDialog() Using WPF Styles to simplify XAML XAML styles are analogous to CSS in HTML. The XAML style can apply to all controls of a type, such as all TextBoxes, defining properties such as the colour, font, border etc. If a property needs to be changed, it only needs to be updated in one place. Its also a flexible solution because Style properties can still be over-ridden on an individual control.\nStyles can be defined in a separate XAML file and then imported on each Window / Page as a ResourceDictionary.\n\u0026lt;!--Styles.xaml defines a TextBox style called ModernTextBox--\u0026gt; \u0026lt;ResourceDictionary xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34;\u0026gt; \u0026lt;Style TargetType=\u0026#34;{x:Type TextBox}\u0026#34; x:Key=\u0026#34;ModernTextBox\u0026#34;\u0026gt; \u0026lt;Setter Property=\u0026#34;BorderThickness\u0026#34; Value=\u0026#34;0,0,0,1\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;BorderBrush\u0026#34; Value=\u0026#34;LightGray\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;Background\u0026#34; Value=\u0026#34;Transparent\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;FontSize\u0026#34; Value=\u0026#34;18\u0026#34;/\u0026gt; \u0026lt;/Style\u0026gt; \u0026lt;/ResourceDictionary\u0026gt; \u0026lt;!--MainWindow.xaml imports Styles.xml as a ResourceDictionary--\u0026gt; \u0026lt;Window xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Height=\u0026#34;850\u0026#34; Width=\u0026#34;450\u0026#34; Title=\u0026#34;Main Window\u0026#34;\u0026gt; \u0026lt;!--Import a style so it can used by controls in this Window--\u0026gt; \u0026lt;Window.Resources\u0026gt; \u0026lt;ResourceDictionary Source=\u0026#34;Styles.xaml\u0026#34;/\u0026gt; \u0026lt;/Window.Resources\u0026gt; ... \u0026lt;!--Page1.xaml imports Styles.xml as a ResourceDictionary--\u0026gt; \u0026lt;Page xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Background=\u0026#34;Transparent\u0026#34;\u0026gt; \u0026lt;!--Import a style so it can used by controls in this Page--\u0026gt; \u0026lt;Page.Resources\u0026gt; \u0026lt;ResourceDictionary Source=\u0026#34;Styles.xaml\u0026#34;/\u0026gt; \u0026lt;/Page.Resources\u0026gt; ... \u0026lt;!--MainWindow.xaml or Page1.xaml uses the style on a Textbox as shown below--\u0026gt; \u0026lt;TextBox x:Name=\u0026#34;txt_InstallArgs\u0026#34; Width=\u0026#34;330\u0026#34; Height=\u0026#34;30\u0026#34; Style=\u0026#34;{StaticResource ModernTextBox}\u0026#34;/\u0026gt; Define the WPF layout with the Grid control The Grid control overlays invisible rows and columns on a parent container such as a Window or Page. The Controls are then positioned based on the Row and Column number. It is fairly simple to design the Grid layout using a basic text editor. For a more WYSIWYG experience, use Microsoft Visual Studio to view the UI at design time.\nThere are three options for Grid Row/Column height / width:\nSize Meaning [Pixels] A fixed size \u0026ldquo;Auto\u0026rdquo; Expand to fit content \u0026ldquo;*\u0026rdquo; Fit to remaining space in Window / Page \u0026lt;!-- Example positioning a Textbox above a TextBlock using a Grid TextBox is in Grid.Row = \u0026#34;1\u0026#34; and TextBlock is in Grid.Row = \u0026#34;2\u0026#34;--\u0026gt; \u0026lt;Grid\u0026gt; \u0026lt;Grid.ColumnDefinitions\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;Auto\u0026#34;/\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;/Grid.ColumnDefinitions\u0026gt; \u0026lt;Grid.RowDefinitions\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;auto\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;auto\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;*\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;/Grid.RowDefinitions\u0026gt; \u0026lt;TextBox Width=\u0026#34;350\u0026#34; Grid.Column=\u0026#34;1\u0026#34; Grid.Row=\u0026#34;1\u0026#34; Style=\u0026#34;{StaticResource ModernTextBox}\u0026#34;/\u0026gt; \u0026lt;TextBlock Grid.Column=\u0026#34;1\u0026#34; Grid.Row=\u0026#34;2\u0026#34; Text=\u0026#34;Enter your name\u0026#34; Style=\u0026#34;{StaticResource ModernTextBlock}\u0026#34;/\u0026gt; \u0026lt;/Grid\u0026gt; Use margins to create space between controls The Margin property is available on most controls. The margin creates a buffer of blank space around a control. The margin can either be the same all round, or different for each vector - left, top, right, and bottom\n\u0026lt;!--Example setting a Margin around a control--\u0026gt; \u0026lt;!--Different margin for left, top, right, and bottom --\u0026gt; \u0026lt;ComboBox x:Name=\u0026#34;combo_Supercedence\u0026#34; Grid.Column=\u0026#34;0\u0026#34; Grid.Row=\u0026#34;11\u0026#34; Width=\u0026#34;360\u0026#34; Height=\u0026#34;24\u0026#34; Margin=\u0026#34;15,5,0,0\u0026#34;/\u0026gt; \u0026lt;!--Same margin all round --\u0026gt; \u0026lt;ComboBox x:Name=\u0026#34;combo_Supercedence\u0026#34; Grid.Column=\u0026#34;0\u0026#34; Grid.Row=\u0026#34;11\u0026#34; Width=\u0026#34;360\u0026#34; Height=\u0026#34;24\u0026#34; Margin=\u0026#34;5\u0026#34;/\u0026gt; Defining Event Handlers WPF controls support events that occur based on user input. Some events are common to most controls, such as MouseEnter and MouseLeave while other events are specific to a control type, such as SelectionChanged in a ComboBox.\nPowerShell uses an \u0026ldquo;Add_[Event Name]\u0026rdquo; syntax to define the code to run when the event fires as below. The per-control event list is available in the MS Documentation.\n# Example event handler - code will run when the Combo box selection changes $UIControls.combo_language.Add_SelectionChanged({ $Language = $UIControls.combo_language.SelectedItem }) # Example event handler - code will run when the button is left clicked $UIControls.btn_Next.Add_Click({ $UIControls.frame_Pages.Content = $UIControls.Page2 }) This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/21/PowerShellWPFPt2/","summary":"\u003cp\u003e\u003cstrong\u003eMore things I learned creating a GUI tool with PowerShell and WPF.\u003c/strong\u003e\u003cbr\u003e\nThe snippets in this article are based on the \u003ca href=\"https://github.com/gbdixg/Show-Win32AppUI\"\u003eShow-Win32AppUI tool\u003c/a\u003e available on GitHub.\u003c/p\u003e","title":"Creating a GUI App with PowerShell and WPF - Part 2 Controls, Events and XAML"},{"content":"Things I learned creating a GUI tool with PowerShell and WPF.\nThe snippets in this article are based on the Show-Win32AppUI tool available on GitHub.\nDisclaimer I realise PowerShell isn\u0026rsquo;t suited to creating GUI apps. The main reason to use PowerShell is supportability. Specifically, when working with Colleagues who aren\u0026rsquo;t comfortable maintaining a compiled language like C#. Its far easier to make small changes to variables or paths in a script.\nWhy WPF? WPF is a more modern and flexible choice for a UI than the something like WinForms. There are many newer frameworks available, but most require a runtime on the target platform. WPF is easy to deploy as it\u0026rsquo;s built-into the .NET Framework and available by default on Windows 10/11.\nPart1 - Design the app around PowerShell Runspaces If you try to create a GUI app with a single thread, it will be unresponsive and hang whenever an action takes more than a few seconds. PowerShell runs under a single thread (STA mode) making it unsuitable for a responsive GUI app. However, creating separate Runspaces is a workaround for this problem. Runspaces are analogous to spinning up new PowerShell sessions in the background to execute discrete script blocks.\nWhile Runspaces are effective, they also add a lot of complexity. For example:\nFunctions and variables are not shared between Runspaces by default and have to be imported when the Runspace is started. You should use a thread-safe collection when updating shared variables inside a Runspace. The WPF UI can\u0026rsquo;t be updated directly from a separate Runspace Writing to a single file from multiple Runspaces requires a locking mechanism, such as a Mutex The lifecycle of Runspaces must be managed, capturing output at completion Warning, Verbose and Error streams in the Runspace are not captured by default Warning, Verbose and Error streams do not appear in the console by default. These concepts are covered in more detail below.\nSharing variables, functions and modules with a Runspace Required modules must be specifically loaded into the InitialSessionState of the Runspace. The Runspace won\u0026rsquo;t automatically have access to modules already loaded in the parent PowerShell session.\n### Example of importing modules into a new Runspace ### $modulesToLoad=@(\u0026#39;Microsoft.Graph.Intune\u0026#39;,\u0026#39;MSAL.PS\u0026#39;) # Must be installed on the computer $initialSessionState = [initialsessionstate]::CreateDefault() foreach($module in $modulesToLoad){ $initialSessionState.ImportPSModule($module) } $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) You can also import standalone Functions into a Runspace using the InitialSessionState. The following imports functions already loaded in the parent session, but you could also load directly from a file on disk.\n### Example importing parent session functions into a new Runspace ### $FunctionsToImport = @(\u0026#39;Write-TxtLog\u0026#39;,\u0026#39;Get-APIResults\u0026#39;) foreach($function in $functionsToImport){ $definition = Get-Content \u0026#34;Function:\\$Function\u0026#34; $entry = New-Object System.Management.Automation.Runspaces.SessionStateFunctionEntry -ArgumentList $function, $definition $initialSessionState.Commands.Add($entry) } $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) Variables can be shared with a Runspace using the SetVariable method of the SessionStateProxy class. SetVariable parameters are the variable name (without the \u0026lsquo;$\u0026rsquo;) and the value to set.\n### Example importing parent session variables into a new Runspace ### $VariablesToImport = @(\u0026#39;username\u0026#39;,\u0026#39;password\u0026#39;,\u0026#39;displayName\u0026#39;) # existing variables in the parent session $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() Foreach($var in $VariablesToImport){ $VarValue = Get-Variable -Name $Var | Select -ExpandProperty Value $Runspace.SessionStateProxy.SetVariable($Var,$VarValue) } Runspace output using a thread-safe collection Output from a Runspace can be captured during execution using a thread-safe collection imported from the parent session. When a Runspace updates the collection the updated values are available in the parent session and any concurrent Runspaces that also import the collection. Thread safe collections usually need to be locked during update to prevent conflicts.\nWith a synchronised Arraylist, values added in the Runspace will immediately available to all other Runspaces and the parent session, as in following example:\n### Example using locks on a thread safe collection ### # Parent session code $BackgroundJobs = [system.collections.arraylist]::Synchronized((New-Object System.Collections.ArrayList)) # Thread safe collection $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() $Runspace.SessionStateProxy.SetVariable(\u0026#39;BackgroundJobs\u0026#39;,$BackgroundJobs) # Pass the variable into the RunSpace $PSCode = [powershell]::Create().AddScript({ # This is the Runspace script block try{ [System.Threading.Monitor]::Enter($BackgroundJobs.SyncRoot) # lock $BackgroundJobs.Add(\u0026#34;New item\u0026#34;) # modify ArrayList }finally{ [System.Threading.Monitor]::Exit($BackgroundJobs.SyncRoot) # unlock } },$True) $PSCode.Runspace = $Runspace $null = $PSCode.BeginInvoke() There are also Queues and Stacks in .NET that automatically implement locking and don\u0026rsquo;t need the System.Threading.Monitor code in the above example e.g. a ConcurrentQueue:\n### ConcurrentQueue example - doesn\u0026#39;t need System.Threading.Monitor ### # Parent session code $colQueue = [System.Collections.Concurrent.ConcurrentQueue[psobject]]::new() $colQueue.TryAdd([pscustomobject]@{ givenName = \u0026#39;Bill\u0026#39;;sn=\u0026#39;Gates\u0026#39;}) $colQueue.TryAdd([pscustomobject]@{ givenName = \u0026#39;Steve\u0026#39;;sn=\u0026#39;Jobs\u0026#39;}) $Runspace.SessionStateProxy.SetVariable(\u0026#39;colQueue\u0026#39;,$colQueue) $PSCode = [powershell]::Create().AddScript({ # This is the Runspace script block # No locking required when ConcurrentQueue is modified in the Runspace $Entry = $null if($colQueue.TryDequeue([ref]$Entry)) { Write-Output $Entry } },$True) More information on thread safe collections is available here\nHow to update the WPF user interface from another Runspace If you try to modify the WPF user interface from a separate Runspace, PowerShell will throw an error indicating only the owning thread (Runspace) can update it.\nThe solution is to wrap the update in a Dispatcher.Invoke method as follows:\n### Example updating a WPF control from another thread (Runspace) ### # In the Runspace script block # txt_SetupFile is a WPF text box created in the parent session of the Runspace $UIControls.txt_SetupFile.Dispatcher.Invoke([action]{ $UIControls.txt_SetupFile.Text = \u0026#34;Successfully updated from another Runspace\u0026#34; },\u0026#34;Normal\u0026#34;) Writing to the same log file from separate Runspaces Runspaces also make it more complex to write to a single log file. There is potential for a deadlock or race condition to occur. A Mutex is one way to implement the required locking:\n### Example using a Mutex lock before writing to a log file ### # In the Runspace script block # LogMutex is an arbitrary name but must be the same when used in any Runspace and the parent session $mtx = New-Object System.Threading.Mutex($false, \u0026#34;LogMutex\u0026#34;) If ($mtx.WaitOne()){ # Wait until this Runspace can get a lock on the LogMutex object # Lock obtained. Other Runspaces are now waiting try{ Add-Content -Path $logFile -Value $Message -ErrorAction Stop }finally{ [void]$mtx.ReleaseMutex() # release the lock $mtx.Dispose() } } Managing Runspace lifecycle A Runspace executes its script block asynchronously and output (if any) is available at the end. The parent session must manage Runspaces, checking for completion, processing output and ultimately disposing of them. If you don\u0026rsquo;t dispose of Runspaces they will persist until the parent PowerShell session is closed and could eat-up memory.\nA Timer is a common way to manage Runspaces in an event-driven WPF script. When the Timer event fires, its script blocks checks for Runspace completion as in the example below. A thread safe collection is used to keep track of Runspaces until they are disposed of.\n### Example Timer code to clean-up completed Runspaces ### # Create a collection to track Runspaces $PS = [powershell]::Create().AddScript($codeToRunInRunspace) $handle = $PS.BeginInvoke() # Start the runspace # Add the new Runspace to the RunspaceJobs collection $RunspaceJobs = [system.collections.arraylist]::Synchronized((New-Object System.Collections.ArrayList)) try{ [System.Threading.Monitor]::Enter($RunspaceJobs.SyncRoot) # lock $RunspaceJobs.Add([PSCustomObject]@{ powerShell = $PS # System.Management.Automation.PowerShell object runspace = $handle # System.Management.Automation.PowerShellAsyncResult Object }) | Out-Null }finally{ [System.Threading.Monitor]::Exit($RunspaceJobs.SyncRoot) # unlock } # Timer to manage Runspace lifecycle $RunspaceCleanupTimer = New-Object System.Windows.Forms.Timer $RunspaceCleanupTimer.Enabled = $true $RunspaceCleanupTimer.Interval = 5000 # Timer code runs every 5 seconds $RunspaceCleanupTimer.Add_Tick({ # In the timer code Foreach($job in $Script:RunspaceJobs){ if($job.runspace.IsCompleted -eq $True){ # Capture completed Runspace output and dispose of it to free-up memory try{ $RSOutput = $job.powerShell.EndInvoke($job.runspace) $job.powerShell.runspace.Dispose() # Remove the job from the tracking collection try{ [System.Threading.Monitor]::Enter($Script:RunspaceJobs.SyncRoot) $Script:RunspaceJobs.Remove($job) }finally{ [System.Threading.Monitor]::Exit($Script:RunspaceJobs.SyncRoot) } }catch{ Write-Host \u0026#34;Runspace disposal Failed \u0026#39;$_\u0026#39;\u0026#34; } } }#foreach })#End of timer scriptblock $RunspaceCleanupTimer.Start() Capturing Verbose, Warning and Error streams from a Runspace By default, the Runspace output streams are not displayed in the parent session console and are lost when the Runspace is disposed.\nThe output can be captured at Runspace completion using the Streams object. The modification below to the Timer script block saves the output to a log file.\n#### Modified Timer code to capture additional output streams at clean-up #### Foreach($job in $Global:BackgroundJobs){ if($job.runspace.IsCompleted -eq $True){ Write-TxtLog \u0026#34;Runspace \u0026#39;$($job.powershell.runspace.name)\u0026#39; completed...\u0026#34; # Could also include \u0026#39;DEBUG\u0026#39; and \u0026#39;Information\u0026#39; streams if used in your Runspaces $Streams = @{ \u0026#39;Verbose\u0026#39;=\u0026#39;VERBOSE\u0026#39; \u0026#39;Warning\u0026#39;=\u0026#39;WARN\u0026#39; \u0026#39;Error\u0026#39;=\u0026#39;ERROR\u0026#39; } Foreach($StreamType in $Streams.Keys){ $StreamOutput = $job.powershell.Streams.\u0026#34;$StreamType\u0026#34; # Capture the Runspace output for each stream if($StreamOutput){ $StreamOutput | Foreach-Object { Write-TxtLog $_ -indent 2 -severity $($Streams[$StreamType]) } } Remove-Variable -name \u0026#39;StreamOutput\u0026#39; -force -ErrorAction SilentlyContinue } Write-TxtLog \u0026#34;Disposing of runspace...\u0026#34; -indent 1 try{ $RSOutput = $job.powerShell.EndInvoke($job.runspace) $job.powerShell.runspace.Dispose() # Remove the job from the tracking list try{ [System.Threading.Monitor]::Enter($Global:BackgroundJobs.SyncRoot) $Global:BackgroundJobs.Remove($job) }finally{ [System.Threading.Monitor]::Exit($Global:BackgroundJobs.SyncRoot) } }catch{ Write-TxtLog \u0026#34;Failed \u0026#39;$_\u0026#39;\u0026#34; -indent 2 -severity ERROR } } }#foreach Displaying Verbose, Warning and Error Streams in the console The approach above captures output when the Runspace code has completed. If you want feedback in the console during execution there are a couple of methods.\nFirstly, the simplest option is to use the .NET Console.Writeline() method. Although this doesn\u0026rsquo;t capture the PowerShell streams, it is a simple way to provide real-time console output in the parent session.\nYou will need to implement your own colour-coding to distinguish between warnings or errors if needed.\n### Example Runspace code to write to parent PowerShell console ### [console]::ForegroundColor=\u0026#39;YELLOW\u0026#39; [Console]::WriteLine(\u0026#39;Username was not found\u0026#39;) # Automatically writes to the parent session [console]::ResetColor() Alternatively, you could pass the built-in $Host variable from the parent session into the Runspace and use methods like WriteVerboseLine() as shown below.\n### Example writing to parent console using the $Host variable # Parent session code $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() $Runspace.SessionStateProxy.SetVariable(\u0026#39;ParentHost\u0026#39;,$Host) # Built-in host variable passed into the Runspace as $ParentHost # Write to the parent console from the Runspace script block $ParentHost.ui.WriteVerboseLine(\u0026#34;Realtime verbose output from Runspace in parent console\u0026#34;) $ParentHost.UI.WriteWarningLine(\u0026#34;Realtime warning output from Runspace in parent console\u0026#34;) $ParentHost.UI.WriteErrorLine(\u0026#34;Realtime error output from Runspace in parent console\u0026#34;) See the Show-Win32AppUI tool for an example of using these ideas together in a WPF app.\nSee part2 of this series for information on WPF controls, events and XAML.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/21/PowerShellWPFPt1/","summary":"\u003cp\u003e\u003cstrong\u003eThings I learned creating a GUI tool with PowerShell and WPF.\u003c/strong\u003e\u003cbr\u003e\nThe snippets in this article are based on the \u003ca href=\"https://github.com/gbdixg/Show-Win32AppUI\"\u003eShow-Win32AppUI tool\u003c/a\u003e available on GitHub.\u003c/p\u003e","title":"Creating a GUI App with PowerShell and WPF - Part 1 Runspaces"},{"content":"A step by step guide to registering a custom Azure application for interactive MSGraph PowerShell. The example will create an app for use with the Show-Win32AppUI tool.\nWhy use a custom app? The Microsoft Graph enables access to a wide scope of Azure / Microsoft 365 providers and resources. A compromised Global Administrator account or errant script could cause widespread damage very quickly.\nA custom Azure application can limit MS Graph access to specific requirements of a PowerShell script, reducing the risk.\nDisable user consent The first step is to prevent users granting application consent. User consent is enabled by default and presents a risk of unwanted access to company data.\nIn the Azure AD portal, navigate to Enterprise applications and then Consent and permissions.\nMany companies change the setting to Do not allow user consent. Note that this does create an admin overhead, so you could look into the advanced options of defining low-risk permissions or using conditional access.\nThe following admin roles can then grant application consent:\nCloud App Administrators Global Administrators Clean-up Microsoft Graph PowerShell If you want to implement Custom Apps for access to MSGraph, you should first review and remove excess permissions from the Microsoft Graph PowerShell app. Over time administrators may have consented to more and more permissions.\nIt isn\u0026rsquo;t possible to remove permissions or revoke consent in the admin portal, but it does provide the PowerShell commands.\nIn the Azure AD portal, navigate to Enterprise applications\nIn the All applications view, select Microsoft Graph PowerShell and then Permissions\nClick Review Permissions\nSelect This application has more permissions that I want. The following PowerShell is displayed (uses the AzureAD module):\nConnect-AzureAD # Get Service Principal using objectId $sp = Get-AzureADServicePrincipal -ObjectId \u0026#34;1aded007-dfd1-49cc-8b70-9923a4f53a05\u0026#34; # Get all delegated permissions for the service principal $spOAuth2PermissionsGrants = Get-AzureADOAuth2PermissionGrant -All $true| Where-Object { $_.clientId -eq $sp.ObjectId } # Remove all delegated permissions $spOAuth2PermissionsGrants | ForEach-Object { Remove-AzureADOAuth2PermissionGrant -ObjectId $_.ObjectId } # Get all application permissions for the service principal $spApplicationPermissions = Get-AzureADServiceAppRoleAssignedTo -ObjectId $sp.ObjectId -All $true | Where-Object { $_.PrincipalType -eq \u0026#34;ServicePrincipal\u0026#34; } # Remove all delegated permissions $spApplicationPermissions | ForEach-Object { Remove-AzureADServiceAppRoleAssignment -ObjectId $_.PrincipalId -AppRoleAssignmentId $_.objectId } Custom app Step by Step The following steps create a new application with delegated API access to MSGraph, suitable for use from a PowerShell script. The permissions are specific to the Show-Win32AppUI\nOpen the Azure AD portal and select App registrations\nClick on New Registration to start the process\nEnter a name for the application and select the single tenant option. Do not enter a redirect URI at this stage.\nClick on Register\nThe application is created and the admin center shows the Overview page.\nClick Add a Redirect URI\nThe authentication step is displayed. Click on Add a platform\nSelect Mobile and desktop applications\nEnable the nativeclient URI to support PowerShell 5.1 scripts. Add a custom redirect URI of http://localhost to support PowerShell 7.x scripts.\nClick Configure to save the URIs\nNow back on the Authentication page, ensure the following options are set:\nSupported account types = Accounts in this organizational directory only Allow public client flows = No Click API permissions in the left pane\nUser.Read delegated access is already assigned by default. Click Add a permission\nSelect the Microsoft Graph API\nSelect Delegated permissions\nSearch for each of the following delegated permissions, enable and click Add permissions\nDeviceManagementApps.ReadWrite.All Repeat the process to add permissions for the following:\nGroup.ReadWrite.All GroupMember.ReadWrite.All Directory.AccessAsUser.All The API permissions page will show the list of added permissions with a warning that consent is not granted.\nClick Grant admin consent for The warnings will be replaced by a green tick.\nReturn to the application Overview page and note the Application (client) ID - this will be used to authenticate\nAdd assignment restrictions By default anyone in the tenant can access the application (although they need Role permissions to make changes in Azure AD and Intune). This step will limit access to a specific group.\nView the Properties page of the app and change Assignment required to Yes\nNext click on Users and groups then add user/group. Select the required Azure AD group and Assign. Only members of this group can authenticate using the app.\nAuthenticating using the custom app Delegated consent uses the intersection of application permissions and user permissions to authorise access. i.e. the authenticated user must have the required permissions as well as the application.\nUse interactive authentication to provide credentials with the required role permissions. Interactive auth prompts using the familiar browser page:\nExamples of PowerShell authentication using the app are below:\nMSAL.PS authentication $TenantID = \u0026lt;your tenant ID obtained from the AAD Overview page\u0026gt; $ClientID = \u0026lt;application ID of the custom app\u0026gt; $Token = Get-MSALToken -TenantID $TenantID -ClientID $ClientID -Interactive IntuneWin32App authentication $TenantID = \u0026lt;your tenant ID obtained from the AAD Overview page\u0026gt; $ClientID = \u0026lt;application ID of the custom app\u0026gt; Connect-MSIntuneGraph -TenantID $TenantID -ClientID $ClientID -Interactive The Connect-MSIntuneGraph command in the IntuneWin32App module creates global variables to store the token for later use:\n$Global:AuthenticationHeader $Global:AccessToken This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/19/AzureAppRegistration/","summary":"\u003cp\u003eA step by step guide to registering a custom Azure application for interactive MSGraph PowerShell. The example will create an app for use with the \u003ca href=\"/2023/03/18/Show-Win32AppUI/\"\u003eShow-Win32AppUI tool\u003c/a\u003e.\u003c/p\u003e","title":"Avoid excess permissions with a custom app to access MSGraph PowerShell"},{"content":"A GUI tool for end-to-end creation of Win32 Apps in Microsoft Intune.\nThis post provides details of \u0026ldquo;Show-Win32AppUI\u0026rdquo;, a GUI tool that simplifies the end-to-end process of creating Win32 Apps in Intune.\nThe GitHub source is available here. Instructions for setup and use are below.\nThis isn\u0026rsquo;t a one size fits all community tool. You will likely need to modify it to meet your needs. However, its written in PowerShell and has code comments and blog posts to make editing simpler.\nSETUP Modules Show-Win32AppUI depends on two PowerShell modules. Install these modules if you don\u0026rsquo;t already have them.\nInstall-Module -Name MSAL.PS Install-Module -Name IntuneWin32App The most recent tested versions are listed below:\nInstall-Module -Name MSAL.PS -RequiredVersion 4.37.0.0 Install-Module -Name IntuneWin32App -RequiredVersion 1.4.0 Tenant ID Update the $TenantID on line 4 of Show-Win32AppUI.ps1 to use your required Azure tenant. Your tenant ID is available from the Azure AD portal Overview page.\nAzure Client App An Azure Client App is used with interactive authentication to access the Microsoft Graph. There are two setup steps required:\nSpecify the Azure application\nBy default, the tool will use the built-in Microsoft Graph PowerShell enterprise application. However, I recommend creating a custom Azure app in your own tenant. A step by step guide to creating a custom app is available here. If using a custom app, update Show-Win32AppUI.ps1 to set the $ClientID variable on line 6 to match the client ID (a.k.a Application ID) of your app.\nConsent to the required permissions on behalf of your tenant\nWhether you use a custom app or Microsoft Graph PowerShell, the app must be configured with the required API permissions and consent must be granted. The delegated permissions are listed below. A step by step for setting these permissions can be found in the second part of this article. Directory.AccessAsUser.All DeviceManagementApps.ReadWrite.All Group.ReadWrite.All GroupMember.ReadWrite.All User.Read User permissions Delegated consent uses the intersection of application permissions and user permissions to authorise access. i.e. the authenticated user must have the required permissions as well as the application. When using the app, authenticate using an Azure account with one of the following roles:\nIntune Administrator Global Administrator Workstation permissions The tool does not need administrative access to the client workstation. Internet access is required, to download the Win32 Content Prep tool on first use.\nPowerShell script execution PowerShell script execution is disabled on Windows clients by default. Use one of the methods below to allow script execution on the workstation.\nset-executionpolicy Unrestricted or\npowershell -executionpolicy bypass -file \u0026lt;path to script\u0026gt; Launch the tool Start a PowerShell 5.1 or Pwsh 7.x console and execute the script as follows:\n.\\Show-Win32AppUI.ps1 To show debug information in the console add the WriteHost switch:\n.\\Show-Win32AppUI.ps1 -WriteHost Using the tool Page1 - Package\rUse the file dialog to select the main Setup File - .msi, .exe or .ps1.\nFor an .msi file, the setup and uninstall automatically uses MSIEXEC, defaulting to a quiet install/uninstall and verbose logging.\nFor an .exe file, the setup parameters default to /S, but you should check the vendor information and replace this as appropriate.\nFor a .ps1 file, setup and uninstall defaults to -noprofile and -executionpolicy bypass.\nThe package source folder is the folder containing the setup file. All the files in this folder are packaged into an .intunewin file in a later step.\nThe tool creates installation wrapper scripts called install.ps1 and uninstall.ps1 in the package source folder. Existing files with these names are overwritten.\nThe Next button is only available when required fields have been completed.\rPage2 - Deployment\rThe Display Name is built from the Publisher, App Name, Version and Package Number. If the language is changed from the default or the Bitness is changed to x86, these are also included in the Display Name\nFor .msi and .exe files, the fields are populated with information from the setup file, but can be edited as required.\nIf there is already an Intune application with the same Display Name a warning will appear in the status bar. The simplest solution is to increment the Package Number.\nThe Next button is only available when required fields have been completed.\rPage3 - Assignment\rAssignment Groups shows the names of three AAD groups for Required Install, Available Install and Uninstall. The group name suffix is based on the App Name from Page 2 and cannot be edited here.\nThe Owner must be a UPN of an AAD user. Start typing a name in the top box to see a list of options. Select a name and click Add. The Owner is set on the properties of the Win32App and the AAD groups.\nThe Dependency and Supercedence lists are populated with existing Win32 Apps. Select from the list if these options are needed. Currently you can only select one of these options due to a limitation in the IntuneWin32App module.\nClick on the Logo box to select a image file for display with the application in the Company Portal.\nThe Next button is only available when required fields have been completed.\rPage3 - Implement\rThe final page follows a step-by-step approach to creating the Win32 App.\nCreate Wrapper Scripts creates an install.ps1 and uninstall.ps1 file in the package source folder.\nCreate Intunewin Package uses the Win32 Content Prep Tool to build an .intunewin file in the Output Folder.\nCreate App Groups creates three AAD groups for Required Install, Available Install and Uninstall. If the groups already exist they are re-used.\nCreate Win32 App creates the Win32 App in Intune and uploads the .intunewin file. This step can take some time depending on the package size.\nConfigure Dependency / Configure Supercedence modifies the Win32 App in Intune. These steps are skipped if they are set to None.\nConfigure Assignment modifies the Win32 App in Intune to add the assignment groups created in the earlier step.\rTroubleshooting The tool creates a debug log on every run with detailed information and error messages. The default LogFolder is C:\\Temp, but can be modified on Line 8 of Show-Win32AppUI.ps1.\nThe -WriteHost switch will also show the debug output in the console.\n.\\Show-Win32AppUI.ps1 -writehost Options The variables section in Show-Win32AppUI.ps1 allows default settings to be modified. The following section is at Line 50:\nDetection Method The Win32 app detection is hardcoded to use a file exists method. The Install.ps1 script wrapper creates a \u0026ldquo;.ps1.tag\u0026rdquo; file under the %PROGRAMDATA% folder and Uninstall.ps1 deletes it - a detection method first suggested by Michael Niehaus\nCredits Show-Win32AppUI is a front-end to the excellent IntuneWin32App module. Full credit to the contributors of this project.\nThe MSAL.PS module has simplified the transition from ADAL to MSAL authentication.\nBoe Prox for PowerShell Runspace tips\nSMSAgent for PowerShell WPF tips\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/18/Show-Win32AppUI/","summary":"\u003cp\u003eA GUI tool for end-to-end creation of Win32 Apps in Microsoft Intune.\u003c/p\u003e","title":"Create Intune Win32Apps with a PowerShell GUI front-end"},{"content":"Part two of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration, looking at network routing, multi-geo considerations and scheduling issues.\nBE AWARE OF NETWORK ROUTING Although the migration tool uses a source UNC path and destination OneDrive URL, files are actually uploaded to Azure blobs before being transferred to OneDrive\ne.g. *.blob.core.windows.net\nThere is a list of required endpoints, and it\u0026rsquo;s important to determine if the migration traffic will route out through the enterprise proxy servers, or go direct e.g. through an ExpressRoute link. This routing will be specific to your network setup.\nUse \u0026ldquo;Agent Groups\u0026rdquo; in an enterprise network Agent Groups are a logical grouping within Migration Manager, allowing each migration to use a specific agent or agent(s)\nCarefully consider the network location of each on-prem migration agent, to optimises the traffic flow and minimise bandwidth impact. The two key considerations are:\nproximity to the home drive file server proximity to the Azure/Internet egress link. Assign each agent to an Agent Group based on its location. Use the Agent Group option when scheduling a migration to control which agent is used for a specific migration batch.\nSPO Admin Portal \u0026gt; Migration \u0026gt; File Shares \u0026gt; Agents \u0026gt; Select an Agent \u0026gt; Edit \u0026gt; Agent Group Scanning always uses the Default agent group Scanning is a pre-migration activity used to identify home drive data issues such as \u0026lsquo;path too long\u0026rsquo;.\nWhile migration tasks can use agent groups, scanning tasks cannot. Scanning automatically uses the Default Agent group.\nIf the default group has no agents the scanning task will just wait indefinitely for an agent to be added back to the Default group.\nGroup migration batches by location When you use the bulk migration option, the portal imports a CSV file and assigns all entries to the same Migration Agent. If you are carrying out migrations in multiple locations, you need to group them by location and split into separate CSV files to make efficient use of the network..\nRun Satelite-geo migrations separately If you are in a multi-geo tenant, don\u0026rsquo;t mix geolocations in the same migration batch. Migration to a satellite geo needs a modified process with dedicated or modified agent config.\nIf you are signed into the SharePoint Online Admin portal \u0026ldquo;central geo\u0026rdquo;, switch to the relevant \u0026ldquo;satellite geo\u0026rdquo; using the link in the top left. The go to Migrations \u0026gt; File Shares. Initially there will be no agents listed.\nRun the agentsetup.exe on the migration server and pause on the first screen of the setup wizard. If the agent is already installed, re-run setup to modify the configuration.\nWith the setup wizard open, edit the following file:\n%temp%\\SPMigrationAgentSetup\\SPMigrationAgentSetup\\Microsoft.SharePoint.Migration.ClientShared.dll.config Under AppSettings, add the following:\n\u0026lt;add key=GeoLocation value=\u0026#34;FRA\u0026#34; /\u0026gt; The value must be a valid Azure tenant location code that is already enabled in your tenant.\nSave the file and continue the setup wizard. The Migration Manager agent will register in the satellite geo and be visible in the SPO Admin Portal for the relevant Geolocation.\nTIMEZONE HEADACHES Migration tasks can either start immediately or at a specified time. However, the time used in the Migration Manager portal is the time zone tenant home location, not the local time of the administrator accessing the portal.\nTo schedule a migration outside business hours, the administrator must take account of the local time where the agent is based and convert it to the tenant home time zone.\nScanning tasks don\u0026rsquo;t have a scheduling option. They just start immediately, although they use a lot less bandwidth.\nFALSE POSITIVE SCAN WARNINGS Temporary MS Office files create false positive warnings in the scanning tab and scan reports. There is no way to ignore the false positives.\nThe purpose of home drive scanning is to highlight problems such as incompatible file names and long paths. However, it is hard to filter out unimportant issues:\nNumerous files beginning with tilde are flagged as warnings in every report - e.g. \u0026ldquo;~budget 2020.xlsx\u0026rdquo;. These temporary files created by MS Office are often not cleaned-up when a document is closed, but it is safe to skip them during migration.\nIf folder redirection to the home drive is enabled, the scan will flag a warning on every instance of desktop.ini. Again these warnings can be safely ignored as the files are recreated by Windows if needed.\nThis isn\u0026rsquo;t to say that the pre-migration scan is a waste of time. It does have some benefits:\nconfirms the Migration Manager service account has access to the home drive\nprovides statistics such as overall home drive size and number of files\nhighlights long path issues\nCOMPLEX MIGRATION SCHEDULING There is nothing in Migration Manager help with user scheduling. Scheduling is required because there is some user impact:\nUser communciation should explain what is changing and link to more information (FAQs, OneDrive training material etc) Ideally users log off prior to the data copy to prevent issues with files locked in-use Remote users may need to use PLAP VPN to ensure the home drive mapping is removed at next logon Post-migration support may be required to help users with the changes (removal of home drive and new data location) If your IT department does not have any scheduling software, you will most likely end-up using spredasheets.\nJUGGLING CSV FILES An enterprise will need to use the \u0026ldquo;bulk migration\u0026rdquo; option in Migration Manager to migrate large numbers of home drives at a time. This option uses a CSV import to specify source UNC paths and target OneDrive URLs. The CSV must be a set format and a template can be downloaded for this purpose.\nThe pre-migration file share scan step also uses a CSV file, but unfortuately it uses different fields and headers.\nThe additional migration steps are likely to also be driven from a list such as a CSV file e.g.\nPre-provisioning OneDrives that don\u0026rsquo;t already exist Removing Active Directory user homeDirectory and homeDrive Updating folder redirection policies Updating home drive ACLs or file shares to prevent post-migration access With multiple CSV files in-play and last minute changes to schedules, user scheduling can be complex in its own right.\nSUMMARY Microsoft Migration Manager is a free but basic migration tool. By the time you have discovered its limitations, you may wish you had paid for an ISV product instead.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2022/06/11/migration-manager-notes2/","summary":"\u003cp\u003ePart two of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration, looking at network routing, multi-geo considerations and scheduling issues.\u003c/p\u003e","title":"Migrate home drives to OneDrive with Microsoft Migration Manager - Pt2"},{"content":"Part one of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration using Microsoft Migration Manager, covering migration tool considerations, architecture and access requirements.\nTOOL SELECTION Migration Manager comes from the Microsoft purchase of Mover.io in 2019. It is presented as an Enterprise migration tool that puts more structure around the migration process than the more basic SharePoint Migration Tool. However, be prepared for limited configuration and customisation options. As a basic guide consider the following tool choices for file share migration:\nSharePoint Migration Tool : Ad hoc data migrations Migration Manager : Large scale migrations using repetitive process 3rd Party Tool : When you need a lot of control or have a complex process WHAT ARE COMPLEX REQUIREMENTS? Migration Manager focuses on the data migration, but for most companies, this is only part of the story. Consider the following:\nHow to move off folder redirection and offline files Removing the home drive mapping Cleaning-up or restricting access to home drives A 3rd-party tool, could be more suitable if you are looking to automate as much of the process as possible.For example, ShareGate Migration Tool provides a PowerShell interface.\nMigration Manager is still attractive as it\u0026rsquo;s effectively free as part of an enterprise M365 license.\nARCHITECTURE Migration Manager consists of the following components:\nAgents Migration Manager agents run on-premises and perform the data migration, reading data from home drives and copying it to OneDrive for Business.\nThe agent can be installed directly on a file server (assuming it is a Windows Server), but the recommended deployment is on a dedicated server.\nThe agent sends heartbeat information to the SharePoint Online (SPO) Admin Portal and downloads scanning and migration tasks in return.\nSharePoint Online Admin Portal The SPO Admin Portal is used to manage the agents and assign migration tasks to them. The \u0026ldquo;Migration\u0026rdquo; blade has tabs for scanning, agents and migrations.\nFile shares File shares are host the data being migrated. The migration agent service account needs read access to the data and acceses it over SMB.\nProxy server The migration agent communicates with an SPO Admin portal endpoint and the Azure blob service over HTTPs. If the company does not have an Express Route, traffic is likely to go through an enterprise proxy server.\nA LOT OF ELEVATED ACCESS If you work with strict security controls, be prepared for Migraton Manager\u0026rsquo;s privileged access requirements:\nAccount Permission Purpose Cloud service account SharePoint Administrator Agent communication with SPO Admin portal Cloud service account OD4B Site Collection Admin Agent write access to OneDrives On-prem service account Read Access to Home Drives Agent read access to home drives I.T. admin account SharePoint Administrator Scheduling scans and migrations in the SPO Admin portal Why SharePoint Administrator? Under the hood, OneDrive for Business is a personal SharePoint site and from an administrative perspective, Microsoft has done very little to separate OneDrive for Business from SharePoint Online.\nThere is no OneDrive-specific RBAC. To manage OD4B, you need to be a SharePoint Administrator, and Migration Manager is operated through a blade in the SharePoint Online Admin Console. The technicians managing and monitoring the data migrations will need the SharePoint Administrator role and so will the Service Account used by the migration agent.\nSharePoint Administrator role is not enough A SharePoint Administrator cannot copy data to OneDrive by default, but they can grant themselves the additional rights needed\nBy default, only the owner (user) has access to OneDrive for Business. One option is to define a OD4B Secondary Admin group at the tenant level, but this will only apply to new OneDrives as they are provisioned.\nSPO Admin Portal \u0026gt; Advanced \u0026gt; More Features \u0026gt; Setup My Sites \u0026gt; Secondary Owner For already provisioned OneDrives, use a script to grant the SiteCollectionAdmin permission on each OneDrive to a group containing the Migration Manager cloud service account.\nConnect-SPOService -URL $AdminURL Set-SPOUser -Site $UsersOneDriveURL -LoginName $CloudServiceAccountorGroup -IsSiteCollectionAdmin $True NOTE: This command normally expects a user account as the LoginName. If using a group name, it must be in claims encoded format.\nPart two in this series will look at network routing, multi-geo considerations and scheduling issues.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2022/06/04/migration-manager-notes1/","summary":"\u003cp\u003ePart one of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration using Microsoft Migration Manager, covering migration tool considerations, architecture and access requirements.\u003c/p\u003e","title":"Migrate home drives to OneDrive with Microsoft Migration Manager - Pt1"},{"content":"Most of the time in GIT we are working at the file level. It is possible to get more granualar and work with hunks which are parts of a file e.g. a number of lines.\nUsing Hunks The --patch option of the git add command causes GIT to automatically split an updated file into hunks. It then prompts for each hunk and the contributor can decide whether to stage some or all of the hunks.\nGIT actually enters a menu system that allows granular control, such as splitting the hunk into smaller units. After adding some hunks but not all to the index, git status shows the same file is ready to be committed and also not yet staged for commit.\nThere are many commands that can work at the hunk level, including:\ngit checkout git stash git reset \u0026gt;git add --patch file.txt # This command (1/1) Stage this hunk [y,n,q,a,d,s,e,?] ? \u0026gt; y - stage this hunk n - do not stage this hunk q - quit a - stage this hunk d - do not stage this hunk or an of the later hunks in the file s - split the current hunk into smaller hunks e - manually edit the current hunk ? - print help \u0026gt; \u0026gt; git status # After adding some hunks but not all, the status shows the same file is ready to be committed and also not yet staged for commit Changes to be committed: modified: file.txt Changes not staged for commit: modified: file.txt Many ways to reference a commit The git show command provides information about a commit. The most common way to reference a commit is using its hash (or partial hash), but there are also other ways:\n\u0026gt; git show 8d4112 # use the partial hash to refer to the commit commit 8d411239358d55f45747c401c5c2c3fba8652d71 Author: Geoff Dixon \u0026lt;dixon@mail.home\u0026gt; Date: Mon Jun 7 17:44:03 +100 ... \u0026gt; \u0026gt; git show HEAD # show information about the latest commit \u0026gt; \u0026gt; git show HEAD^ # show the parent commit of HEAD \u0026gt; \u0026gt; git show HEAD^^ # show the second parent of HEAD \u0026gt; \u0026gt; git show HEAD~2 # show the 2nd commit before HEAD (same as above) \u0026gt; \u0026gt; git show HEAD@{\u0026#34;1 week ago\u0026#34;} # show head 1 week ago This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/07/04/gitedgecases/","summary":"\u003cp\u003eMost of the time in GIT we are working at the file level.  It is possible to get more granualar and work with \u003cem\u003ehunks\u003c/em\u003e which are parts of a file e.g. a number of lines.\u003c/p\u003e","title":"GIT edge cases"},{"content":"GitHub hosts open source projects that have multiple contributors. Only a few maintainers have read-write access to the repository, so how do people contribute suggested changes?\nThe answer is by copying the repo to their own GitHub account, making changes and submitting a pull request to the maintainers of the original project. This post explains the steps.\nCONTRIBUTING ON GitHub Forking a repo A fork is a clone of the repo on the same hosting provider site (i.e. GitHub). You can view all the forks from the repo home page by clicking on Insights \u0026gt; Forks\nAs an example of how forks are used, the Hugo repository on GitHub contains code for a static website generator. The maintainers of the project have write access and can push changes, but other GitHub users have read access.\nAny GitHub contributor can suggest changes by first forking the repo into their own GitHub account. The Fork button at the top-right of the home page clones the Hugo repo to a copy that appears under the contributors GitHub account. The contributor now has write access to the forked copy, but it isn\u0026rsquo;t that convenient to work directly on GitHub. They need to clone the fork to a GIT repo on their development workstation.\nCloning a fork The contributor clones the forked repo to a local GIT repo using git clone \u0026lt;SSH or HTTP address of fork repo\u0026gt;. By default, this clone will track a single remote repo called origin (the fork) that will aceept pushed updates. But how does the local repo keep track of the primary Hugo repo, and how are contributor changes pushed to the original source?\nAdding a second remote While the contributor is working on their local changes, the original Hugo repo on GitHub is also getting updates from other contributors. Any conflicts need to be resolved locally rather than expecting the maintainers of the repo to deal with them. So how does the contributor stay up-to-date while working locally on their changes?\nThe local workstation repo already has a remote (origin, the forked repo on GitHub). Adding the original Hugo repo as a second remote allows the local repo to pull chages from other contributors. This second remote is usually called upstream.\n❯ git remote -v # show current remote repos origin https://github.com/myaccount/myfork.git (fetch) origin https://github.com/myaccount/myfork.git (push) ❯ ❯ git remote add upstream https://github.com/sourcerepo/sourceproject.git # Add a second remote pointing to upstream How to push changes to upstream The final piece of the puzzle, is how to get the contributor\u0026rsquo;s changes into the upstream repo without having write access.\nGIT itself doesn\u0026rsquo;t offer a solution to this, but GitHub does hence it\u0026rsquo;s popularity for open source projects.\nFirst the contributor pushes their local repo changes to the GitHub fork (origin) using standard GIT commands. Next the contributor creates a pull request asking the maintainers of the upstream source repo to pull changes from the fork. The pull request is a messaging system that describes the changes and enables differencing checks.\nThe maintainers of the upstream repo can review and comment on the changes, possibly asking for changes before pulling and merging them into the original repo.\nUsing GIT Diff and Blame DIFF and BLAME are useful when reviewing a pull request or the project history.\ngit blame shows the file history on a line-by-line basis.\nFor each line, it displays the last commit where a line was changed, who made the change and when.\n❯ git blame .\\index.md cf6ff2bb (GD 2021-05-05 22:09:05 +0100 1) --- cf6ff2bb (GD 2021-05-05 22:09:05 +0100 2) author: GD cf6ff2bb (GD 2021-05-05 22:09:05 +0100 3) --- cc0e10d2 (GD 2021-05-11 15:55:25 +0100 4) **This is the first post in a series on creating a graph database CMDB.** cf6ff2bb (GD 2021-05-05 22:09:05 +0100 5) 7c78446d (GD 2021-05-24 19:50:01 +0100 6) Part 1: This article 7c78446d (GD 2021-06-24 21:03:19 +0100 7) Part 2: How to export computer information from Microsoft Active Directory using PowerShell, for use in a Neo4j CMDB git diff shows the differences between commits or between the git areas (working directory, index, repo). For example, to show the differene between the current commit and back two commits:\n\u0026gt; git diff HEAD HEAD~2 # show the difference between the current position of HEAD and 2 commits before HEAD diff --git a/content/blog/gitnotes/index.md b/content/blog/gitnotes/index.md index 10fa938..cc16e9f 100644 --- a/content/blog/gitnotes/index.md +++ b/content/blog/gitnotes/index.md @@ -14,48 +14,31 @@ -The version control system GIT can seem complex, but an understanding of the internal working can help with day-to-day use and is essential to get yourself out of an unexpected state. This article covers the basics of how git works, exploring the files in the object database and laying a foundation for you to explore futher on your own. +An understanding of git internals helps with day-to-day use and is essential if you need to get yourself out of an unexpected state. This article covers the basics of how git works, allowing you to explore further on your own. The first line shows the diff command diff --git...\nThe next line is git metadata that isn\u0026rsquo;t normally needed index 10fa938..cc16e9f 100644\nThe next two lines show assign symbols to the two versions of the index.md file i.e changes from HEAD are marked with \u0026ldquo;\u0026mdash;\u0026rdquo; and tchanges from HEAD~2 are marked with \u0026ldquo;+++\u0026rdquo;\n--- a/content/blog/gitnotes/index.md +++ b/content/blog/gitnotes/index.md The next line represents the header of a hunk (portion) of the file showing the lines that have been modified. In this case, 48 lines were removed at line 14 and 31 lines added at line 14. Realistically, when lines are removed and added at the same location it is a modification of the lines.\n@@ -14,48 +14,31 @@ Finally, the actual lines that are removed and added are displayed. The \u0026ldquo;-\u0026rdquo; at the start of the line means it was removed. The \u0026ldquo;+\u0026rdquo; at the start of the line means it was added. The line was actually edited to change some of the words.\n-The version control system GIT can seem complex... +An understanding of git internals helps... Another way to view the differences is using the --color-words option. This shows the changes in-line using red for removed and green for added i.e.\ngit diff --color-words HEAD HEAD~2\nWhat else can you compare with GIT DIFF? \u0026gt; git diff # show uncommited changes since the last commit \u0026gt; \u0026gt; git diff --cached # compare the repo HEAD to the index \u0026gt; \u0026gt; git diff feature1 main # compare two branches How to see a diff of every commit The git log command has an option to diff every commit. This obviously produces a lot of output.\n\u0026gt; git log --patch How to compare the list of commits between two branches Rather than looking at the changes in the commits. You will sometimes want to just look at the history of commits and understand which are only in one branch compared to another. git log can also help here:\n\u0026gt; git log feature1..main --oneline # Compare the feature1 branch to main showing the commits only in main This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/30/github/","summary":"\u003cp\u003eGitHub hosts open source projects that have multiple contributors. Only a few maintainers have read-write access to the repository, so how do people contribute suggested changes?\u003c/p\u003e\n\u003cp\u003eThe answer is by copying the repo to their own GitHub account, making changes and submitting a \u003cem\u003epull request\u003c/em\u003e to the maintainers of the original project. This post explains the steps.\u003c/p\u003e","title":"GitHub - Contribute to a repo using Fork, Clone, Push and Pull Requests"},{"content":"This article focuses on common GIT actions that affect branches, such as merging, rebasing branches and squashing local commits.\nMERGE Merging brings changes from one branch into another. A feature branch is often created to work on a particular update. When complete, the branch needs to be merged back into \u0026ldquo;main\u0026rdquo;. The process would be as follows:\nFirst switch to the main branch using either git switch main or git checkout main.\nNext merge the featureA branch into main using git merge featureA.\nGIT will create a new commit with the merged changes. This is a special commit as it has two parents - the previous commit on main and the previous commit on the featureA branch. The main branch is updated to point to this new commit and HEAD continues to point to main. FeatureA still points to the last commit on that branch.\nCONFLICTS If a conflict is detected during a merge, GIT will interupt the process and prompt for user action\nGIT will be in a special state where it expects the conflict to be resolved before continuing.\ngit status at this point shows the message \u0026ldquo;you have unmerged paths\u0026hellip;fix conflicts and run git commit\u0026hellip;use git merge --abort to abort the merge\u0026rdquo;.\n❯ git status On branch master You have unmerged paths. (fix conflicts and run \u0026#34;git commit\u0026#34;) Unmerged paths: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to mark resolution) both modified: consolidate.py no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) GIT will display the files that have conflicts. The problem can be resolved at the command line, but a graphical diff tool such as p4merge may be better.\nIf you open the files in a basic text editor you will see GIT has marked the conflicts.\nIn the example below, the file \u0026ldquo;consolidate.py\u0026rdquo; is in conflict. GIT has updated the file with markers showing the lines that need attendtion. The HEAD section shows the lines as they appear in the current branch (main). Then there is a section break and directly below are the same lines as they appear in the featureA branch.\nimport sys \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD name=read \u0026#34;Enter your name\u0026#34; age=read \u0026#34;Enter your age\u0026#34; ======= read \u0026#34;Please enter your name\u0026#34; read \u0026#34;Enter your age\u0026#34; \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; featureA To resolve the conflict, manually edit the file so it reflects the desired state and remove the markers and section break.\nSave the file, add it to the GIT index, then commit the change to complete the merge, as shown below:\n❯ git add consolidate.py ❯ ❯ git commit [main 1aca0e1] Introduce featureA that enables user input In summary:\nSwitch to the branch you are merging into (e.g. git switch main) Merge the required branch into the current branch (i.e. git merge feature1) Fix any conflict by editing the file Add the updated file to the index (git add \u0026lt;file\u0026gt;) Commit to complete the merge (git commit) FAST FORWARD MERGE A fast forward merge occurs automatically when git moves a branch without having to create a new commit. It just re-uses an existing commit.\nThe most common case is when you want to merge a feature branch into main, but then continue working on the Feature branch with the latest updates from main.\nWhen you first merge the branch into main, GIT creates a new commit on main that has two parents - the previous commit on main and the previous commit on the branch.\n❯ git switch main ❯ git merge featureA The status is now:\nmain branch: contains the latest changes and any conflict resolutions featureA branch: contains the working feature before the merge into main To continue developing on featureA, you need to merge main back into the featureA branch so it has the latest updates\n❯ git switch featureA ❯ git merge main At this point, GIT realises that there is already a commit that has the merged contents of featureA and main, so it just re-uses this commit. The merge message shows it performed a \u0026ldquo;fast-forward merge\u0026rdquo;\n❯ git merge main Updating 68a874e..1aca0e1 Fast-forward consolidate.py | 6 +++--- orders.py | 4 ++++ 2 files changed, 7 insertions(+), 3 deletions(-) DETACHED HEAD HEAD is a reference to a branch or a commit. Normally HEAD points to the current branch and thereby indirectly to the latest commit on that branch. Detached head is a state where the HEAD is not referencing a branch, it is pointing to an older commit.\nWhy would this happen? Perhaps you want to do some experimentation without creating a branch. You would checkout a commit rather than creating a branch. At this point HEAD is no longer tracking a branch and so it is detached. It acts like a temporary branch. After making some commits, you could do one of the following:\na) Switch back to a branch\nb) Put a branch on the current commit\nIf you switch back to a branch, any previous commits outside a branch are isolated in the object database and are not referenced by any branch. Eventually they will be removed by the GIT garbage collector.\nAlternatively, git branch \u0026lt;branchname\u0026gt; can be used to put a branch on the current commit. At this point it is like any other branch and HEAD is no longer detached.\n❯ git checkout 460ce0e Note: switching to \u0026#39;460ce0e\u0026#39;. You are in \u0026#39;detached HEAD\u0026#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch. REBASE Rebase is an alternative to a merge. It changes the base of a branch, effectively adding it to the top of another branch as if the changes were sequential rather than created in parallel.\nThe rebase process looks at the first commit that is shared by two branches and uses the next commit as the base of the branch being rebased. It detaches this branch and re-attaches it to the head of the other branch. Under the hood, the commits do not actually move, new commits are created that are copies of the original commits on the branch. GIT moves the branch and the original commits become orphaned and eventually garbage collected.\n\u0026gt; git switch featureA # make featureA the current branch \u0026gt; \u0026gt; git rebase main # rebase the featureA branch onto main Why use Rebase? Rebase can help to simplify the history of a project. If there is a lot of merging it can complicate the history.\nHowver, use with caution. Rebased history is not the true history, so merging is safer.\nSquashing commits Squashing re-writes the GIT history, making two or more commits appear as if they were a single commit. Why would you want to do this? Developers often commit very frequently when working locally on a feature, but don\u0026rsquo;t want to complicate the shared history with all these individual commits. Squashing commits before merging or pushing to an origin repo simplifies the history in a large project.\nThe interactive mode of git rebase is used to squash commits. This is totally different to the basic use of rebase. A starting point commit must be specified as we don\u0026rsquo;t normally want to edit the entire history. The starting point is excluded from the list and the interactive mode starts from the next commit.\n\u0026gt; git rebase --interactive 80f137 pick fd4d8d9 Updates Adsense css pick 8d41123 Adds adsense css pick 9b2aafa Adds article pick 32735fb Fixes error in post pick 7f0063a Fixes typo pick c85a17f Adds article # Rebase 55a0831..c85a17f onto 55a0831 (6 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit\u0026#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # An interactive editor opens as shown above. The commits are listed (in reverse order compared to most git commands). The editor gives the commands - such as pick, reword etc.\nBy default all commits are on a line that starts with \u0026ldquo;pick\u0026rdquo;. Editing these lines will modify the history.\nFor example, you can move entire lines to change the order of commits. Changing \u0026ldquo;pick\u0026rdquo; to \u0026ldquo;squash\u0026rdquo; will cause the commit to be merged with the one above and GIT will prompt to select or edit one of the two commit messages.\nThe golden rule of Rebase Rebasing can lead to problems when sharing a repo across a team. The purpose of the rebase is to simplify history, but it can lead to duplication in the GIT multi-master sharing model. The golden rule is therefore:\nNever rebase shared commits. Only use rebase for commits that have not yet been shared.\nAmmending a commit If you want to update the latest commit to add additional files, add them to the index then use the --ammend option on git commit. This will create a new commit with the additional files and leave the previous commit as an orphan that will eventually get garbage collected. You can only use this option to update the latest commit.\n\u0026gt; git commit --ammend TAGS Tags are labels for a commit. Tags are normally used to mark releases.\n❯ git tag version1_0 -a -m \u0026#34;First version. Basic features\u0026#34; # Create a new annotated tag ❯ ❯ git tag version2_0 # Create a lightweight tag (not annotated) ❯ ❯ git tag # get a list of tags Tags are rederences to a commit, but unlike branches, tags never move.\nCLONE git clone is used to copy an existing repository into an empty local folder. The copy contains the working files and the full GIT history.\nThe existing repo can be local or remote. It can be referenced by an SSH or HTTP address. By default, it will clone the branch HEAD is pointing to, but this can be modified using the -branch option.\nThe command is mostly used to clone a repo from a hosting service such as Github - so it can be edited locally and then pushed back up to Github.\n❯ git clone https://github.com/myaccount/myrepo # clone a repo from Github into the current local folder The source repo is registered as a \u0026ldquo;remote\u0026rdquo; called origin by default. git status will show if the local branch is ahead or behind the remote branch.\nThe information about the remote repo is stored in the /git/config file. The remote branches are tracked by objects in the .git/refs/remotes folder.\n❯ git show-ref main # show all branches that have main in the name and the commit they are pointing to 8195805D refs/heads/main B05DB506 refs/remotes/origin/main If the local repo is in-sync with the remote, they will be pointing to the same commit, i.e. the hashes above would be the same.\nThe git clone -bare option will clone the history but not the working area. It will also not setup the original source as a remote. This can be used to create a central repo that is just a source for cloning and not worked on directly.\nPUSH / FETCH / PULL git push is used to send local changes to the remote origin repo. But what happens if there have been other changes on the remote before we push and we now have a conflict?\nThe answer is to fetch the remote changes and resolve the conflict locally before pushing. There is a comand to get the latest changes from the remote - git fetch, but rather than fetching and then merging in two steps, the git pull command peforms a fetch and merge in one command.\nWhen working with a remote, you should always pull before pushing.\n❯ git pull origin This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/27/usinggit/","summary":"\u003cp\u003eThis article focuses on common GIT actions that affect branches, such as merging, rebasing branches and squashing local commits.\u003c/p\u003e","title":"GIT branches - merging, rebasing and squashing commits"},{"content":"GIT commands move data between four areas of the object database. This article explores the four areas and the common commands that affect them.\nSee the earlier post on How GIT stores information for information on the GIT object database.\nWhat are the four areas GIT uses? The Working Area stores project source files updated directly by a code editor The Index (or Staging Areas) tracks which files from the working area included in the next commit The Repository (or Repo) is the most important area, where GIT stores snapshots of the tracked files, called commits The Stash is a temporary area, a bit like a clipboard, that can store and retrieve a saved copy of the working area and index Understanding git requires understanding how commands move data between these four areas.\nGIT STATUS git status is one of the most important commands. It shows the status of the working area and index. The most common output shows untracked files and modified files.\n❯ git status On branch master nothing to commit, working tree clean New files When a file is created or copied into the project, it only exists in the working area. git status will show the file as untracked. It has not been added to the index (staging) area and will not be part of the next commit until added. The status command shows the list of new files and suggests how to add them to the index.\n❯ git status On branch master Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) consolidate.py nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) Modified files Files added to the index are tracked by GIT. When a tracked file is updated or modified, git status shows the it as modified and not yet staged for commit. The working area version has changed, but GIT is only tracking the previous version in the index. The updated file needs to be added to the index to be included in the next commit. The status command shows the modified files and suggests actions to take.\n❯ git status On branch master Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: consolidate.py no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Changes not committed git status also shows changes in the index but not yet committed to the repo. These could be new and/or modified files.\n❯ git status On branch master Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: consolidate.py Remote A remote is a linked repository, for example, a central source repo that the local repo was originally cloned from. GIT maintains the link to the remote and tracks differences. By default, the remote is named origin. git status will show if the remote repo is ahead or behind the current local commit.\n❯ git status On branch master Your branch is ahead of \u0026#39;origin/master\u0026#39; by 1 commit. (use \u0026#34;git push\u0026#34; to publish your local commits) nothing to commit, working tree clean GIT ADD The git add command updates the index (staging) area to track new files or add new versions of already tracked files. The most common parameters are as follows:\n❯ git add mynewfile.txt # stage a specific file in the index ❯ git add folder1/ # stage all new and modified files in folder1 ❯ git add -A # stage all new and modified files in the entire project ❯ git add . # same as above ❯ git add -i # interactive mode. Prompts for a decision on a file-by-file basis GIT DIFF git diff shows differences between the GIT areas, such as between the working area and index, or between the index and repo. Differences are displayed at the command line. git diff is convenient for quick checks and small changes. Graphical tools may be better suited for complex comparisons.\n❯ git diff # shows differences between the working area and index ❯ git diff --cached # shows differences between the index and repo diff --git a/consolidate.py b/consolidate.py index de10111..97f66ce 100644 --- a/consolidate.py +++ b/consolidate.py @@ -1 +1,2 @@ import sys +print (sys.version) In the example output above, there is a difference between the index and the repo. A line has been added in the index version, indicated by the \u0026ldquo;+\u0026rdquo; symbol.\nGIT COMMIT The git commit command creates a point-in-time snapshot of tracked files in the repo. New and modified files from the index are added to the repo as new objects. Unchanged files are just referenced by a link to the existing object in the repo. The commit itself is an object in the repository, pointing to a tree and any parent commit.\nA commit is named using a SHA1 hash. It can be referenced in commands using just the first few characters of the hash (enough that it is not ambiguous).\n❯ git commit -m \u0026#34;Enabled handling of user input\u0026#34; [main b9b6064] Enabled handling of user input 1 file changed, 1 insertion(+), 1 deletion(-) In the output above, a new commit is created on the main branch with partial SHA1 hash b9b6064\nGIT BRANCH A branch is just a reference to a commit. Creating a branch does not actually change objects in the repo, it just creates a named pointer to an existing commit. If you are on the main branch and create a new branch called dev, GIT adds a new dev object in the .git/refs folder pointing to the current commit. Initially both branches are pointing to the same commit.\nThe git branch command shows the current branches or creates a new branch, but does not switch to it. The asterisk in the output below shows that after creating a new dev branch, main is still the current branch:\n❯ git branch dev # create a new branch. ❯ ❯ git branch # show branches dev * main GIT SWITCH git switch is a relatively new command that switches between branches. It does not make changes to the repo, but it does affect the working area and index. When you switch to another branch, the GIT HEAD reference is updated to point to the selected branch. The branch refers to a commit and the files and folders in the working area and index are replaced by the files in this commit.\n❯ git switch dev Switched to branch \u0026#39;dev\u0026#39; ❯ ❯ git branch * dev main You can create a branch and switch to it with the -c option\n❯ git switch -c feature Switched to a new branch \u0026#39;feature\u0026#39; GIT CHECKOUT When it comes to moving between branches, git checkout is almost identical to git switch. The checkout command has been around for much longer and has options that perform other actions. The variety of uses for the checkout command was deemed confusing and switch was introduced to focus purely on moving between branches.\n❯ git checkout dev # change to the dev branch Switched to branch \u0026#39;dev\u0026#39; ❯ ❯ git branch # show \u0026#39;dev\u0026#39; is now the current branch * dev feature main ❯ ❯ git checkout -b feature2 # create a new branch and switch to it Switched to a new branch \u0026#39;feature2\u0026#39; ❯ ❯ git branch # show \u0026#39;feature2\u0026#39; is not the current branch dev feature * feature2 main GIT LOG git log shows the history of commits and branches. It does not make any changes to the repo, index or working area. Use the \u0026ndash;graph option to see a basic diagram of branches and merges:\n❯ git log --oneline --graph --decorate * a7d531d (HEAD -\u0026gt; main) Merge Feature2, resolve conflict in orders.py |\\ | * d0773eb (feature2) Adds input validation to orders.py * | bba3001 Modifies output in orders.py |/ * 460ce0e Enables user input * b9b6064 Creates orders.py GIT RESET git reset is used to rollback to a previous commit. It can affect the repo, the index and the working area depending on the parameters.\nA common use case is to abandon some changes that have been committed and revert to a previous version of the project. Another is to abandon changes in the working area and revert to the last commit in the repo.\nThe reset command will move the current branch to the specified commit. By default, it will also overwrite the index with the contents of the commit. If you want to completely remove all traces of the unwanted change, you can also overwrite the working area using the \u0026ndash;hard option\n\u0026gt; git log --oneline # show the history of commits 848eae0 (HEAD -\u0026gt; main) Adds search to 404 page 364b33f Changes format of 404 page d4ff0dc Modifies position of search box c55dbb1 Enables comments cccb986 Adds home page, header and footer 4478398 Initial commit \u0026gt; \u0026gt; git reset --hard d4ff0dc # Rollback the repo two commits, overwriting the index and working area HEAD is now at d4ff0dc Modifies position of search box Other options are:\n\u0026gt; git reset --mixed d4ff0dc # Rollback the repo two commits but only overwrite the index. The working area is not touched. This is also the default if no options are specified. \u0026gt; git reset --soft d4ff0dc # Rollback the repo two commits, but don\u0026#39;t touch the index or working area HEAD RESET git reset HEAD is a quick way to return the index/working area to the committed state of the repo. This is an unsual reset as it doesn\u0026rsquo;t actually change anything in the repo.\nHEAD is usually poiting to the latest commit on the current branch. Resetting the repo to HEAD changes nothing in the repo, but by default it will perform a mixed reset, overwriting the index. We can also use the \u0026ndash;hard option to overwrite both the index and working area:\n\u0026gt; git reset --hard HEAD GIT STASH The Stash is a temporary storage area to save the working area and index when you need to working on another feature. You don\u0026rsquo;t want to lose or commit what you are currently working on, so you stash the changes, work on something else and then retrieve them later. Its a bit like saving something in a clipboard.\nWhen you run git stash, GIT copies tracked files in the working area and index that aren\u0026rsquo;t in the current commit and saves them to the stash. It then checks out the current commit (overwriting the working area and index so they match the repo).\nWhen ready to restore the stashed files, run git stash apply.\n\u0026gt; git stash --include-untracked # Save changes to the stash including untracked files \u0026gt; \u0026gt; git stash list # List the contents of the stash. This is an array of saves starting with zero \u0026gt; \u0026gt; git stash apply # Copies the latest entry in the stash to the working area and index. You can also specify an array element number if you don\u0026#39;t want the latest This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/24/gitandthefourareas/","summary":"\u003cp\u003eGIT commands move data between four areas of the object database. This article explores the four areas and the common commands that affect them.\u003c/p\u003e\n\u003cp\u003eSee the earlier post on \u003ca href=\"/2021/06/17/git-cheatnotes/\"\u003eHow GIT stores information\u003c/a\u003e for information on the GIT object database.\u003c/p\u003e","title":"GIT storage - understanding the stash, working, index, repo"},{"content":"GIT has become the de-facto version control system, but it can get complicated quickly. A look under-the-hood can help with day-to-day use and file recovery. This article explores the files in the object database laying a foundation for more advanced use.\nHOW DOES GIT STORE OBJECTS? GIT stores information in the hidden .git folder in the root of the project. The folder is created when a repository is initialized using git init\nCommits, trees and blobs are the fundamental objects in GIT\nThey are stored in the .git/objects folder:\nCommits are a point in time reference to a tree Trees represent folders Blobs represent files* *Blobs can also represent \u0026ldquo;hunks\u0026rdquo; (chunks of a file), but thats a more advanced topic for another article\nWHAT\u0026rsquo;S WITH ALL THE SHA1 HASHES? GIT creates a SHA1 checksum for each object and stores them in files under the.git/objects folder. The files are named after the SHA1 hash which means objects in the database are immutable - they cannot change. Modified files always result in new objects with a new hash, rather than updating the existing.\nGIT uses the hash values to determine which files have been modfied during a commit. New and modified files are added as new blobs. Unchanged files are just referenced, keeping the existing blob.\nTo avoid storing everything in one folder, git creates subfolders under .git/objects. The subfolder folder names are the first two characters of the SHA1 hash and the objects are grouped in these subfolders. The filename in the subfolder is the remaining characters of the hash.\nFor example:\n❯ ls .git/objects 00 00/24a57c6cee77755693e0514f244b1cfa5e645d 00/5b63f2cf1d596fa3f88834b98272a9d1bf9fc3 00/f823e0b5420e1051c80e0b37922409125e9156 01 01/28a8cb2ac88861ec18599c0b05f9481bdd3600 01/8c65d03d8269df96a7da4c3de1a62cd1d1c0ab 02 02/188f346460de1876df7dac2669360396f84a58 In the above example, there are three subfolders under .git/objects, called \u0026ldquo;00\u0026rdquo;, \u0026ldquo;01\u0026rdquo; and \u0026ldquo;02\u0026rdquo;. The full SHA1 hash of an object is constructed by adding the parent folder name to the filename.\nSo the final file listed above has the full hash of 02188f346460de1876df7dac2669360396f84a58\nCAN YOU LOOK INSIDE THE OBJECTS? The objects inside the git database are compressed, but can be viewed with the command git cat-file\nSpecify the object hash and either:\n-t = show the object type\n-p = print the contents\nNOTE: You only need part of the hash when using most GIT commands (and GIT sometimes truncates the hash in its own output)\nWHAT TYPE OF OBJECT IS THIS? ❯ git cat-file 0b4271c56 -t # display the object type commit The above object is a commit.\nWHAT\u0026rsquo;S INSIDE A COMMIT OBJECT? ❯ git cat-file 0b4271c56 -p tree 30b4d42bbe1a39dcc314f7c280b1437a1925585e parent cc0e10d238e78a57115572360a93deba2554d185 author GD \u0026lt;GD@LOCAL.HOME\u0026gt; 1621179226 +0100 committer GD \u0026lt;GD@LOCAL.HOME\u0026gt; 1621179226 +0100 Updated summaries. Added article In the above output:\ntree is a hash reference to the root tree (folder). parent is the hash of the parent commit (unless this is the first commit) Author and committer are the operator who created the commit Finally there is the commit message, Updated summaries\u0026hellip; WHAT\u0026rsquo;S IN A TREE OBJECT? We can view the contents of a tree object in the same way e.g. using the hash of the tree in the commit above:\n❯ git cat-file -p 30b4d42bbe 100644 blob d298be107f27247a24d24f8f78c55d42359007be .gitignore 100644 blob e3720ce5ced245ef02620afca619727c001e85bf 404.html 100644 blob 82b909c8a3de119782d6b66288734f82a4a57d1b about.md 040000 tree 272bc4b082fa15dd84b08712206d2edfe2b41e9a archetypes 040000 tree e305983083fc1872542004d046abdf3a683407e1 config 040000 tree 955f968be02f980640e570874f4c155da51882d4 content The first three items in the output are references to blobs (files) in the root of the tree (e.g. the .gitignore file). The rest are references to child trees, which can be explored further using the cat-file command.\nWHAT\u0026rsquo;S IN A BLOB? ❯ git cat-file -t d298be107 # get the object type blob ❯ ❯ git cat-file -p d298be107 # get the object contents public/ The blob finally contains the actual content, rather than a reference.\nIn this case, it is the .gitignore file that contains a single line to exclude public from the repo\nWHAT ABOUT BRANCHES? Branches are very simple in GIT. They are just references to a commit.\nBranch objects aren\u0026rsquo;t compressed so we can look at the contents of the file directly (without needing git cat-file).\nLocal branches are stored in the .git/refs/heads folder:\n❯ cat .git/refs/heads/main # show the contents of the main file 0b4271c561e6c7ad5dcf788afdc29bebbf11e171 This output is what we expected, the contents of the main branch are a reference to a commit using the SHA1 hash.\nIf we explore the branch using git cat-file, it gives us information about the commit the branch is pointing to:\n❯ git cat-file -t main # show the object type commit ❯ ❯ git cat-file -p main # show the object contents tree 30b4d42bbe1a39dcc314f7c280b1437a1925585e parent cc0e10d238e78a57115572360a93deba2554d185 author Geoff Dixon \u0026lt;GBDixg@WESTCLIFF.HOME\u0026gt; 1621179226 +0100 committer Geoff Dixon \u0026lt;GBDixg@WESTCLIFF.HOME\u0026gt; 1621179226 +0100 Updated summaries. Added article This is identical to the contents of the commit we looked at earlier, because that was the latest commit on the main branch.\nWHAT\u0026rsquo;S THE HEAD? Head is a special pointer in GIT. It is a reference to the commit that is currently checked-out. Usually the latest commit on the current branch, but not always.\nThe contents of HEAD is not a hash. It contains a pointer to the name of a branch or commit.\n❯ cat .git/HEAD # show contents of the HEAD file ref: refs/heads/main SUMMARY The git object database is all about references.\nHEAD is a reference to the current commit A branch is a reference to a commit A commit is a reference to a tree A tree is a reference to blobs and child trees A blob is the actual content This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/17/git-cheatnotes/","summary":"\u003cp\u003eGIT has become the de-facto version control system, but it can get complicated quickly. A look under-the-hood can help with day-to-day use and file recovery. This article explores the files in the object database laying a foundation for more advanced use.\u003c/p\u003e","title":"GIT basics - under-the-hood"},{"content":"Windows Subsystem for Linux (WSL) is a fantastic Dev and Test environment, providing seamless integration for Linux apps and shells running on Windows 10. This post is a quick summary of the manual steps to enable WSL2 in Windows 10.\nWindows Insider Builds At the time of writing, Microsoft is making it much simpler to install and enable WSL2 using a single command. This option is only available in Insider Builds 20262 and higher.\nwsl --install For the stable release versions of Windows 10, the following manual steps are required.\nEnable virtualization support This is separate to the Hyper-V optional feature, but does use the same architecture.\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Enable WSL feature dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart WSL kernel update package This is required because Microsoft removed the previously included Linux kernel from Windows. It now gets updated and patched through Windows Update\nDownload and install the update package\nSet the default version Make sure everything is version 2\nwsl --set-default-version 2 Install the required distro Ubuntu is the most reliable on Windows at the time of writing\nUbuntu 20.04 LTS\nCreate username Start the distro from the Start Menu shortcut. When prompted, specify a username and password.\nThe user is automatically added to the Sudo group.\nUpdate packages in the distro Launch Ubuntu and start a terminal shell\nsudo apt-get update sudo apt-get upgrade Check you are running version 2 Windows 10 PowerShell:\nwsl --list -v NAME STATE VERSION * docker-desktop-data Running 2 Ubuntu-20.04 Running 2 docker-desktop Running 2 Location of the .vhdx file The virtual disk containing the Linux OS is located here:\n%LOCALAPPDATA%\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\\ext4.vhdx\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/05/15/wsl2-quick/","summary":"\u003cp\u003eWindows Subsystem for Linux (WSL) is a fantastic Dev and Test environment, providing seamless integration for Linux apps and shells running on Windows 10. This post is a quick summary of the manual steps to enable WSL2 in Windows 10.\u003c/p\u003e","title":"WSL2 on Windows 10 - quick setup"},{"content":"BACKGROUND VSCode snippets are a productivity feature allowing blocks of code to be inserted with a couple of keystrokes or tab completion. Its simple to add your own Snippets and dramatically boost your productivity. Read on for the details.\nAll paths and keyboard shortcuts in this article assume VSCode is running on Windows\nPredefined Snippets are included with most of the VSCode language extensions (use @category:\u0026ldquo;snippets\u0026rdquo; in the extensions pane to see which ones). However, they may not match your coding style, or have trigger text that suits you. User-defined snippets allow complete customisation. They also support variables that are replaced with required values on insert, making them perfect for boilerplate code.\nEDITING USER-DEFINED SNIPPETS Open a language-specific Snippet file as follows (example using PowerShell):\nFile \u0026gt; Preferences \u0026gt; User Snippets \u0026gt; PowerShell\nA .json file is displayed, empty at first.\nA Snippet is made up of the following elements\nElement Example Description name \u0026ldquo;Function template\u0026rdquo; The name is shown by Intellisense if there is no description prefix [\u0026ldquo;ft\u0026rdquo;,\u0026ldquo;function\u0026rdquo;] One or more trigger words that activate intellisense (uses substring matching) body [\u0026ldquo;function Verb-Noun {\\r\u0026rdquo;,\u0026quot;[cmdletbinding()]\\r\u0026quot;] The template code to be inserted description Advanced function boilerplate Optional description displayed by intellisense placeholder ${1:Verb-Noun} An element within the body that is replaced by the user after insertion. The number represents the tab stop position. The text is the default value that is replaced choices ${1|one,two,three|} This placeholder will prompt to choose one of the options between the pipe characters $0 [\u0026ldquo;while($i -lt 10){\\r\u0026rdquo;,\u0026quot;\\t$0\\r\u0026quot;,\u0026quot;}\u0026quot;] A special placeholder that always comes last and ends insertion mode Snippet example \u0026#34;Advanced function\u0026#34;: { \u0026#34;prefix\u0026#34;: [\u0026#34;fa\u0026#34;,\u0026#34;function\u0026#34;] \u0026#34;body\u0026#34;: [ \u0026#34;Function ${1:Verb-Noun}{\\r\u0026#34;, \u0026#34;[cmdletBinding()]\\r\u0026#34;, \u0026#34;param(\\r\u0026#34;, \u0026#34; \\r\u0026#34;, \u0026#34;)\\r\u0026#34;, \u0026#34;BEGIN{\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;PROCESS{\\r\u0026#34;, \u0026#34;$0\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;END{\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;Advanced function boilerplate\u0026#34; Note the use of a JSON array for the body and control characters for new lines. This is quite laborious to create by hand, but VSCode extensions can make this much easier\u0026hellip;\nMarketplace Snippet Extension There are a number of extensions in the VSCode marketplace that will create a Snippet from highlighted code in the editor.\nFor example, Snippet Creator will automatically detetect the in-use language and then prompt for the Snippet prefix and description. You can then edit the Snippet to fine-tune it.\nSnippet Scope Language-specific\nMost Snippets will be created in a language-specific Snippet file and will only prompt for insertion when using that language e.g.\n%APPDATA%\\Code\\User\\snippetsPowershell.json\nGlobal\nThere is also a global Snippets file that applies to all languages. This file does not exist by default but can be created from File \u0026gt; Preferences \u0026gt; User Snippets \u0026gt; New Global Snippets file. The file can have any name, but always ends in .code-snippets. For example:\n%APPDATA%\\Code\\User\\snippets\\GlobalSnippets.code-snippets\nThe global Snippets can use an additional property called Scope to limit them to a list of languages. If it isn\u0026rsquo;t specified, they are available to all.\nProject-specific\nIf a global Snippets file is placed in the .vscode folder at the root of a project, it is scoped only to that project. It can still use the scope property to further limit Snippets to specific languages.\nKeyboard Shortcut Use File \u0026gt; Preferences \u0026gt; Keyboard Shortcuts \u0026gt; Open Keyboard Shortcuts (JSON) to assign a shortcut to a Snippet. Custom shortcuts are saved in the file %AppData%\\Code\\User\\keybindings.json\nIf the Snippet is not in the Global Snippets file, the langId is used to specify a language specific Snippet:\nKeybinding example { \u0026#34;key\u0026#34;: \u0026#34;cmd+k 1\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;editor.action.insertSnippet\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;langId\u0026#34;: \u0026#34;csharp\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;NewClass\u0026#34; } } Hiding Snippets Hiding Snippets is useful when there is a lot of noise in the Intellisense prompts. This can occur when you create a user snippet with the same trigger as a language extension snippet.\nOpen the insert Snippet dialog using CTRL + ALT + J Start typing the tigger characters to show the Snippet options in the list Hover over each item and click the Hide from Intellisense option on the right hand side Extension Snippets I don\u0026rsquo;t recommend trying to edit or remove extension Snippets. Changes are likely to get overwritten when the extension updates.\nFor information, extension Snippets are stored under %USERPROFILE%\\.vscode\\extensions. For example the Microsoft PowerShell extension Snippets are at:\n%USERPROFILE%\\.vscode\\extensions\\ms-vscode.powershell-2021.2.2\\snippets\\PowerShell.json\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/05/07/vscodesnippets/","summary":"\u003ch2 id=\"background\"\u003eBACKGROUND\u003c/h2\u003e\n\u003cp\u003eVSCode snippets are a productivity feature allowing blocks of code to be inserted with a couple of keystrokes or tab completion. Its simple to add your own Snippets and dramatically boost your productivity. Read on for the details.\u003c/p\u003e","title":"Create custom code snippets in VSCode"},{"content":"Read-on for a PowerShell command to get the Active Directory Subnet and Site from the computername or IP Address.\nActive Directory Sites represent locations with good network connectivity. An ADSite is often created for each office or a group of offices in a metropolitan area, to generate the replication topology between Domain Controllers, and to help workstations/servers locate closest services.\nActive Directory Subnets define the IP ranges included in an AD Site. A member workstation/server will have an IP address that should fall within a defined AD subnet, making it part of an AD Site.\nAD Site membership is not fixed. Laptops can move between Sites and Subnets when they roam to another location.\nAn incorrect or undefined AD subnet can lead to slow logon times and slow access to DFS shares. A domain member would use any server that responds when it isn\u0026rsquo;t in a defined subnet.\nPowerShell script The Find-ADSite PowerShell function below will return the AD Site and Subnet for a specified computer name or IP Address.\nIf a company populates subnet descriptions with useful information, it can also identify information such as the specific Office or floor.\nAD Administrators may define a catch-all subnet with a wide address range. By default if the IP address is within more than one subnet, the output will only include the smallest range. Use the -AllMatches parameter to see everything.\nFunction Find-ADSite { \u0026lt;# .Synopsis Used to get the Active Directory subnet and the site it is assigned to for a Windows computer/IP address .Description Requires only standard user read access to AD and can determine the ADSite for a local or remote computer .PARAMETER IPAddress Specifies the IP Address for the subnet/site lookup in as a .NET System.Net.IPAddress When this parameter is used, the computername is not specified. .PARAMETER Computername Specifies a computername for the subnet/site lookup. The computername is resolved to an IP address before performing the subnet query. Defaults to %COMPUTERNAME% When this parameter is used, the IPAddress and IP are not specified. .PARAMETER DC A specific domain controller in the current users domain for the subnet query If not specified, standard DC locator methods are used. .PARAMETER AllMatches A switch parameter that causes the subnet query to return all matching subnets in AD This is not normally used as the default behaviour (only the most specific match is returned) is usually prefered. This switch will include \u0026#34;catch-all\u0026#34; subnets that may be defined to accomodate missing subnets .Example PS C:\\\u0026gt;Find-ADSite -ComputerName PC123456789 ComputerName : PC123456789 IPAddress : 162.26.192.151 ADSubnetName : 162.26.192.128/25 ADSubnetDesc : 3rd Floor Main Road Office ADSiteName : EULON01 ADSiteDescription : London .Notes Version: 1.1 #\u0026gt; [CmdletBinding(DefaultParameterSetName = \u0026#34;byHost\u0026#34;)] Param( [Parameter(Position = 0, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $True, ParameterSetName = \u0026#34;byHost\u0026#34;)] [string]$ComputerName = $Env:COMPUTERNAME , [Parameter(Position = 0, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $True, Mandatory = $True, ParameterSetName = \u0026#34;byIPAddress\u0026#34;)] [System.Net.IPAddress]$IPAddress , [Parameter(Position = 1)] [string]$DC , [Parameter()] [switch]$AllMatches ) PROCESS { switch ($pscmdlet.ParameterSetName) { \u0026#34;byHost\u0026#34; { try { $Resolved = [system.net.dns]::GetHostByName($Computername) [System.Net.IPAddress]$IP = ($Resolved.AddressList)[0] -as [System.Net.IPAddress] }catch{ Write-Warning \u0026#34;$ComputerName :: Unable to resolve name to an IP Address\u0026#34; $IP = $Null } } \u0026#34;byIPAddress\u0026#34; { try { $Resolved = [system.net.dns]::GetHostByAddress($IPAddress) $ComputerName = $Resolved.HostName } catch { # Write-Warning \u0026#34;$IP :: Could not be resolved to a hostname\u0026#34; $ComputerName = \u0026#34;Unable to resolve\u0026#34; } $IP = $IPAddress } }#switch if ($IP) { # The following maths loops over all the possible subnet mask lengths # The masks are converted into the number of Bits to allow conversion to CIDR format # The script tries to lookup every possible range/subnet bits combination and keeps going until it finds a hit in AD [psobject[]]$MatchedSubnets = @() For ($bit = 30 ; $bit -ge 1; $bit--) { [int]$octet = [math]::Truncate(($bit - 1 ) / 8) $net = [byte[]]@() for ($o = 0; $o -le 3; $o++) { $ba = $ip.GetAddressBytes() if ($o -lt $Octet) { $Net += $ba[$o] } ELSEIF ($o -eq $octet) { $factor = 8 + $Octet * 8 - $bit $Divider = [math]::pow(2, $factor) $value = $divider * [math]::Truncate($ba[$o] / $divider) $Net += $value } ELSE { $Net += 0 } } #Next #Format network in CIDR notation $Network = [string]::join(\u0026#39;.\u0026#39;, $net) + \u0026#34;/$bit\u0026#34; # Try to find this Network in AD Subnets list Write-Verbose \u0026#34;Trying : $Network\u0026#34; try{ $de = New-Object System.DirectoryServices.DirectoryEntry(\u0026#34;LDAP://\u0026#34; + $DC + \u0026#34;rootDSE\u0026#34;) $Root = New-Object System.DirectoryServices.DirectoryEntry(\u0026#34;LDAP://$DC$($de.configurationNamingContext)\u0026#34;) $ds = New-Object System.Directoryservices.DirectorySearcher($root) $ds.filter = \u0026#34;(CN=$Network)\u0026#34; $Result = $ds.findone() }catch{ $Result = $null } if ($Result) { write-verbose \u0026#34;AD Site found for $IP\u0026#34; # Try to split out AD Site from LDAP path $SiteDN = $Result.GetDirectoryEntry().siteObject $SiteDe = New-Object -TypeName System.DirectoryServices.DirectoryEntry(\u0026#34;LDAP://$SiteDN\u0026#34;) $ADSite = $SiteDe.Name[0] $ADSiteDescription = $SiteDe.Description[0] $MatchedSubnets += [PSCustomObject][Ordered]@{ ComputerName = $ComputerName IPAddress = $IP.ToString() ADSubnetName = $($Result.properties.name).ToString() ADSubnetDesc = \u0026#34;$($Result.properties.description)\u0026#34; ADSiteName = $ADSite ADSiteDescription = $ADSiteDescription } $bFound = $true }#endif }#next }#endif if ($bFound) { if ($AllMatches) { # output all the matched subnets $MatchedSubnets } else { # Only output the subnet with the largest mask bits [Int32]$MaskBits = 0 # initial value Foreach ($MatchedSubnet in $MatchedSubnets) { if ($MatchedSubnet.ADSubnetName -match \u0026#34;\\/(?\u0026lt;Bits\u0026gt;\\d+)$\u0026#34;) { [Int32]$ThisMaskBits = $Matches[\u0026#39;Bits\u0026#39;] Write-Verbose \u0026#34;ThisMaskBits = \u0026#39;$ThisMaskBits\u0026#39;\u0026#34; if ($ThisMaskBits -gt $MaskBits) { # This is a more specific subnet $OutputSubnet = $MatchedSubnet $MaskBits = $ThisMaskBits } else { Write-Verbose \u0026#34;No match\u0026#34; } } else { Write-Verbose \u0026#34;No match\u0026#34; } } $OutputSubnet }#endif } else { Write-Verbose \u0026#34;AD Subnet not found for $IP\u0026#34; if ($IP -eq $null) {$IP = \u0026#34;\u0026#34;} # required to prevent exception on ToString() below New-Object -TypeName PSObject -Property @{ ComputerName = $ComputerName IPAddress = $IP.ToString() ADSubnetName = \u0026#34;Not found\u0026#34; ADSubnetDesc = \u0026#34;\u0026#34; ADSiteName = \u0026#34;\u0026#34; ADSiteDescription = \u0026#34;\u0026#34; } }#end if }#process } This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/04/13/find-adsite/","summary":"\u003cp\u003eRead-on for a PowerShell command to get the Active Directory Subnet and Site from the computername or IP Address.\u003c/p\u003e","title":"Find a computer's Active Directory Site and Subnet with PowerShell"},{"content":"This article includes a PowerShell Export-Eventlog command to quickly export a Windows event log from a remote computer and copy it to the local machine.\nEvent logs are a cornerstone of troubleshooting, but getting access to them can be difficult across the network.\nIt can be faster to export a Windows event log on a remote computer, copy the .evtx file over the network and then query it locally.\nThe PowerShell Get-Winevent command can work against remote event logs, but it can be painfully slow over the network. Copying an entire exported log (.evtx file) across the same connection is much faster. Get-Winevent can still be used with the -path parameter to query the locally copied .evtx file.\nPowerShell Script wevtutil.exe is Windows .exe that can export event logs. The PowerShell function below uses wevtutil to export one ore more event logs and copy them locally. The computer name is pre-pended to the log name.\nFunction Export-EventLog { \u0026lt;# .SYNOPSIS Exports a remote event log to a file. .DESCRIPTION Uses wevtutil.exe to perform the export on the remote computer The log(s) are saved to c:\\Windows\\Temp and then moved over the network to the local computer The resulting log file is $Path\\$computername-$logname.evtx The file can then be opened in Windows Event Viewer or queried directly using \u0026#34;Get-Winevent -Path....\u0026#34; The remote computer must be online and the Windows Firewall must allow inbound RPC and SMB connections .PARAMETER Computername The name of the remote computer. .PARAMETER Logname The name(s) of the log file to export. .PARAMETER Path The local folder path where the output file will be saved Default = %TEMP% .PARAMETER RemotePath The remote folder path used to stage the exported file prior to moving it to the local folder path. Environment variables are not supported. Default = C:\\Windows\\Temp .EXAMPLE PS C:\\\u0026gt; Export-EventLog -Computername \u0026#34;PC654321\u0026#34; This command will export the System and Application event logs from the remote computer PC654321 The logs will be exported to c:\\windows\\temp on the remote computer then moved to c:\\temp on the local computer .EXAMPLE PS C:\\\u0026gt; Export-EventLog -Computername \u0026#34;PC654321\u0026#34; -LogName \u0026#34;System\u0026#34;,\u0026#34;Security\u0026#34; This command will export the System and Security event logs from the remote computer PC654321 .EXAMPLE PS C:\\\u0026gt; \u0026#34;PC654321\u0026#34; | Export-EventLog -LogName \u0026#34;Application\u0026#34;,\u0026#34;Security\u0026#34; This command will export the Application and Security event logs from the remote computer PC654321 .EXAMPLE PS C:\\\u0026gt; Get-Winevent -Computername $Computer -Listlog * -EA 0 | Where{$_.RecordCount -gt 0} | Export-EventLog -Computername $Computer This command will export all the event logs from the remote computer represented by the $computer variable .NOTES Version: 1.0 #\u0026gt; [CmdletBinding()] param( [parameter(position = 0, valuefromPipeline = $true, valuefrompipelinebypropertyname = $true)] [string[]]$Computername = $Env:COMPUTERNAME , [parameter(position = 1, valuefrompipelinebypropertyname = $true)] [string[]]$LogName = @(\u0026#34;System\u0026#34;, \u0026#34;Application\u0026#34;) , [parameter(position = 2)] [ValidateScript( { Test-Path $_ -PathType \u0026#39;Container\u0026#39; })] [string]$Path = $ENV:TEMP , [parameter(position = 3)] [string]$RemotePath = \u0026#34;C:\\Windows\\Temp\u0026#34; ) PROCESS { Foreach ($Name in $Computername) { Write-Progress -id 1 -Activity \u0026#34;Computer \u0026#34; -Status \u0026#34;$Name\u0026#34; If (Test-Connection -ComputerName $Name -Count 1 -ErrorAction SilentlyContinue) { $LogName | ForEach-Object { $Log = $_ $Output = New-Object PSObject | Select Computername, LogName, Path, Result $Output.Computername = $Name $Output.LogName = $Log if ((Get-WinEvent -LogName $Log -ComputerName $Name -MaxEvents 1 -ErrorAction SilentlyContinue | Measure-Object).Count -lt 1) { Write-Warning -Message \u0026#34;$Name::$log log is empty. Skipping export\u0026#34; return } $OutputFileName = \u0026#34;$Name-$($Log -replace \u0026#34;/\u0026#34;,\u0026#34;-\u0026#34;).evtx\u0026#34; Write-Progress -id 2 -ParentId 1 -Activity \u0026#34;Exporting\u0026#34; -Status \u0026#34;$Log\u0026#34; if ($Name -eq $Env:COMPUTERNAME) { Write-Verbose \u0026#34;Local computer...\u0026#34; $Cmd = \u0026#34;$($Env:windir)\\system32\\wevtutil.exe epl \u0026#39;$Log\u0026#39; \u0026#39;$Path\\$OutputFileName\u0026#39; /r:$Name /ow:True 2\u0026gt;\u0026amp;1\u0026#34; $CmdResult = Invoke-Expression -Command $cmd if ($CmdResult -eq $Null) { Write-Verbose \u0026#34;$Name::$log log export to \u0026#39;$path\u0026#39; = \u0026#39;Success\u0026#39;\u0026#34; $Output.Result = \u0026#34;Success\u0026#34; } else { Write-Error \u0026#34;$Name::$log log export to \u0026#39;$path\u0026#39; = \u0026#39;$CmdResult\u0026#39;\u0026#34; $Output.Result = \u0026#34;Error - $CMDResult\u0026#34; } } else { Write-Verbose \u0026#34;Remote computer...\u0026#34; # Wevtutil LogName filepath /r:\u0026lt;remote computer\u0026gt; /ow:\u0026lt;Overwrite true/false\u0026gt; $Cmd = \u0026#34;$($Env:windir)\\system32\\wevtutil.exe epl \u0026#39;$Log\u0026#39; \u0026#39;$RemotePath\\$OutputFileName\u0026#39; /r:$Name /ow:True 2\u0026gt;\u0026amp;1\u0026#34; $CmdResult = Invoke-Expression -Command $cmd if ($CmdResult -eq $Null) { # Convert \u0026lt;Drive\u0026gt;:\\ to \\\u0026lt;Drive\u0026gt;$ for remote connection $RemoteUNC = $RemotePath -Replace \u0026#39;(?\u0026lt;Drive\u0026gt;[A-Za-z]+):\u0026#39;, \u0026#39;${Drive}$$\u0026#39; # c:\\ = c$\\ Write-Verbose \u0026#34;$Name::$log log export to \u0026#39;\\\\$Name\\$RemoteUNC\\$OutputFileName\u0026#39; = \u0026#39;Success\u0026#39;\u0026#34; Write-Progress -id 3 -ParentId 1 -Activity \u0026#34;Copying\u0026#34; -Status \u0026#34;$Log\u0026#34; Try { move-item -path \u0026#34;filesystem::\\\\$Name\\$RemoteUNC\\$OutputFileName\u0026#34; -Dest $Path -Force -ErrorAction Stop Write-Verbose \u0026#34;$Name::$log log move to \u0026#39;$path\u0026#39; = Success\u0026#34; $Output.Result = \u0026#34;Success\u0026#34; } Catch { Write-Error \u0026#34;$Name::$log log move to \u0026#39;$path\u0026#39; failed - \u0026#39;$_\u0026#39;\u0026#34; $Output.Result = \u0026#34;Error - \u0026#39;$_\u0026#39;\u0026#34; } } else { Write-Error \u0026#34;$Name::$log log export to \u0026#39;\\\\$Name\\$RemotePath\u0026#39; = \u0026#39;$CmdResult\u0026#39;\u0026#34; $Result = \u0026#34;Error - $CMDResult\u0026#34; } }#end if $Output.Path = \u0026#34;$Path\\$OutputFileName\u0026#34; $Output }#foreach logname } else { Write-Warning -Message \u0026#34;$Name :: ping failed\u0026#34; } } #foreach Name }#process }#EndFunction This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/04/06/export-eventlog/","summary":"\u003cp\u003eThis article includes a PowerShell \u003cem\u003eExport-Eventlog\u003c/em\u003e command to quickly export a Windows event log from a remote computer and copy it to the local machine.\u003c/p\u003e","title":"Export Remote Eventlog with PowerShell"},{"content":" ADSystemInfo is a built-in COM object in Windows that simplifies lookup of Active Directory user and computer information.\nADSystemInfo can only return information about the local computer and current user. The computer must be joined to a domain and a domain controller must be reachable when the function is called.\nIts simple to instantiate COM objects in PowerShell. The function below shows how to use this object.\nEXAMPLE OUTPUT POWERSHELL SCRIPT Function Get-ADSystemInfo{ \u0026lt;# .Synopsis Used to lookup specific AD user/computer object properties of the current session .Description Uses \u0026#34;ADSystemInfo\u0026#34; COM object to get Active Directory attributes for the current user and computer .Example PS C:\\\u0026gt;Get-ADSystemInfo ComputerDN : CN=EGBLHCNU335BQCG,OU=GBR,OU=Workstations,OU=EU,OU=Regions,DC=mycompany,DC=com SiteName : EULON DomainDNSName : mycompany.com DomainShortName : MYCOMPANY ForestDNSName : mycompany.com IsNativeMode : True PDCRoleOwner : CN=527616-NAADCP01,CN=Servers,CN=Global,CN=Sites,CN=Configuration,DC=mycompany,DC=com SchemaRoleOwner : CN=527616-NAADCP01,CN=Servers,CN=Global,CN=Sites,CN=Configuration,DC=mycompany,DC=com UserDN : CN=gdixon2,OU=Users,OU=GBR,OU=Accounts,OU=EU,OU=Regions,DC=mycompany,DC=com .Notes Version: 1.0 .Link http://msdn.microsoft.com/en-us/library/aa705962(VS.85).aspx #\u0026gt; [CmdletBinding()] Param() Process{ $Output = New-Object -TypeName PSObject | Select ComputerDN,SiteName,DomainDNSName,DomainShortName,ForestDNSName,IsNativeMode,PDCRoleOwner,SchemaRoleOwner,UserDN $obj = new-object -com ADSystemInfo $type = $obj.gettype() $Output.ComputerDN = $type.InvokeMember(\u0026#34;ComputerName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.SiteName = $type.InvokeMember(\u0026#34;sitename\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.DomainDNSName = $type.InvokeMember(\u0026#34;DomainDNSName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.DomainShortName = $type.InvokeMember(\u0026#34;DomainShortName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.ForestDNSName = $type.InvokeMember(\u0026#34;ForestDNSName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.IsNativeMode = $type.InvokeMember(\u0026#34;IsNativeMode\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.PDCRoleOwner = ($type.InvokeMember(\u0026#34;PDCRoleOwner\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) -replace \u0026#34;CN=NTDS Settings,\u0026#34;,\u0026#34;\u0026#34;) $Output.SchemaRoleOwner = ($type.InvokeMember(\u0026#34;SchemaRoleOwner\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) -replace \u0026#34;CN=NTDS Settings,\u0026#34;,\u0026#34;\u0026#34;) $Output.UserDN = $type.InvokeMember(\u0026#34;UserName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output } } This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/03/30/adsysteminfo/","summary":"\u003cblockquote\u003e\n\u003cp\u003eADSystemInfo is a built-in COM object in Windows that simplifies lookup of Active Directory user and computer information.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Active Directory ADSystemInfo with PowerShell"},{"content":"This post includes a Get-WLAN function to show information about wireless LAN connections, including the SSID and signal strength. It also demonstrates creating a PowerShell wrapper for a built-in Windows command.\nWhy create a PowerShell exe wrapper PowerShell Tools are re-usable functions that can be used stand-alone or in a pipeline\nSometimes its more convenient to create a wrapper script using the output of a command line tool than try to create the function entirely in PowerShell. The example below creates a PowerShell command to get information about WI-Fi connections on the local computer.\nUsing Regex to parse text output Regular expressions are the ideal way to convert text output from a command line tool into PowerShell objects, making a re-usable pipeline tool. Regex is very powerful, but also intimidating. The solution below uses a handy shortcut to identify boundaries in the output - the not operator - ^.\nFor example, [^:]+ means match one or more characters that are not a colon. In the example below, this is used to split the text on each line in the command output.\nThe netsh output below needs to be split into key value pairs (e.g. SSID = MyWifi) and converted to a PSObject. For each line of output, the colon character is the obvious boundary between the key name and the value.\nNative Command Output C:\\\u0026gt; netsh wlan show interfaces There is 1 interface on the system: Name : Wi-Fi Description : Intel(r) Dual Band Wireless-AC 8260 GUID : 42bce393-237c-4bd4-9d5e-18020ba8bb87 Physical address : b7:8a:60:a5:f7:d8 State : connected SSID : MyWiFi BSSID : 30:d4:2e:50:de:7f Network type : Infrastructure Radio type : 802.11n Authentication : WPA2-Personal Cipher : CCMP Connection mode : Profile Channel : 6 Receive rate (Mbps) : 115.6 Transmit rate (Mbps) : 115.6 Signal : 97% Profile : MyWiFi Hosted network status : Not available The regex explained The PowerShell snippet below shows the regular expression and how the matches are added to a hash table collection as name = value.\n$Properties = @{} $result = netsh wlan show interfaces $Result | ForEach-Object { if ($_ -match \u0026#39;^\\s+(?\u0026lt;name\u0026gt;[^:]+):\\s(?\u0026lt;value\u0026gt;.*)$\u0026#39;) { $name = $Matches[\u0026#39;name\u0026#39;].Trim() $val = $Matches[\u0026#39;value\u0026#39;].Trim() $Properties.Add($name, $val) } } The Foreach-Object loop above processes the NetSH command output line-by-line.\nEach line (the $_ variable) is tested for a match against the RegEx expression using the PowerShell -match operator.\nThe \u0026ldquo;not\u0026rdquo; operator [^:]+ captures all the characters until the colon and saves them in the named capture group \u0026ldquo;name\u0026rdquo; ?\u0026lt;name\u0026gt;. The match then expects a colon followed by a space. Finally, everything until the end of the line is saved to the named capture group \u0026ldquo;value\u0026rdquo; ?\u0026lt;value\u0026gt;.\nMATCHES(0) = the entire line MATCHES(\u0026rsquo;name\u0026rsquo;) = from the start of the line, match any character that is not a colon MATCHES(\u0026lsquo;value\u0026rsquo;) = match everything from colon [space] to the end of the line A complete Get-WLAN PowerShell function is provided below.\nPowerShell Script Function Get-WLAN { \u0026lt;# .SYNOPSIS Gets the properties of WiFI connections .DESCRIPTION A PowerShell wrapper around NETSH WLAN to convert the output into a PS object .INPUTS None .OUTPUTS PSObject .EXAMPLE Get-WLAN .NOTES Author:GD Website: www.write-verbose.com Twitter: @writeverbose #\u0026gt; [cmdletBinding()] param() PROCESS { $Properties = @{} $result = netsh wlan show interfaces if ($LASTEXITCODE -eq 0) { $Properties.Add(\u0026#39;Computername\u0026#39;, $ENV:COMPUTERNAME) $Result | ForEach-Object { \u0026lt;# Example NETSH command output: Name : Wi-Fi Description : Intel(R) Dual Band Wireless-AC 8260 State : connected SSID : MyWiFi #\u0026gt; if ($_ -match \u0026#39;^\\s+(?\u0026lt;name\u0026gt;[^:]+):\\s(?\u0026lt;value\u0026gt;.*)$\u0026#39;) { $name = $Matches[\u0026#39;name\u0026#39;].Trim() $val = $Matches[\u0026#39;value\u0026#39;].Trim() $Properties.Add($name, $val) } } #foreach if ($Properties.Count -gt 0) { [PSCustomObject][Ordered]$Properties } else { Write-Warning \u0026#34;Failed to parse NETSH output\u0026#34; } } else { Write-Warning \u0026#34;Error from NETSH - \u0026#39;$($Error[0])\u0026#39;\u0026#34; } }#process } Regex Links For some practice with Regular Expressions, check out RegEx Golf or Regex Crosswords. There is even a Regular Expressions day.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/01/06/powershell-toolmaking-regex/","summary":"\u003cp\u003eThis post includes a Get-WLAN function to show information about wireless LAN connections, including the SSID and signal strength. It also demonstrates creating a PowerShell wrapper for a built-in Windows command.\u003c/p\u003e","title":"Get-WLAN - PowerShell Toolmaking"},{"content":"This post includes a PowerShell Get-ChromeExtension script to list installed extensions on the local or remote computer.\nBrowser extensions are supposed to be curated and vetted, but there have been many examples of malware. If you don\u0026rsquo;t already have control of extensions through an allow or blocklist, the first step is to find out what is in-use.\nThe code below is a PowerShell function to get the installed Google Chrome browser extensions from a local or remote Windows computer.\nChrome Browser Extensions install into the user profile and do not appear in the Add/Remove Programs list.\nChrome Extensions are a challenge to audit due to the way they install and lack of enumeration options. The PowerShell script below gets the installed extensions using the following method:\nGet the extension IDs from the folders names under %userprofile%\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions Lookup the extension name on the Chrome Web Store using the extension ID Get the extension version from the manifest.json file in the extension folder Example script output C:\\\u0026gt; Get-ChromeExtension | Select Name,Version,Description | ft -AutoSize Name Version Description ---- ------- ----------- Docs 0.10 Create and edit documents Google Drive 14.1 Google Drive: create, share and keep all your stuff in one place. YouTube 4.2.8 The official YouTube website Sheets 1.2 Create and edit spreadsheets Google Docs Offline 1.4 Get things done offline with the Google Docs family of products. Google Wallet 1.0.0.4 Gmail 8.1 Fast, searchable email with less spam. Chrome Cast 6618.312.0.2 Slides 0.10 Create and edit presentations Docs 0.10 Create and edit documents Google Drive 14.2 Google Drive: create, share and keep all your stuff in one place. YouTube 4.2.8 The official YouTube website OneTab 1.18 Save up to 95% memory and reduce tab clutter uBlock Origin 1.20.0 Finally, an efficient blocker. Easy on CPU and memory. Dark Reader 4.7.12 Dark mode for every website. Take care of your eyes, use dark theme for night and daily browsing. Share link via email 3.2.1 Adds a button and context menu item to send the page URL or a link URL via email Sheets 1.2 Create and edit spreadsheets Google Docs Offline 1.7 Get things done offline with the Google Docs family of products. Pinterest Save Button 4.0.82 Save the things you find on the Web. Google Wallet 1.0.0.4 ColorPick Eyedropper 0.0.2.29 An eye-dropper \u0026amp;amp; color-picker tool that allows you to select color values from webpages. Gmail 8.2 Fast, searchable email with less spam. Chrome Cast 7519.422.0.3 PowerShell Script function Get-ChromeExtension { \u0026lt;# .SYNOPSIS Gets Chrome Extensions from a local or remote computer .DESCRIPTION Gets the name, version and description of the installed extensions Admin rights are required to access other profiles on the local computer or any profiles on a remote computer. Internet access is required to lookup the extension ID on the Chrome web store .PARAMETER Computername The name of the computer to connect to The default is the local machine .PARAMETER Username The username to query i.e. the userprofile (c:\\users\\\u0026lt;username\u0026gt;) If this parameter is omitted, all userprofiles are searched .EXAMPLE PS C:\\\u0026gt; Get-ChromeExtension This command will get the Chrome extensions from all the user profiles on the local computer .EXAMPLE PS C:\\\u0026gt; Get-ChromeExtension -username Jsmith This command will get the Chrome extensions installed under c:\\users\\jsmith on the local computer .EXAMPLE PS C:\\\u0026gt; Get-ChromeExtension -Computername PC1234,PC4567 This command will get the Chrome extensions from all the user profiles on the two remote computers specified .NOTES Version 1.0 #\u0026gt; [cmdletbinding()] PARAM( [parameter(Position = 0)] [string]$Computername = $ENV:COMPUTERNAME , [parameter(Position = 1)] [string]$Username ) BEGIN { #REGION --- Child function function Get-ExtensionInfo { \u0026lt;# .SYNOPSIS Get Name and Version of the a Chrome extension .PARAMETER Folder A directory object (under %userprofile%\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions) #\u0026gt; [cmdletbinding()] PARAM( [parameter(Position = 0)] [IO.DirectoryInfo]$Folder ) BEGIN{ $BuiltInExtensions = @{ \u0026#39;nmmhkkegccagdldgiimedpiccmgmieda\u0026#39; = \u0026#39;Google Wallet\u0026#39; \u0026#39;mhjfbmdgcfjbbpaeojofohoefgiehjai\u0026#39; = \u0026#39;Chrome PDF Viewer\u0026#39; \u0026#39;pkedcjkdefgpdelpbcmbmeomcjbeemfm\u0026#39; = \u0026#39;Chrome Cast\u0026#39; } } PROCESS { # Extension folders are under %userprofile%\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions # Folder names match extension ID e.g. blpcfgokakmgnkcojhhkbfbldkacnbeo $ExtID = $Folder.Name if($Folder.FullName -match \u0026#39;\\\\Users\\\\(?\u0026lt;username\u0026gt;[^\\\\]+)\\\\\u0026#39;){ $Username = $Matches[\u0026#39;username\u0026#39;] }else{ $Username = \u0026#39;\u0026#39; } # There can be more than one version installed. Get the latest one $LastestExtVersionInstallFolder = Get-ChildItem -Path $Folder.Fullname | Where-Object { $_.Name -match \u0026#39;^[0-9\\._-]+$\u0026#39; } | Sort-Object -Property CreationTime -Descending | Select-Object -First 1 -ExpandProperty Name # Get the version from the JSON manifest if (Test-Path -Path \u0026#34;$($Folder.Fullname)\\$LastestExtVersionInstallFolder\\Manifest.json\u0026#34;) { $Manifest = Get-Content -Path \u0026#34;$($Folder.Fullname)\\$LastestExtVersionInstallFolder\\Manifest.json\u0026#34; -Raw | ConvertFrom-Json if ($Manifest) { if (-not([string]::IsNullOrEmpty($Manifest.version))) { $Version = $Manifest.version } } } else { # Just use the folder name as the version $Version = $LastestExtVersionInstallFolder.Name } if($BuiltInExtensions.ContainsKey($ExtID)){ # Built-in extensions do not appear in the Chrome Store $Title = $BuiltInExtensions[$ExtID] $Description = \u0026#39;\u0026#39; }else{ # Lookup the extension in the Store $url = \u0026#34;https://chrome.google.com/webstore/detail/\u0026#34; + $ExtID + \u0026#34;?hl=en-us\u0026#34; try { # You may need to include proxy information # $WebRequest = Invoke-WebRequest -Uri $url -ErrorAction Stop -Proxy \u0026#39;http://proxy:port\u0026#39; -ProxyUseDefaultCredentials $WebRequest = Invoke-WebRequest -Uri $url -ErrorAction Stop if ($WebRequest.StatusCode -eq 200) { # Get the HTML Page Title but remove \u0026#39; - Chrome Web Store\u0026#39; if (-not([string]::IsNullOrEmpty($WebRequest.ParsedHtml.title))) { $ExtTitle = $WebRequest.ParsedHtml.title if ($ExtTitle -match \u0026#39;\\s-\\s.*$\u0026#39;) { $Title = $ExtTitle -replace \u0026#39;\\s-\\s.*$\u0026#39;,\u0026#39;\u0026#39; $extType = \u0026#39;ChromeStore\u0026#39; } else { $Title = $ExtTitle } } # Screen scrape the Description meta-data $Description = $webRequest.AllElements.InnerHTML | Where-Object { $_ -match \u0026#39;\u0026lt;meta name=\u0026#34;Description\u0026#34; content=\u0026#34;([^\u0026#34;]+)\u0026#34;\u0026gt;\u0026#39; } | Select-object -First 1 | ForEach-Object { $Matches[1] } } } catch { Write-Warning \u0026#34;Error during webstore lookup for \u0026#39;$ExtID\u0026#39; - \u0026#39;$_\u0026#39;\u0026#34; } } [PSCustomObject][Ordered]@{ Name = $Title Version = $Version Description = $Description Username = $Username ID = $ExtID } } }#End function #ENDREGION ----- $ExtensionFolderPath = \u0026#39;AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions\u0026#39; } PROCESS { Foreach ($Computer in $Computername) { if ($Username) { # Single userprofile $Path = Join-path -path \u0026#34;fileSystem::\\\\$Computer\\C$\\Users\\$Username\u0026#34; -ChildPath $ExtensionFolderPath $Extensions = Get-ChildItem -Path $Path -Directory -ErrorAction SilentlyContinue } else { # All user profiles that contain this a Chrome extensions folder $Path = Join-path -path \u0026#34;fileSystem::\\\\$Computer\\C$\\Users\\*\u0026#34; -ChildPath $ExtensionFolderPath $Extensions =@() Get-Item -Path $Path -ErrorAction SilentlyContinue | ForEach-Object{ $Extensions += Get-ChildItem -Path $_ -Directory -ErrorAction SilentlyContinue } } if (-not($null -eq $Extensions)) { Foreach ($Extension in $Extensions) { $Output = Get-ExtensionInfo -Folder $Extension $Output | Add-Member -MemberType NoteProperty -Name \u0026#39;Computername\u0026#39; -Value $Computer $Output } } else { Write-Warning \u0026#34;$Computer : no extensions were found\u0026#34; } }#foreach } } This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2018/12/15/audit-google-chrome-extensions/","summary":"\u003cp\u003eThis post includes a PowerShell \u003cem\u003eGet-ChromeExtension\u003c/em\u003e script to list installed extensions on the local or remote computer.\u003c/p\u003e","title":"Audit Chrome Extensions with PowerShell"},{"content":"I live in the U.K. and work remotely on technology projects.\nI\u0026rsquo;ve been self-employed for many years, working as an I.T. consultant for Fortune 500 companies, mainly in the financial sector.\nDisclaimers: Views expressed are solely my own\nProduct reviews are based on real experience\nScripts and guides on this site should be tested in a non-production environment\nThere is no guarantee of accuracy\n","permalink":"https://write-verbose.com/about/","summary":"I live in the U.K. and work remotely on technology projects.\nI\u0026rsquo;ve been self-employed for many years, working as an I.T. consultant for Fortune 500 companies, mainly in the financial sector.\nDisclaimers: Views expressed are solely my own\nProduct reviews are based on real experience\nScripts and guides on this site should be tested in a non-production environment\nThere is no guarantee of accuracy","title":"WhoAmI"}]