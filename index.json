[{"content":"PSMDE is a PowerShell module providing interactive access to Device information, Advanced Hunting data and machine actions.\nModule Functions Connect-PSMDE Get-PSMDEDeviceInfo Get-PSMDELatestVersion Invoke-PSMDEAvScan Invoke-PSMDEAdvancedHunting Invoke-PSMDEIsolation Revoke-PSMDEIsolation Invoke-PSMDEFileQuarantine Save-PSMDESupportInfo Test-PSMDEMapsConnection Installation Create Azure App Create an Azure application to control authentication and authorization.\nA step by step process is available here The API permissions required are available in the Help text of each module function.\nInstall MSAL.PS PSMDE relies on another module for authentication. Install the MSAL.PS module from the PowerShell gallery.\nInstall-Module MSAL.PS -Scope CurrentUser Download PSMDE The PSMDE module is available from GitHub\nClick on Code \u0026gt; Download Zip\nUnblock the zip file and extract it to a folder in the $ENV:PSModule path\nMake sure you rename the root folder from PSMDE-master to PSMDE\nEdit the Public\\Connect-PSMDE.ps1 file to set your TenantID and ApplicationID\n# Connect-PSMDE before edit Param( [parameter()] [ValidateNotNullOrEmpty()] [String]$TenantID = \u0026#39;00000000-0000-0000-0000-TENANTID\u0026#39; , [parameter()] [ValidateNotNullOrEmpty()] [String]$ClientID = \u0026#39;00000000-0000-0000-0000-APPID\u0026#39; Usage Security Context Start PowerShell in the context of an account with access to Defender information i.e. a member of\nA built in reader role such as Global Reader or Security Reader A privileged role such as Global Admin, Security Opertator, Security Admin A custom role with delegated access to your tenant Interactive PowerShell Use the module interactively at PowerShell console and explore the module functions\nImport-Module PSMDE # Load the module Get-Command -Module PSMDE # List available commands Get-Help Get-PSMDEDeviceInfo # Get help on a command EXAMPLE 1 - Get device information The example below confirms the following for an endpoint:\nDefender is active and onboarded Engine and signatures are up-to-date Last scan times OS version and IP address information Logged-on users MDE alerts Vulnerabilities Import-Module PSMDE Connect-PSMDE -TenantID $TenantID -ClientID $AppID # Optional parameters. Set defaults in the Connect-PSMDE.ps1 file Get-PSMDEDeviceInfo -Computername PC12345 Computername : PC12345 osPlatform : Windows10 version : 22H2 osBuild : 19045 isPotentialDuplication : False machineTags : {MDEPilot} healthStatus : Active onboardingStatus : Onboarded defenderAvStatus : Updated exposureLevel : Medium riskScore : Medium avEngineVersion : 1.1.23080.2005 avSignatureVersion : 1.395.1403.0 avPlatformVersion : 4.18.23080.2006 avIsSignatureUpToDate : True avIsEngineUpToDate : True avIsPlatformUpToDate : True avSignatureDataRefreshTime : 27/08/2023 15:35:36 avSignatureDataRefreshTimeUTC : 27/08/2023 14:35:36 quickScanTime : 22/08/2023 03:16:00 quickScanTimeUTC : 22/08/2023 02:16:00 fullScanTime : fullScanTimeUTC : avmode : 0 LastSeen : 27/08/2023 14:53:08 LastSeenUTC : 27/08/2023 13:53:08 lastIpAddress : 192.168.1.140 lastExternalIpAddress : 100.19.112.28 managedBy : Intune loggedOnUsers : {@{id=RnD\\user1; lastSeen=27/08/2023 16:23:38; logonTypes=RemoteInteractive}, @{id=azuread\\admin1; lastSeen=27/08/2023 16:02:54; logonTypes=Interactive}} alertCount : 9 alerts : {@{serverity=Informational; alertCreationTime=2023-05-08T19:45:47.8359999Z; detectionSource=AutomatedInvestigation; category=SuspiciousActivity; threatName=; threatFamilyName=}, @{serverity=Informational; alertCreationTime=2023-08-15T21:55:56.9250938Z; detectionSource=WindowsDefenderAv; category=Malware; threatName=Virus:DOS/EICAR_Test_File; threatFamilyName=EICAR_Test_File}, @{serverity=Medium; alertCreationTime=2023-05-08T16:11:02.4388031Z; detectionSource=WindowsDefenderAv; category=SuspiciousActivity; threatName=Trojan:PowerShell/Powersploit.L; threatFamilyName=Powersploit}, @{serverity=Medium; alertCreationTime=2023-05-08T16:02:13.0120825Z; detectionSource=WindowsDefenderAtp; category=Execution; threatName=; threatFamilyName=}...} CVEs : {@{name=CVE-2023-33144; description=Visual Studio Code Spoofing Vulnerability; severity=Medium; publicExploit=False; firstDetected=2023-06-13T17:30:51Z}, @{name=CVE-2023-21779; description=Visual Studio Code Remote Code Execution Vulnerability; severity=High; publicExploit=False; firstDetected=2023-05-02T14:45:15Z}, @{name=CVE-2023-24893; description=Visual Studio Code Remote Code Execution Vulnerability; severity=High; publicExploit=False; firstDetected=2023-05-02T14:45:15Z}, @{name=CVE-2023-29338; description=Visual Studio Code Information Disclosure Vulnerability; severity=Medium; publicExploit=False; firstDetected=2023-06-08T10:30:35Z}} deviceid : c6a833d9a0da6ad439076368d1781e7940c49fef EXAMPLE 2: Scan computers based on Advanced Hunting Query results The example below achieves the following:\nRuns an Advanced Hunting query to find endpoints where a file has executed in the last 6 hours Triggers a full scan on those endpoints Invoke-PSMDEAdvancedHunting -Query @\u0026#39; DeviceProcessEvents | where Timestamp \u0026gt; ago(6h) | where ActionType == \u0026#34;ProcessCreated\u0026#34; | where SHA1 == \u0026#34;1bc5066ddf693fc034d6514618854e26a85fd0d1\u0026#34; | distinct DeviceName \u0026#39;@ | Invoke-PSMDEAvscan -ScanType Full Computername : PC123456 type : RunAntiVirusScan ScanType : Full status : Pending errorHResult : 0 requestor : admin@tenant.com requestorComment : Full scan initiated by PSMDE DeviceId : c6a833d9a0da6ad439056368d1681e7940c49fee EXAMPLE 3: Isolate computers The example below performs the following:\nRuns an Advanced Hunting query to find endpoints where PowerShell has communicated with a specific public IP Triggers full isolation of those endpoints Invoke-PSMDEAdvancedHunting -Query @\u0026#39; DeviceNetworkEvents | where Timestamp \u0026gt; ago (6h) | where InitiatingProcessFileName =~\u0026#34;PowerShell.exe\u0026#34; | where RemoteIP == \u0026#34;20.50.201.195\u0026#34; | distinct DeviceName \u0026#39;@ | Invoke-PSMDEIsolation computername : PC123456 isolationType : Full comment : PSMDE: Isolate device requestor : \u0026lt;admin@tenant.com\u0026gt; status : Pending deviceid : c6a833d9a0da6ad439056368d1681e7940c49fee EXAMPLE 4 The following example queries a Microsoft URI for the latest available version of Defender for Endpoint:\nGet-PSMDELatestVersion Engine Platform Signatures ------ -------- ---------- 1.1.23070.1005 4.18.23070.1004 1.395.1451.0 This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/08/27/PSMDE/","summary":"\u003cp\u003ePSMDE is a PowerShell module providing interactive access to Device information, Advanced Hunting data and machine actions.\u003c/p\u003e","title":"PSMDE - PowerShell Defender for Endpoint Module"},{"content":"The git filter option isn\u0026rsquo;t well documented, but its very useful for removing sensitive information you don\u0026rsquo;t want appearing in your public repo. This post provides an example of replacing the Azure TenantID and AppID with dummy values during the git commit process for a PowerShell script.\nBackground Best practice is to never hard code sensitive information in your scripts. Better strategies include:\nAccessing an encrypted vault at runtime (Azure KeyVault, CyberArk Privileged Access etc) Operator supplied information at runtime (for interactive scripts) Having said that, there is information that isn\u0026rsquo;t \u0026ldquo;secret\u0026rdquo; but is specific to an environment that needs to be removed before making code public.\nScenario I\u0026rsquo;ll use the PSMDE PowerShell module as an example.\nThis PowerShell module is used interactively to manage Defender for Endpoint from the command line. The code uses the delegated authentication of an Azure Enterprise Application to access the Defender Security API. It uses Authorization Code Flow with the following environment-specific information:\nValue Format Comment TenantID GUID string Specific to an Azure environment ClientID GUID string A.K.A ApplicationID. Specific to an Azure Enterprise App. Credentials Multiple options Username / Password supplied interactively in this case The Connect-SecurityCenter.ps1 script makes authentication simple by including default values for the TenantID and ClientID. The administrator just calls the script and is prompted for credentials with sufficient access.\nConnect-SecurityCenter.ps1 extract with default values for TenantID and ClientID (AppID):\nFunction Connect-SecurityCenter{ [CmdletBinding()] Param( [parameter()] [ValidateNotNullOrEmpty()] [String]$TenantID = \u0026#39;f8856e0c-f363-4583-9a2f-ef0208473d3c\u0026#39; , [parameter()] [ValidateNotNullOrEmpty()] [String]$ClientID = \u0026#39;7c0cc95f-d722-4533-94e2-92530f29c07c\u0026#39; ) ... Someone downloading the public code is expected to edit the default values for their own environment.\nSo how do we develop this module with our own variable defaults, but avoid committing them to the public repo?\nGit Filter Git can apply filter scripts to the code during the staging step to replace text. The working directory retains the default values, but the repo code has sanitised replacement text.\nA Clean filter script is applied when the code is staged to the git index. A Smudge filter script is applied at checkout - effectively the opposite direction. The steps below explain how to implement these filters.\nStep 1: Enable filtering in the git global config Update the git config to enable the filters as follows. This can be done in the per-repo git config, but I find its better as a global option\u0026hellip;\nCommand format:\ngit config \u0026ndash;global filter.\u0026lt;filter name\u0026gt;.\u0026lt;clean|smudge\u0026gt;\u0026lt;path to a script | inline command\u0026gt;\n# Commands to run git config --global filter.gitClean.clean \u0026#34;~/gitclean.sh\u0026#34; git config --global filter.gitClean.smudge \u0026#34;~/gitsmudge.sh\u0026#34; gitClean is the freeform name for the filter. You can create multiple filters with separate scripts. gitClean has a separate clean script (gitclean.sh) and smudge script (gitsmudge.sh) Both scripts are in the user\u0026rsquo;s home folder (~/ is c:\\users\\username on Windows) Step 2: Create the clean and smudge scripts Even on Windows git uses bash to execute scripts. You might be able to get PowerShell working, but I found it easier just to stick with bash. The built-in sed command can be used to replace text as follows:\nCommand format:\nsed -e \u0026ldquo;s/\u0026lt;pattern\u0026gt;/\u0026lt;replacement\u0026gt;/\u0026lt;flags\u0026gt;\u0026rdquo;\nThe text specified by \u0026lt;pattern\u0026gt; is replaced by the text in \u0026lt;replacement\u0026gt;. The \u0026ldquo;g\u0026rdquo; flag replaces all occurrences in the file.\nThe gitclean.sh script replaces the real tenant ID and application ID with dummy text and the gitsmudge.sh script reverses the replacement.\n#!/usr/bin/env bash # gitclean.sh TenantID=\u0026#34;f8856e0c-f363-4583-9a2f-ef0208473d3c\u0026#34; AppID=\u0026#34;7c0cc95f-d722-4533-94e2-92530f29c07c\u0026#34; sed -e \u0026#34;s/$TenantID/00000000-0000-0000-0000-TENANTID/g\u0026#34; -e \u0026#34;s/$AppID/00000000-0000-0000-0000-APPID/g\u0026#34; #!/usr/bin/env bash # gitsmudge.sh TenantID=\u0026#34;f8856e0c-f363-4583-9a2f-ef0208473d3c\u0026#34; AppID=\u0026#34;7c0cc95f-d722-4533-94e2-92530f29c07c\u0026#34; sed -e \u0026#34;s/00000000-0000-0000-0000-TENANTID/$TenantID/g\u0026#34; -e \u0026#34;s/00000000-0000-0000-0000-APPID/$AppID/g\u0026#34; On Windows the files should be saved in the c:\\users\\\u0026lt;username\u0026gt; folder.\nWatch out for unix line ending and encoding requirements - In Visual Studio Code:\nIf the status bar shows CRLF, double-click on it and change to LF If the status bar shows UTF-8 with BOM, double-click on it and change to UTF-8 Step 3: Specify the file extensions that use the filter Edit the git attributes file in the repo:\n.git\\info\\attributes\nIf the file doesn\u0026rsquo;t exist, create it as a new text file with no file extension.\nAdd a line with \u0026lt;file extension\u0026gt; filter=\u0026lt;filter name\u0026gt; as follows:\n*.ps1 filter=gitClean Step 4: Stage the files in the index The filter is applied at staging, but it won\u0026rsquo;t be immediately obvious that anything has happened. Add or update the file using git add:\ngit add public\\Connect-SecurityCenter.ps1 The command will complete as normal. The file in the working directory will still have the real TenantID and AppID, but the staged file will have the replacement dummy values. Commit the staged file to the local repo and push to update to the public remote repo.\ngit commit -m \u0026#34;Update to Connect-SecurityCenter.ps1\u0026#34; git push origin Check the GitHub repo to confirm the text replacement was successful.\nOn subsequent updates, git diff will confirm the text replacement is successful at staging, before commit. It won\u0026rsquo;t show any differences for these lines - indicating the dummy values are in-place.\ngit diff --staged Now that git filtering is in-place, you can edit and update the code without worrying that unwanted information is in the public domain.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/08/20/GitClean/","summary":"\u003cp\u003eThe git filter option isn\u0026rsquo;t well documented, but its \u003cstrong\u003every useful for removing sensitive information you don\u0026rsquo;t want appearing in your public repo\u003c/strong\u003e. This post provides an example of replacing the Azure TenantID and AppID with dummy values during the git commit process for a PowerShell script.\u003c/p\u003e","title":"Replace sensitive information before committing PowerShell scripts using Git Clean and Smudge Filters"},{"content":"This post cover the following:\nAn overview of the steps to create the parent virtual disk A script to automate creation of child VMs with a differencing disk Environment My environment is a Windows 11 host running Client Hyper-V. The VMs are running Windows 11 Enterprise (Eval).\nI\u0026rsquo;m using PowerShell 5.1 and haven\u0026rsquo;t tested on other versions of the OS or PowerShell.\nParent Disk creation The following is a brief overview of creating the base image that becomes the parent disk.\nCreate a Hyper-V Generation 2 VM Enable the TPM (Settings \u0026gt; Security) Install Windows 11 During the OOBE phase, select Other Signin Options \u0026gt; Domain Join Create a local user e.g. \u0026ldquo;Bob\u0026rdquo; Log on as Bob and configure the system and software NOTE: Install all software per machine or Sysprep will fail Enable the built-in Administrator account and set the password Log off from the Bob account and log on as Administrator Delete the local profile for Bob\u0026rsquo;s account as follows: $p = Get-WMIObject -Class Win32_UserProfile -Filter \u0026#34;LocalPath=\u0026#39;c:\\\\users\\\\Bob\u0026#39;\u0026#34; $p.Delete() $p.Dispose() Delete the local user account for Bob Run Sysprep as follows: c:\\windows\\system32\\sysprep\\sysprep.exe /generalize /oobe /mode:vm Delete the VM in the Hyper-V console (the disk will remain) Move the VHDX disk file to an appropriate location and set the NTFS properties to be read-only. PowerShell Script The script below creates a new VM with a Differencing Disk linked to the parent disk.\nThe main benefits of a Differencing Disk are:\nRapid deployment - only have to go through OOBE and you have a working OS Save disk space - the base OS is in the parent disk Example This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/08/13/HyperVDiff/","summary":"\u003cp\u003eThis post cover the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn overview of the steps to create the parent virtual disk\u003c/li\u003e\n\u003cli\u003eA script to automate creation of child VMs with a differencing disk\u003c/li\u003e\n\u003c/ul\u003e","title":"Create a Hyper-V VM with a differencing disk using PowerShell"},{"content":"Windows 11 has multiple virtualization features with similar names. This short post explains when to enable each one.\nIt\u0026rsquo;s not immediately clear which features need to be enabled on Windows 11 for local virtualisation support. The following features appear to overlap:\nHyper-V Virtual Machine Platform Windows Hypervisor Platform Firstly, where do you see these optional features on Windows 11?\nSettings \u0026gt; Apps \u0026gt; Optional Features \u0026gt; More Windows Features PowerShell Get-WindowsOptionalFeature -online | Out-Gridview Purpose Feature Purpose Hyper-V Host local VMs using client Hyper-V Virtual Machine Platform Pre-req for Windows Subsystem for Linux* Windows Hypervisor Platform API for 3rd party virtualisation e.g. Docker, VirtualBox *NOTE Virtual Machine Platform is automatically enabled if you use the command wsl \u0026ndash;install\nEnable You can enable the features you need as follows:\n# Hyper-V Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All # Virtual Machine Platform Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform # Windows Hypervisor Platform Enable-WindowsOptionalFeature -Online -FeatureName HypervisorPlatform This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/06/01/EnableWindows11HyperV/","summary":"\u003cp\u003eWindows 11 has multiple virtualization features with similar names. This short post explains when to enable each one.\u003c/p\u003e","title":"Hyper-V or Virtual Machine Platform or Windows Hypervisor Platform"},{"content":"Attackers can use Windows Firewall to block EDR telemetry leaving the endpoint. Read-on for how this is mitigated.\nRisk background As well as the usual source and destination variables, Windows Firewall can also block outbound communication based on the service name or program that initiates the communication.\nAn attacker with elevated endpoint access will want to shut down Defender EDR as soon as possible and one method is to block the client agent communication with the cloud service.\nThe following PowerShell commands can achieve this, blocking outbound TCP/443 from:\nWinDefend service (main MDAV process) SenseCNDProxy process (acts as a communication broker) MSSense process (Main MDE process) # PowerShell commands to Disable outbound 443 from MDE agent to cloud service New-NetFirewallRule -DisplayName \u0026#34;Block 443 MsMpEng\u0026#34; -Name \u0026#34;Block 443 MsMpEng\u0026#34; -Direction Outbound -Service WinDefend -Enabled True -RemotePort 443 -Protocol TCP -Action Block New-NetFirewallRule -DisplayName \u0026#34;Block 443 SenseCncProxy\u0026#34; -Name \u0026#34;Block 443 SenseCncProxy\u0026#34; -Direction Outbound -Program \u0026#34;%ProgramFiles%\\Windows Defender Advanced Threat Protection\\SenseCncProxy.exe\u0026#34; -RemotePort 443 -Protocol TCP -Action Block New-NetFirewallRule -DisplayName \u0026#34;Block 443 MsSense\u0026#34; -Name \u0026#34;Block 443 MsSense\u0026#34; -Direction Outbound -Program \u0026#34;%ProgramFiles%\\Windows Defender Advanced Threat Protection\\MsSense.exe\u0026#34; -RemotePort 443 -Protocol TCP -Action Block Mitigation 1 - Tamper Protection The most important mitigation is to enable MDE Tamper Protection.\nTamper protection prevents users with elevated rights on the endpoint making changes to the MDE client configuration. Tamper Projection includes prevention of local firewall rules affecting MDE processes. If you run the commands above, MDE will generate an alert and block the changes.\nEnable Tamper Protection as follows:\nMDE Security Portal \u0026gt; Settings \u0026gt; Endpoints \u0026gt; Advanced Features \u0026gt; Tamper Protection = On\n2- Firewall Rule Merging Windows Firewall local rule merging should be disabled to prevent local changes.\nThe Windows Firewall has Rule Sources. For example, Group Policy is one rule source and local rules are another. When the Firewall is managed by GPO or Intune, it still allows local rule merging by default.\nWhen rule merging is enabled, local block rules can over-ride policy-based allow rules. Similarly, local allow rules can override policy-based profile defaults (profile defaults are initial rules applied to each profile - Domain / Private / Public).\nDisable Local Rule Merging as follows:\nGroup Policy Editor:\nComputer Configuration \u0026gt; Security Settings \u0026gt; Windows Firewall with Advanced Security \u0026gt; Properties \u0026gt; Settings \u0026gt; [PROFILE NAME] \u0026gt; Settings \u0026gt; Apply local firewall rules = No\nIntune Policy:\nEndpoint security \u0026gt; Firewall \u0026gt; Create policy \u0026gt; Create a profile \u0026gt; Windows 10 / 11 / Server \u0026gt; Microsoft Defender Firewall \u0026gt; [Provide a Name] \u0026gt; Allow local policy merge = False\nSummary With Tamper Protection enabled and Windows Firewall local rule merging disabled, the chances of at attacker abusing firewall rules are slim.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2022/05/31/EDRBypass/","summary":"\u003cp\u003eAttackers can use Windows Firewall to block EDR telemetry leaving the endpoint.  Read-on for how this is mitigated.\u003c/p\u003e","title":"Bypassing Defender EDR using Windows Firewall - mitigations"},{"content":"This post is an introduction to accessing the Defender Security Center API in PowerShell using interactive authentication. It is the basis for building tools and scripts to enhance day-to-day productivity that I\u0026rsquo;ll explore in future posts.\nBackground The Defender Security API is a gold mine as it can provide access to key MDE features including:\nDevice information Alerts MDE Configuration Run Advanced Hunting queries Initiate Live Response and manage Library scripts Initiate Scans and Isolation I struggled to find examples of using the API interactively. Microsoft articles have examples using a client secret, which is fine for automation, but not ideal for interactive use. The following steps demonstrate interactive access to the API.\nAzure App The first step is to create an Azure Application to control the permissions available and limit admin access to the API.\nCreate the App In the Azure Portal, go to the App Registrations blade and select New Registration\nEnter a name for the app e.g. DefenderSecurityAPI (visible in MFA challenges and sign-in logs)\nLeave the Supported Account Types on the default Single Tenant\nDo not configure a Redirect URL at this stage.\nSelect Register to create the app.\nYou are presented with the App Overview page. Select Add a redirect URI, then Add a Platform\nSelect Mobile and Desktop applications\nEnable the check box next to the nativeclient URL and also add local host in the Custom Redirect URIs:\nhttps://login.microsoftonline.com/common/oauth2/nativeclient http://localhost Select Configure to save the redirect URLs\nNext select API Permissions in the left pane, then Add a permission\nSelect the APIs my organization uses tab and then use the search box to find WindowsDefenderATP\nSelect Delegated Permissions\nExpand the categories and select the individual permissions required\ne.g. Alert.Read, AdvancedQuery.Read, Machine.Read, Machine.Scan, Machine.RestrictExecution, Machine.Isolate, Machine.CollectForensics, Machine.Offboard, Software.Read\nOnce you\u0026rsquo;ve finished adding permissions, select Grant admin consent for tenant_name\nDelegated permissions mean both the user and the app must have the required permission. The app won\u0026rsquo;t elevate privileges.\nRestrict Access to the app To restrict who can use the app, we need to go to the Enterprise applications blade, either using the Azure Portal search box, or by navigating to the root of Azure AD and selecting it in the left pane.\nSelect the app in the list to open its Overview page, then select Properties in the left pane.\nChange Assignment Required to Yes and then Save\nSelect Users and groups in the left pane and then select Add user/group\nSelect an existing group that will be allowed to authenticate using the App.\nNOTE: Although we consented to API permissions in the previous step, it was only delegated permissions. The group members must also be granted permission to read or change Security assets e.g. through a built-in Role, such as Security Administrator.\nSelect the Overview link in the left pane and copy the Application ID as this will be a script variable along with the TenantID.\nAuthenticate using MSAL.PS Use the MSAL.PS PowerShell module to authenticate interactively and cache an access token. The important part is to specify a security API scope.\n# Pre-req Install-Module -Name MSAL.PS -MinimumVersion 4.37.0.0 # Replace with your Application and Tenant IDs $ApplicationID = \u0026#39;446e4714-3226-489e-b602-1515d6822e09\u0026#39; $TenantID = \u0026#39;b0530087-fc9a-44e7-ad6f-1b045d56e15d\u0026#39; # Specify at least one security center API scope $Scopes = @(\u0026#34;https://api.securitycenter.microsoft.com/Machine.Read\u0026#34;) # Authenticate interactively with authorization code flow $AccessToken = Get-MsalToken -ClientId $ApplicationID -TenantId $TenantID -Scopes $Scopes If authentication fails with a message \u0026ldquo;You can\u0026rsquo;t get there from here\u0026rdquo;, see the Conditional Access section below.\nCall the API With an access token cached, its straight forward to call the Security Center API using Invoke-Restmethod. The following is a basic example of getting device information:\n# Build the authentication header $AuthenticationHeader = @{ \u0026#34;Content-Type\u0026#34; = \u0026#34;application/json\u0026#34; \u0026#34;Authorization\u0026#34; = $AccessToken.CreateAuthorizationHeader() \u0026#34;ExpiresOn\u0026#34; = $AccessToken.ExpiresOn.UTCDateTime } # Example getting details of an MDE client device # The deviceID is available on the device details blade in the Security Center portal $Method = \u0026#39;Get\u0026#39; $MachineID = \u0026#39;c6a833d8a0da6ad439076368d1681e7930c49fef\u0026#39; $URI = \u0026#34;https://api.securitycenter.microsoft.com/api/machines/$MachineID\u0026#34; Invoke-RestMethod -Uri $URI -Headers $AuthenticationHeader -Method $Method Example output:\nConditional Access There\u0026rsquo;s an automatically created Conditional Access policy that must be modified before you can query the API. If authentication fails with the error below, you need to edit the policy.\nThe CA policy gets created during Defender for Endpoint setup, when MDE is linked to Intune. Its only visible in the CA Classic view:\nAzure Active Directory \u0026gt; Security \u0026gt; Conditional Access \u0026gt; Classic Policies \u0026gt; [Windows Defender ATP] Device Policy\nDon\u0026rsquo;t delete or disable the policy. It could have undesirable results as indicated in this Microsoft article. Instead, add a group exclusion to the policy for the same group authorised to use the Azure App.\nSummary This article is an introduction to accessing the Defender Security Center API using PowerShell with interactive authentication. Future articles will explore practical use cases.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/05/24/DefenderSecurityAPI/","summary":"\u003cp\u003eThis post is an introduction to accessing the Defender Security Center API in PowerShell using interactive authentication. It is the basis for building tools and scripts to enhance day-to-day productivity that I\u0026rsquo;ll explore in future posts.\u003c/p\u003e","title":"Access api.securitycenter.microsoft.com interactively with PowerShell"},{"content":"The recent incident of malicious extensions in the Visual Studio Code Marketplace got me thinking about how to audit extensions across a large estate. This post includes a script to get installed extensions on a local or remote computer.\nYou\u0026rsquo;re probably aware of the recent discovery of malicious extensions in the VSCode Marketplace.\n\u0026lsquo;Theme Darcula dark\u0026rsquo; – Described as \u0026ldquo;an attempt to improve Dracula colors consistency on VS Code,\u0026rdquo; this extension was used to steal basic information about the developer\u0026rsquo;s system\u0026hellip;downloaded over 45,000 times\n\u0026lsquo;python-vscode\u0026rsquo; – This extension was downloaded 1,384 times despite its empty description and uploader name of \u0026rsquo;testUseracc1111,\u0026rsquo;. Analysis of its code showed that it is a C# shell injector that can execute code or commands on the victim\u0026rsquo;s machine.\nVSCode extensions are installed in the user profile and do not appear in Configuration Manager or Intune inventory. Although Defender for Endpoint is able to centrally report on installed Edge browser extensions in the user profile, this does not extend to VSCode.\nEnterprise reporting The script below is just the first part of a solution to audit installed VSCode extensions. Suggestions to capture information across a large estate include:\nUse PoshRSJob or PSThreadJob to rapidly query online computers over the network Save the output to the local registry and use RegKeytoMof to bring it into the Configuration Manager inventory Save the output to a file or registry key and upload to LogAnalytics Example script output The VSCode extension script below is similar to the Chrome Extension script published some time ago.\nThis is what the output looks like:\nC:\\\u0026gt; Get-VSCodeExt ExtensionID : eliostruyf.vscode-msgraph-autocomplete Version : 1.2.0 Publisher : Elio Struyf Path : c:/Users/gd/.vscode/extensions/eliostruyf.vscode-msgraph-autocomplete-1.2.0 MarketPlaceURL : https://marketplace.visualstudio.com/items?itemName=eliostruyf.vscode-msgraph-autocomplete MarketPlaceInstalls : 4359 Source : https://github.com/estruyf/vscode-msgraph-autocomplete ExtensionID : redhat.vscode-yaml Version : 1.12.2 Publisher : Red Hat Path : c:/Users/gd/.vscode/extensions/redhat.vscode-yaml-1.12.2 MarketPlaceURL : https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml MarketPlaceInstalls : 12120909 Source : https://github.com/redhat-developer/vscode-yaml.git PowerShell Script Get-VSCodeExts.ps1:\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/05/19/AuditVSCodeExt/","summary":"\u003cp\u003eThe recent incident of malicious extensions in the Visual Studio Code Marketplace got me thinking about how to audit extensions across a large estate. This post includes a script to get installed extensions on a local or remote computer.\u003c/p\u003e","title":"Audit Installed VSCode Extensions with PowerShell"},{"content":"Part three of a deep dive series on Purview Sensitivity Labels:\nPart 1 - Manual Labelling\nPart 2 - Automatic Labelling\nPart 3 - Recommendations and Limitations\nLabelling Recommendations Data Classification requires broad organizational support to be successful. The following design recommendations should help with implementation\u0026hellip;\nKeep the number of labels to a minimum\nLabelling needs to be simple and the options clearly defined, otherwise adoption will suffer\nCreate separate labels for Items and Containers\nItem and Container Labels apply different controls. Separate labels means the name and description can be clearer. For example, Confidential may be suitable for documents, but it would be less clear for M365 Groups.\nCreate separate labels for Files, Emails and Meetings\nThere are sub-scopes within Items for Files, Emails and Meetings (Calendar events). Again, the controls are different so labels can be more specific if targeted. For example, email labels can control view, reply, forwarding, whereas file labels can control Save, Print, Copy, Expiry, Offline Access.\nIt helps that client apps only show labels applicable to the current scope, so a label scoped to Items - Email will not be displayed in a document, spreadsheet or presentation.\nCreate a baseline set of company-wide labels and a small number of targeted additions\nA core set of labels should be applicable to everyone. Specific departments or teams may need extra labels for custom data, such as PII, or Blueprints. Label policies can deploy the extra labels just to specific AAD groups.\nCreate labels that will endure over a long period\nChoose labels that will still be applicable in years to come and then consult users (and perhaps run a limited pilot) to get labels right before mass adoption. Deleting labels is problematic. The label is added to the metadata of items and deleting the label does not remove it from the metadata. The recommended way to retire a label is to stop publishing it (label policy), but don\u0026rsquo;t delete the label itself.\nAvoid sub labels\nThere are some quirks with sub labels and they don\u0026rsquo;t add much value.\nUse same text for the Label Name and Display Name\nThe Label Name is used internally and the Display Name is visible to users. Making them different is just adding complexity. The label is actually assigned a GUID that must be used in advanced administrator operations anyway.\nDon\u0026rsquo;t apply policy controls straight-away\nConsider publishing Labels and using them without controls initially. You can then use the Data Classification Content Explorer and Activity Explorer to review how Labels are being used before applying that knowledge to policy creation.\nUse file encryption to protect intellectual property\nEmployee-only content can be encrypted with Co-Author permissions for All users and groups in your organization. Leavers who attempt to take protected data with them will be denied access when their account is disabled. Protected Data sent or copied to a third party will be inaccessible.\nCreate an FAQ and include the link in the Label Policy Users will need reassurance to feel confident in applying labels with content restrictions. There will also be edge cases that need more information. Label Policies include an option to deploy a URL help link.\nCheck your Service and Tenant settings Depending on the age of your Tenant, the following may already be enabled\u0026hellip;\nEnable sensitivity labels for Office files in SharePoint and OneDrive Data Classification support is not enabled by default in SharePoint. The following features are turned on when enabled:\nDefault Sensitivity Label option is available for Document Libraries Sensitivity column can be added to Library views Encrypted document can be indexed and returned in search results Data classification reports can show documents by Label in SharePoint and OneDirve Auto-labelling policies can apply to data at reset in SharePoint and OneDrive PS C:\\\u0026gt; Install-module -Name Microsoft.Online.SharePoint.PowerShell PS C:\\\u0026gt; Connect-SPOService -Url \u0026#34;https://$($tenant)-admin.sharepoint.com\u0026#34; PS C:\\\u0026gt; Get-SPOTenant | Select-Object EnableAIPIntegration EnableAIPIntegration -------------------- False PS C:\\\u0026gt; Set-SPOTenant -EnableAIPIntegration $True Enable co-authoring for files encrypted with sensitivity labels Check that document co-authoring is enabled in the Tenant. Older versions of the Information Protection SDK (before v1.7) did not support co-authoring or Autosave. In version 1.7, changes were made to encrypted document metadata to enable these features.\nPS C:\\\u0026gt; Instal-module -Name ExchangeOnlineManagement PS C:\\\u0026gt; Connect-IPPSSession PS C:\\\u0026gt; get-PolicyConfig | Select EnableLabelCoauth EnableLabelCoauth ------------------ False PS C:\\\u0026gt; Set-PolicyConfig -EnableLabelCoauth $True Enable cross-Tenant access for encrypted files By default, a Tenant will accept B2B authentication from other Azure AD Tenants, but if users receive error messages, it\u0026rsquo;s possible these settings have been restricted by an Administrator.\nFor example, on attempting to authenticate and decrypt a document the user may see one of the following messages, depending on whether B2B authentication has been restricted in the source organization (inbound) or the receiving organization (outbound)\nYour tenant administrator has restricted which organizations can be accessed. Contact your IT department to request access to the Wingtiptoys.com organization\nThe Contoso.com administrator has restricted which organizations can access their tenant. Contact the Contoso.com IT department to request access\nPS C:\\\u0026gt; Import-Module Microsoft.Graph.Identity.SignIns PS C:\\\u0026gt; Connect-MGGraph # INBOUND PS C:\\\u0026gt; Get-MgPolicyCrossTenantAccessPolicyDefault | select -ExpandProperty B2BCollaborationInbound | select -ExpandProperty UsersAndGroups AccessType ---------- allowed PS C:\\\u0026gt; Get-MgPolicyCrossTenantAccessPolicyDefault | select -ExpandProperty B2BCollaborationInbound | select -ExpandProperty Applications AccessType ---------- allowed # OUTBOUND PS C:\\\u0026gt; Get-MgPolicyCrossTenantAccessPolicyDefault | select -ExpandProperty B2BCollaborationOutbound | select -ExpandProperty UsersAndGroups AccessType ---------- allowed PS C:\\\u0026gt; Get-MgPolicyCrossTenantAccessPolicyDefault | select -ExpandProperty B2BCollaborationOutbound | select -ExpandProperty Applications AccessType ---------- allowed If needed, the settings can be modified in the Azure AD portal, External Identities \u0026gt; Cross-tenant access settings \u0026gt; Default settings, or using the Update-MgPolicyCrossTenantAccessPolicyDefault command.\nExchange Online: IRM Configuration The following features are dependent on Information Rights Management Licensing in Exchange Online:\nMessage Encryption using mail flow rules Encryption using DLP policies Support for sensitivity labels with encryption using Outlook on the Web, Mac, iOS and Android Auto-labelling policies in Exchange with encryption Ensure Azure RMS licensing is enabled as follows:\nPS C:\\\u0026gt; Instal-module -Name ExchangeOnlineManagement PS C:\\\u0026gt; Connect-ExchangeOnline PS C:\\\u0026gt; Get-IRMConfiguration | Select AzureRMSLicensingEnabled AzureRMSLicensingEnabled ------------------------- False Set-IRMConfiguration -AzureRMSLicensingEnabled $True Multi-language support Sensitivity Labels and their descriptions can be localised to match the Office language. Local language support can only be added using the Set-Language PowerShell command.\nSet-Label is available in the Security and Compliance PowerShell module (available after using Connect-IPPSSession from the ExchangeOnlineManagement PowerShell module).\nYou may see references to adding multi-language support using an XML export and import process in the Azure Information Protection blade of the Azure Portal. This method relates to AIP Classic that is now deprecated.\nLimitations Supported Office versions Sensitivity Labels are not supported in the Office Desktop Perpetual Editions (standalone). In addition, a subscription version of Office must be:\nSemi-annual Enterprise 2002+ Monthly Enterprise and Current Channel 1910+ There are some features that require later versions, such as Dynamic content marking, Let users assign permissions and audit label-related user activity.\nUser encryption over-ride Users are able to over-ride the encryption settings applied by a manual label. After they apply the label to their content, they can make changes to the protection using File \u0026gt; Info \u0026gt; Protect Document \u0026gt; Restrict Access.\nThe only way to avoid this is to provide at least one Label with Let users assign their own permissions and educate users to choose this Label rather than modifying a pre-configured one.\nLost Email Labels Labelled emails can lose their Label when a reply comes from an external organization that doesn\u0026rsquo;t use Outlook. Any encryption is retained, but the original Label will be removed.\n\u0026ldquo;Encrypt with Password\u0026rdquo; missing in PDF If a PDF is encrypted with Information Protection, the Adobe option to Encrypt with Password is no longer available.\nLimits on SharePoint auto-labelling Service-side Auto-Labelling for SharePoint and OneDrive has some limitations when scanning data at rest:\nMaximum of 25,000 files labelled per day Maximum of 100 Auto-label policies per Tenant Maximum of 100 Sites (SPO or OneDrive) when targeting individual Sites in a policy (alternative is to target All Sites) Limits on SharePoint indexing SharePoint can\u0026rsquo;t index encrypted files that have any of the following:\nExpiring access Double-key encryption This affects search, preview and e-Discovery.\nUpdate delays Updates to Labels can take a few hours to apply and be visible to users. Similarly, there is often a delay with publishing Label Policies.\nSharePoint caches Labels and Label Policies, so changes can take even longer e.g. 24hrs.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/04/26/sensitivitylabelspt2/","summary":"\u003cp\u003ePart three of a deep dive series on Purview Sensitivity Labels:\u003c/p\u003e\n\u003cp\u003ePart 1 - \u003ca href=\"/2023/04/24/sensitivitylabelspt1/\"\u003eManual Labelling\u003c/a\u003e\u003cbr\u003e\nPart 2 - \u003ca href=\"/2023/04/25/sensitivitylabelspt2/\"\u003eAutomatic Labelling\u003c/a\u003e\u003cbr\u003e\nPart 3 - Recommendations and Limitations\u003c/p\u003e","title":"Purview Information Protection Deep Dive Pt3 - Recommendations and Limitations"},{"content":"Part two of a deep dive series on Purview Sensitivity Labels:\nPart 1 - Manual Labelling\nPart 2 - Automatic Labelling\nPart 3 - Recommendations and Limitations\nAutomatic Labelling Manual Labelling is often the introduction to Sensitivity Labels, but eventually you will want to look at automation. In particular, it will be needed to add coverage of data at rest. Manual labelling, even if mandatory, only applies when an existing file is re-saved.\nThe main blocker for many companies is that Automatic Labelling is not available with an E3 license. It requires either Microsoft 365 E5, or E3 plus Microsoft 365 E5 Compliance or Microsoft 365 E5 Information Protection and Governance.\nThere are two options for automatically applying Sensitivity Labels:\nClient-side auto-labelling for files and emails Service-side auto-labelling for SharePoint Online, Exchange Online and OneDrive Client-side Auto-labelling Auto-labelling for files and emails is a client option, configured per-label, that either automatically applies the label, or recommends it to the user based on pattern matching in the content.\nClient-side auto-labelling only occurs when an item is being created or edited, it doesn\u0026rsquo;t apply to data at rest or in-transit. The auto-label policy is based on matching one or more Sensitive Info Types or Trainable Classifiers. For example, if the content matches a passport number.\nService-side auto-labelling Service-side auto-labelling is a separate option in the Information Protection blade of the Purview Compliance Center. This option is not dependent on supported client apps or reliant on user adoption. Labels are applied by the back-end M365 services.\nCreate an Auto-labelling policy to apply one of the pre-defined labels to unlabelled data at rest in supported locations.\nThe policy wizard has pre-defined Templates for matching content based on well-known regulatory and enterprise requirements. For example, the UK Financial Template includes the Sensitive Info Types that match credit card numbers, debit card numbers and SWIFT bank codes in the scanned content.\nAlternatively, the Custom Template option creates a policy with bespoke pattern matching, as follows\u0026hellip;\nFirst select one or more of the supported storage locations to apply the automatic labelling:\nExchange Online SharePoint Online OneDrive for Business SharePoint and OneDrive both support auto-labelling of data at rest. Exchange Online only supports auto-labelling of data in-transit.\nNext create rules containing include and exclude Conditions to identify items in-scope. A rule can contain multiple conditions and it can apply to all three storage locations, or be specific to a particular one.\nConditions available for SharePoint Online and OneDrive for Business:\nCondition Description Content is shared Inside or Outside the organisation Content Contains Sensitive Info Types and/or Trainable Classifiers Exchange Online has many additional Conditions:\nCondition Description Content is shared Inside or Outside the organisation Recipient Domain is List of email domains Recipient is Specific email addresses Sender IP Address Specific IPv4 address or a range Sender domain is List of email domains Sender is Specific email addresses Attachment file extension is List of file extensions Attachment is password protected Detects email attachments with password protection Attachment\u0026rsquo;s content could not be scannedAttachment\u0026rsquo;s content didn\u0026rsquo;t complete scanning Detects emails with attachments that can\u0026rsquo;t be scanned Header matches pattern Regex match on portion of email header Subject matches pattern Regex match on email subject Recipient address contains wordsRecipient address matches pattern List of words in recipient address or match to regex pattern Sender address matches wordsSender address matches pattern List of words in sender address or match to regex pattern Content Contains Sensitive Info Types and/or Trainable Classifiers The additional conditions are only available for Exchange when creating a per-location rule. Simulation mode is a mandatory step in creating an auto-labelling policy. Content discovery takes place and the administrator can review matched items to ensure rules are correct. The policy can then be turned-on for real.\nSensitive Information Types Sensitive information Types are search patterns for named data types. They are used to automatically classify content that matches the pattern, for example:\nUS / UK Passport number SWIFT code Japan Social Security Number Credit Card Number Azure AD client secret Sensitive Info Types can be used in Client-side and Service-side auto-labelling.\nThere is a long list of built-in Sensitive Information Types (SIT) provided by Microsoft. You can also create your own in the \u0026lt;em\u0026gt;Data Classification\u0026lt;/em\u0026gt; blade of the Purview Compliance Center. Custom SITs are one of the following:\nPattern-based SIT When you create a pattern-based Sensitive Info Type, you can specify a Primary element and Supporting elements. The elements consist of the following options:\nRegular Expression Keyword list Keyword dictionary (longer list of keywords) Existing Sensitive Info type The Secondary Element can be a list of the above, grouped by Any, All or None. The Primary Element and Supporting Elements can be anywhere in the document or near each other within a specified number of characters.\nFingerprint-based SIT A Fingerprint-based Sensitive Info Type is created by uploading an example document. It works best with Forms and Templates.\nTrainable Classifiers This method of automatic-labelling is based on machine learning. A Classifier is trained to recognise a document through examples. The classifier must first be fed a selection of documents that are in-scope and out-of-scope of the required label. The administrator must confirm or reject the automatic classification to train the learning model. It can then be applied to a bulk repository such as a document library.\nMicrosoft provides many \u0026amp;ldquo;ready-to-use\u0026amp;rdquo; classifiers, including ones to detect profanity, threats and discrimination and even resumes (CVs). Creating a custom Trainable Classifier requires at least 50 sample documents. It can take up-to 2 weeks to scan your environment using these classifiers.\nTrainable Classifiers can be used in Client-side and Service-side auto-labelling. They can also be used to apply Retention Labels.\nExchange Mail Flow Rules Encryption can be applied using Labels in Exchange Mail Flow rules. This option may be an attractive alternative if you don\u0026rsquo;t have an E5 license that supports Automatic Label Policies. The Label and associated encryption is applied to messages in transit (not messages at rest in mailboxes).\nCreate a Rule as follows\u0026hellip;\nIn the Exchange Admin Portal, select Mail Flow \u0026gt; Rules \u0026gt; Add a rule\nIn the drop-down list select the option to Apply Office 365 Message Encryption and rights protection to messages\nComplete the Rule Conditions and select Rights protect message with [SELECTED LABEL]\nSharePoint Default Sensitivity Label You can set a default Sensitivity Label for SharePoint content and it will apply to documents uploaded or re-saved in a Library. The setting can be applied at the Site Level (Settings \u0026gt; Site Information) or the Library Level (Settings \u0026gt; Library Settings). It requires Site Admin permissions.\nA manually-applied label will always win over a SPO Library default. The Library setting can override an auto-labelling policy or label policy default setting, if the Library setting is higher priority.\nA SharePoint default Label only applies to new document uploads. Existing files in the Library only receive the Label when they are re-saved.\nLabels with the following settings can\u0026rsquo;t be used as a SharePoint default label:\nA Label with encryption set to Let users assign permissions when they apply the label A Label set to In Word, PowerPoint and Excel, prompt users to specify permissions If a user manually removes encryption from a labelled document in a SharePoint Library, the encryption will be restored the next time it is accessed or downloaded.\nThe automatic Labelling process does not affect the Last Modified date of files.\nNOTE that an SPO Default Label is different to Label Policies that target SharePoint. Label Policies apply container settings such as external access.\nSharePoint Sensitivity column Don\u0026rsquo;t forget to update the All Documents view to show a column for Sensitivity\nIn a Library \u0026gt; Add column \u0026gt; Show or hide columns \u0026gt; Select Sensitivity \u0026gt; Apply Then click the All Documents drop-down and select Save view as, leave the default \u0026ldquo;All Documents\u0026rdquo; and click Save\nThe Library will then always show the Sensitivity Label alongside the File Name and Modified Date.\nProtecting Teams Meetings and Chat A Label can be applied to Teams meetings and Chat if the following are true:\nOrganization has a Teams Premium license (included in E5 but not E3) The Label is scoped to both Files and Emails The meeting owner is using M365 Apps for Enterprise or OWA on a Desktop computer (not supported on mobile apps) When encryption is turned-on, the following controls can be configured by the Label:\nWho can bypass the lobby Who can present, record Automatic recording Prevent copy of meeting chat Add a watermark during screen sharing and camera streams What is Azure Information Protection? Azure Information Protection (AIP) extends what is available in Purview Information Protection and also provides some of the services used by Purview Information Protection. In its original form AIP used separate Sensitivity Labels managed in the Azure Portal. Since 2019, AIP has been updated with a Unified Labelling Client that uses Purview Sensitivity Labels.\nThe following are some reasons to extend the Data Classification Framework with AIP:\nBulk labelling and protection of on-prem file shares and SharePoint libraries Bulk decryption for data recovery Apply labels directly from File Explorer or PowerShell Supports additional file types for classification and protection Supports Office 2003-2007 file formats Logging to the Windows event log To use AIP you will need to deploy the unified labelling client to Windows computers. You may see references in documentation to the AIP Classic Client. This is the older version of AIP that is now deprecated.\nIf the AIP client is installed, the built-in labelling interfaces in Office are disabled (e.g. the Ribbon Sensitivity button) and an Office add-in displays an information bar instead. It\u0026rsquo;s a supported scenario to use AIP for some features while still using the built-in labelling for Office. To achieve this, just disable the AIP Office add-ins (MSIP.WordAddin, MSIP.ExcelAddin, MSIP.PowerPointAddin, MSIP.OutlookAddin) using a Group policy / CSP setting.\nAIP Unified Labelling Scanner? The Scanner works by running jobs that crawl the specified data stores to label and protect the documents. This can be a one-off exercise to ensure wide coverage (then relying on native labelling options), or it could be a repeat scheduled process.\nThe Scanner runs as a service on a Windows Server (2016 or later for long path support) and also requires a SQL server backend. It requires an application registration in Azure AD, that provides a token for the service to authenticate with AIP. The scan jobs are configured in the AIP blade of Azure rather than on the Server.\nThe process of scanning large data stores can be time consuming, so the Scanner supports inclusion and exclusion by file type (file extension) to pre-filter the file list. It can also be clustered with multiple servers working in parallel. It uses the same built-in iFilters used by Windows Search to access document content and look for matches to automatic label patterns.\nThe Scanner can be run first in Discovery Mode to create a report of labels and protection that would be applied through automatic classification rules.\nAIP Supported file types for scanner inspection .doc, docx, .docm, .dot, .dotx, .xls, .xlt, .xlsx, .xlsm, .xlsb, .ppt, .pps, .pot, .pptx, .pdf, .txt, .xml, .csv This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/04/25/sensitivitylabelspt2/","summary":"\u003cp\u003ePart two of a deep dive series on Purview Sensitivity Labels:\u003c/p\u003e\n\u003cp\u003ePart 1 - \u003ca href=\"/2023/04/24/sensitivitylabelspt1/\"\u003eManual Labelling\u003c/a\u003e\u003cbr\u003e\nPart 2 - Automatic Labelling\u003cbr\u003e\nPart 3 - \u003ca href=\"/2023/04/26/sensitivitylabelspt2/\"\u003eRecommendations and Limitations\u003c/a\u003e\u003c/p\u003e","title":"Purview Information Protection Deep Dive Pt2 - Automatic Labelling"},{"content":"Part one of a deep dive series on Purview Sensitivity Labels:\nPart 1 - Manual Labelling\nPart 2 - Automatic Labelling\nPart 3 - Recommendations and Limitations\nPurview Information Protection (formerly AIP and then MIP) uses Labels to categorise data based on sensitivity and privacy requirements. Settings and Policies linked to the labels apply security controls. This article is a deep dive on Information Protection, including design and configuration.\nSensitivity Labels are metadata added to items and containers to identify and enforce privacy constraints using simple descriptions, such as Company Confidential or Secret.\nLabels and Label Policies are part of the M365 Data Classification Framework\nLabel Scope Applying Labels and Label Policies to Items is known as Information Protection.\nItems can be:\nFiles (Word, Excel, PowerPoint etc) Email messages Calendar events and meetings Teams meetings and chats (Teams Premium license required) M365 Apps (Office) files must be in the Open XML format e.g. docx, xlsx. Older formats such as .doc are not supported.\nApplying Labels and Policies to Containers is known as Container Management.\nContainers (a.k.a Groups \u0026amp; Sites) can be:\nSharePoint Online sites Teams M365 Groups Purview Data Map is a third lesser-used label scope, that allows Labels to be applied to some Azure assets such as:\nAzure SQL columns Azure Cosmos DB Azure MySQL Azure Data Explorer Purview Data Labelling is not enabled by default. It can be turned-on in the Information Protection UI, or using the PowerShell compliance module.\nPS C:\\\u0026gt; Install-Module ExchangeOnlineManagement PS C:\\\u0026gt; Connect-IPPSSession # Purview Labelling is disabled by default PS C:\\\u0026gt; Get-PolicyConfig | fl purview* PurviewLabelConsent : False PurviewLabelConsentCaller : PurviewLabelConsentTime : PurviewLabelConsentDetails : # Enable Purview Labelling: PS C:\\\u0026gt; Set-PolicyConfig -PurviewLabelConsent $True What\u0026rsquo;s in a name? Label names are freeform, configured by an administrator in the Purview Compliance Center. The label display name and description are visible to users and should be intuitive.\nItems and Containers can only have one Sensitivity Label (although they can also have an M365 Retention Label - a separate topic).\nExample Labels The following example is a baseline set of labels published to everyone in an organization:\nLABEL NAME Scope Description PUBLIC Items - Files Freely shared with anyone inside and outside the organization PARTNER ACCESSIBLE Items - Files Shared within the organization and with trusted partners EMPLOYEES ONLY Items - Files Only shared within the organization. Encrypted and limited to any member of the organization CONFIDENTIAL Items - Files Sensitive information. Encrypted with permissions set by the owner HIGHLY CONFIDENTIAL Items - Files Do not share - Regulated data such as PII, GDPR, HIPPA, or Company secret. Encrypted and limited to a predefined group BLUEPRINT Items - Files Company-specific design document. Encrypted ENCRYPTED Items - Email Encrypted NO FORWARD Items - Email Encrypted. Forwarding disabled OPEN ACCESS Groups \u0026amp; Sites Employees can join freely and invite external guests INVITE ONLY - External Groups \u0026amp; Sites Employees must be invited. Owners can invite external guests INVITE ONLY - Internal Groups \u0026amp; Sites Employees must be invited. No external guests Sub labels Sub labels are used to group similar labels in a parent-child relationship. They are mainly for organisation, there\u0026rsquo;s no inheritance of settings (although they do inherit the colour setting).\nIn the example below, Confidential is the parent label and Partners and Employees are the two sub labels.\nCONFIDENTIAL\nCONFIDENTIAL\\PARTNERS\nCONFIDENTIAL\\EMPLOYEES\nTo create a sub label, click the kebab menu (3-dots) next to an existing label and select Create Sublabel.\nManage Labels with PowerShell PS C:\\\u0026gt; Install-Module exchangeonlinemanagement -MinimumVersion 3.0.0 PS C:\\\u0026gt; Connect-IPPSSession # Create a new label PS C:\\\u0026gt; New-label -ContentType \u0026#34;File\u0026#34; -Name \u0026#34;EMPLOYEES ONLY\u0026#34; -DisplayName \u0026#34;EMPLOYEES ONLY\u0026#34; -Tooltip \u0026#34;Only shared within the organization. Encrypted\u0026#34; # Change label priority PS C:\\\u0026gt; Set-Label -Identity \u0026#34;EMPLOYEES ONLY\u0026#34; -Priority 2 Label Settings Sensitivity Labels aren\u0026rsquo;t just visual markings. They can apply permissions and privacy controls:\nControl Scope Description ENCRYPTION Items Usage Rights: View,Edit,Save,Print,Copy,Allow MacrosWhether access expiresIf it can accessed offline. CONTENT MARKING Items Background Watermark (DOCX/PPTX)HeaderFooter ENCRYPTION Email Messages Reply, Reply All, ForwardEncrypt only PRIVACY and EXTERNAL ACCESS Containers Membership: Public, Private, None(Defined by users)Ability to add guests EXTERNAL SHARING AND CONDITIONAL ACCESS Containers SharePoint external sharing options Label Priority Labels have a priority with (1) being least sensitive e.g. Public. The priority order is used to identify downgrading and to resolve conflicts.\nDowngrading occurs when users manually change a label to a less sensitive option. An Item Owner (person who applied the label) is free to change it.\nLabel Policies Label Policies have two purposes:\nPublishing labels so they are visible in supported apps Controlling how labels are applied Publishing Labels Sensitivity Labels are published to users, either everyone in the organization, or a selected [mail-enabled] group. This is different to Retention Labels that are published to storage locations.\nControlling how labels are applied The following options are available in a Label Policy, depending on the Label scope.\nControl Description Downgrade justification Prompt user for a reason when changing to a lower priority label or removing a label Mandatory labelling Users are forced to apply a label before saving (files) or sending (emails) if the item doesn\u0026rsquo;t already have a label Help Link A URL to a page with user guidance. The link will be available on the Sensitivity button in MS Office Mandatory Power BI labelling Not covered in this article Default Document Label Automatic client-side labelling in supported apps. Applied to new and modified items. Users can override Default Email Label Automatic client-side labelling in supported apps. Applied to new and existing items. Users can override Default Calendar Label Automatic client-side labelling in supported apps. Applied to new and existing meetings. Users can override Default Sites \u0026amp; Groups Label Applied to new SharePoint Sites and M365 Groups. Owners can override Default Power BI Label Applied to new dashboards, reports and datasets. Users can override. Default vs Mandatory Labelling User adoption is one of the key challenges of Sensitivity Labelling. The following options exist to help with this challenge:\nDefault Labels Mandatory Labelling Automatic Labelling It can be difficult to decide between default and mandatory labelling. If you configure default labels in the Label Polices, it will achieve the high-level goal of wide adoption, but it may fail to achieve a useful implementation. Users can just accept the default and make no effort to change it to something more appropriate.\nFor this reason, mandatory labelling may be a better choice, forcing users to choose a label when they save a new item or modify an unlabelled one. If you enable mandatory labelling, make sure the Label Display Names and Descriptions are intuitive and explain details like encryption.\nMandatory Labelling = Require users to apply a label to their emails and documents.\nEncryption, Permissions and Usage Rights Sensitivity Labels use AES 256 symmetric encryption to control access to items. Supported apps, such as Word, Excel, PowerPoint, will seamlessly decrypt the item if the recipient is authenticated and authorised. Encryption is not limited to users within the organization, external recipients are also supported.\nDocument metadata is not encrypted, allowing other applications to read the Sensitivity Labels on a document regardless of whether it is encrypted. Exchange Online and SharePoint (when enabled) are able to remove encryption and re-apply it, allowing them to index and scan content.\nThere are two aspects that control access to protected items:\nPermissions Usage Rights Assigning Permissions The following permission options control who can authenticate for access to a protected item:\nAssignee Description All users and groups in your organisation Any user in your Tenant Any authenticated user Users in any Azure AD Tenant, personal MSA accounts, Federated accounts (GMail, Yahoo), Guests, OTP authenticated B2B accounts Users or groups Specific users or groups in your Tenant Specific email addresses or domains Individual external recipients or entire external email domains External recipients are fully supported by the above options. If authentication fails the item cannot be opened.\nUsage Rights and Permission Levels Once the recipient is authenticated, the Publishing License attached to an encrypted item is checked to determine what usage rights have been assigned.\nUsage rights are granular controls over common activities as follows:\nRight Scope Description View content (VIEW) Files Enables document opening and viewing content. Doesn\u0026rsquo;t allow sorting or filtering in Excel View rights (VIEWRIGHTSDATA) Files Enables viewing the document protection policy Edit content (DOCEDIT) Files Enables document modify, rearrange, sort content. Not Save or Change Tracking Save (EDIT) Files Enables document Save and Save As (Open XML format only) Print (PRINT) Files Enables document printing Copy and extract content (EXTRACT) Files Enables document copy/paste and screen capture/sharing Reply (REPLY) Email Enables email reply, but not changes to recipients. Combine with Save and Edit content to work correctly Reply all (REPLYALL) Email Enables email reply all, but not changes to recipients. Combine with Save and Edit content to work correctly Forward (FORWARD) Email Enables email forward or adding recipients. Combine with Save and Edit content to work correctly Edit rights (EDITRIGHTSDATA) Files Enables changing the protection policy, including removing protection Export content (EXPORT) Files Enables document SaveAs to other formats and integrations such as Send to OneNote Allow macros (OBJMODEL) Files Enables run macros option in documents Full control (OWNER) Files Enabled all rights to a document including remove protection Caveats: Save (EDIT) only grants Save/Save As in the Desktop versions of Office, but it also grants Edit rights in Office for the Web.\nPermission Levels are groups of usage rights that meet the most common requirements, as follows:\nPermission Level Scope Description Viewer Files Open, Read, View rights, Allow macros Reviewer Files \u0026amp; Email Open, Read, Save, Edit content, View rightsReply, Reply All, Forward Co-Author Files \u0026amp; Email Open, Read, Save, Edit content, View rights, Copy, Export, PrintReply, Reply All, Forward Co-Owner Files \u0026amp; Email Open, Read, Save, Edit content, View rights, Copy, Export, Print, Change RightsReply, Reply All, Forward,Full Control Co-Authors and Co-Owners can Copy, Export and Print from documents. Co-Owners can change or remove document protection.\nPDF Support PDF files can be encrypted and Adobe Acrobat / Acrobat Reader will show a banner message to indicate the applied Sensitivity Label. Since June 2022 versions of Acrobat and Acrobat Reader no longer need a separate plugin.\nMS Edge version 83.0.478.37 or higher can also view encrypted PDF files.\nIf you Save or Print an encrypted Office document to PDF, the resulting PDF will inherit the encryption and permissions.\nContent Marking Content Marking puts visual markers directly in the data, specifically, the header, footer or background of a file item and header or footer of an email.\nThis can be useful as the label is hidden in the item metadata and only displayed in the UI of supported applications. Furthermore, the label only displays in the UI if the Tenant ID in the metadata matches the current Tenant.\nIn practice, content marking may be as simple as ensuring the Label is clearly visible in the document, or it could be a lengthy paragraph explaining who is allowed to read the content and how it can be shared. Headers and Footers can be up-to 1024 characters in Word and PowerPoint and up to 255 in Excel.\nBackground Watermarks are not supported in email and are normally only be used for high sensitivity documents e.g. to print Confidential diagonally in the background.\nDynamic Content Marking Content Marking supports basic conditions and variables, most commonly used to apply different marking based on the document type.\nAn IF condition can include the name of an Office application, or the single character abbreviation for multiple applications, as shown in the Header marking examples below:\n# LABEL POLICY EXAMPLE 1 - Only mark Excel files ${If.App.Excel} CONFIDENTIAL SPREADSHEET: INTERNAL RECIPIENTS ONLY ${If.End} Even though the label applies to Word, Excel and PowerPoint files, the header marking is only applied to Excel files\n# LABEL POLICY EXAMPLE 2 - Target multiple apps ${If.App.WXP} PUBLIC: DISTRIBUTE FREELY ${If.End} In the second example, the header marking is added to Word, Excel and PowerPoint files, but not Outlook emails.\n# LABEL POLICY EXAMPLE - multiple options ${If.App.WO} ${Item.Label} - This content is confidential. ${If.End} ${If.App.PowerPoint} ${Item.Label} - This presentation is confidential. ${If.End} In the third example, the header is different for Word and Outlook compared to PowerPoint (and Excel does not apply a header).\nDynamic Content Variables There are only a few variables that can be used in content marking:\nVariable Description ${Item.Label} The label display name ${Item.Name} File name or email subject of the document ${Item.Location} Full path to document or email subject ${User.Name} Display name of user applying the label ${User.PrincipalName UPN of user applying the label ${Event.DateTime} DateTime when label is applied Information Protection Licensing Office 365 E3 includes Information Protection for Office 365 – Standard and Office 365 E5 includes Information Protection for Office 365 – Premium. Broadly these map to manual and automatic labelling. Information Protection is a separate Service Plan with a SKU, allowing it to be selectively enabled for specific users / groups.\nAn O365 E3 license covers most requirements for manual labelling:\nManual labelling (Sensitivity Labels and Retention Labels) using M365 Apps for Enterprise, Office Online and Office mobile Manual labelling for containers (Teams / SharePoint / M365 Groups) Basic Office Message Encryption (OME) including Encrypt only and Do Not Forward email templates DLP for Exchange Online and SharePoint Online NOTES:\nDLP for Teams requires an Office E5 license. Administrators and M365 Group Owners need an Azure AD P1 license to apply labels to containers (included in E3 and E5).\nAn Office E5 license is needed for most automatic labelling actions and the following:\nAutomatically applying Sensitivity Labels and Retention Labels Default container labels inherited by unlabelled documents (e.g. SharePoint) Bring Your Own Key (BYOK) and Hold Your Own Key (HYOK) encryption Double Key Encryption (DKE) Sensitivity Labels for PowerBI NOTES:\nAlternative licensing - add one of the following to an E3 license: Microsoft 365 E5 Compliance, Microsoft 365 E5 Information Protection and Governance, Advanced OME and customer key for Office 365. Refer to the downloadable spreadsheet Microsoft 365 Compliance Licensing Comparison for more information.\nRole Based Admin Delegation M365 administrators working with Information Protection need delegated access to the configuration settings in the Purview Compliance Center. Add users to one of the following built-in roles:\nGlobal Administrator Compliance Data Administrator Compliance Administrator Security Administrator Alternatively, create a custom Azure AD role group and add one of the following:\nOrganization Configuration Sensitivity Label Administrator Sensitivity Label Reader To review Data Classification Reports (e.g. simulation mode results) you need:\nContent Explorer List viewer Content Explorer Content viewer This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/04/24/sensitivitylabelspt1/","summary":"\u003cp\u003ePart one of a deep dive series on Purview Sensitivity Labels:\u003c/p\u003e\n\u003cp\u003ePart 1 - Manual Labelling\u003cbr\u003e\nPart 2 - \u003ca href=\"/2023/04/25/sensitivitylabelspt2/\"\u003eAutomatic Labelling\u003c/a\u003e\u003cbr\u003e\nPart 3 - \u003ca href=\"/2023/04/26/sensitivitylabelspt2/\"\u003eRecommendations and Limitations\u003c/a\u003e\u003c/p\u003e","title":"Purview Information Protection Deep Dive Pt1 - Manual Labelling"},{"content":"Read-IMELog - A PowerShell script to read Intune Management Extension (IME) logs\nBackground The Intune Management Extension is a Windows client component responsible for running Intune scripts and installing Win32 apps. It creates log files in the following folder:\nC:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\nIME Log entries are in the CMTrace format used by System Center Configuration Manager e.g.\n\u0026lt;![LOG[[Win32App] Checking ESP status and phase for sessionId: 0]LOG]!\u0026gt;\u0026lt;time=\u0026#34;23:05:44.2390147\u0026#34; date=\u0026#34;4-16-2023\u0026#34; component=\u0026#34;IntuneManagementExtension\u0026#34; context=\u0026#34;\u0026#34; type=\u0026#34;1\u0026#34; thread=\u0026#34;65\u0026#34; file=\u0026#34;\u0026#34;\u0026gt; \u0026lt;![LOG[[Proxy Poller] Processing session id 2 starts]LOG]!\u0026gt;\u0026lt;time=\u0026#34;23:05:47.8411669\u0026#34; date=\u0026#34;4-16-2023\u0026#34; component=\u0026#34;IntuneManagementExtension\u0026#34; context=\u0026#34;\u0026#34; type=\u0026#34;1\u0026#34; thread=\u0026#34;25\u0026#34; file=\u0026#34;\u0026#34;\u0026gt; The log entries are best viewed using the CMTrace tool, but this won\u0026rsquo;t be readily available for a company that doesn\u0026rsquo;t use System Center Configuration Manager. There is no official download for cloud-only businesses using Intune.\nRead-IMELog is a PowerShell script that converts IME logs to PowerShell objects, allowing flexible filtering and sorting. It can also be used for Config Manager logs.\nExample usage 1 - filter on message text $IMELog = \u0026#39;C:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\\IntuneManagementExtension.log\u0026#39; Read-IMELog -Path $IMELog | where-object{$_.message -like \u0026#34;*ProcessDetection*\u0026#34;} | Select-object -first 2 Example usage 2 - read all IME logs $IMELogFolder = \u0026#39;C:\\ProgramData\\Microsoft\\IntuneManagementExtension\\Logs\u0026#39; Get-Childitem -path $IMELogFolder | Read-IMELog | Out-Gridview Read-IMELog.ps1 See below for the PowerShell script:\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/04/17/readimelog/","summary":"\u003cp\u003eRead-IMELog - A PowerShell script to read Intune Management Extension (IME) logs\u003c/p\u003e","title":"Read Intune IME Logs with PowerShell (CMTrace format)"},{"content":"More things I learned creating a GUI tool with PowerShell and WPF.\nThe snippets in this article are based on the Show-Win32AppUI tool available on GitHub.\nPart2 - Creating a multi-page WPF app in PowerShell (See part1 of this series for information on using PowerShell Runspaces with WPF).\nWPF uses .XAML text files to define the layout and properties of supported controls such as Textboxes, Buttons, Status bars etc. XAML is a form is XML, with opening and closing tags that create a hierarchy of controls. A child control such as a Combobox is displayed within a parent container such as a Window.\nVery simple apps may be able to fit controls in a single Window, but a wizard-driven interface will usually need multiple Pages to guide the user through selections. There are many ways to create a multi-page WPF app. The method below is the one I prefer:\nMain window and child pages The starting point is a WPF Window control defined in it\u0026rsquo;s own .XAML file. A Window is a container for other WPF controls, most importantly in this case, a Frame that can load Pages. The main Window can show a header, footer and sidebar that is always visible while the Page within the Frame control changes as the user navigates the app.\nEach page can be defined in a separate .XAML file. A Page is also a container so each page can host controls such as Textboxes, Textblocks, Comboboxes etc. The Frame content is updated at runtime to show a new Page when an event occurs such as clicking navigation buttons.\nA Frame does have a built-in navigation control, but it isn\u0026rsquo;t pretty. I turn it off and use Button controls in the main Window.\n\u0026lt;!--Partial XAML for a Main Window with a Frame and navigation Buttons--\u0026gt; \u0026lt;Window xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Height=\u0026#34;850\u0026#34; Width=\u0026#34;450\u0026#34; ...\u0026gt; \u0026lt;!--Pages will be loaded into this frame at runtime--\u0026gt; \u0026lt;Frame x:Name=\u0026#34;frame_Pages\u0026#34; Grid.Row=\u0026#34;2\u0026#34; Grid.Column=\u0026#34;1\u0026#34; Grid.ColumnSpan=\u0026#34;3\u0026#34; NavigationUIVisibility=\u0026#34;Hidden\u0026#34; # Built-in navigation disabled Margin=\u0026#34;0,10,0,0\u0026#34; /\u0026gt; \u0026lt;!--Navigation Buttons below the pages--\u0026gt; \u0026lt;Button x:Name = \u0026#34;Btn_Previous\u0026#34; Content=\u0026#34;Previous\u0026#34; Height=\u0026#34;35\u0026#34; Width=\u0026#34;60\u0026#34; Grid.Row=\u0026#34;3\u0026#34; Grid.Column=\u0026#34;2\u0026#34; BorderThickness=\u0026#34;0\u0026#34;/\u0026gt; \u0026lt;Button x:Name = \u0026#34;Btn_Next\u0026#34; Content=\u0026#34;Next\u0026#34; Margin=\u0026#34;3,0,0,0\u0026#34; Height=\u0026#34;35\u0026#34; Width=\u0026#34;60\u0026#34; Grid.Row=\u0026#34;3\u0026#34; Grid.Column=\u0026#34;3\u0026#34; BorderThickness=\u0026#34;0\u0026#34;/\u0026gt; See the Show-Win32UI tool for an example of separate XAML files per Page and main Window.\nLoad XAML controls into PowerShell variables A thread safe Hashtable collection allows WPF controls to be referenced in PowerShell at runtime. Grouping the controls in a collection is convenient as it simplifies passing them into Runspaces.\n### Example of reading controls from XAML files and assigning them to variables function LoadXml ($filename) { # Convert a .XAML file to an XMLDocument $XmlLoader = (New-Object System.Xml.XmlDocument) $XmlLoader.Load($filename) return $XmlLoader } # Load the XAML files $xmlMainWindow = LoadXml(\u0026#34;$PSScriptRoot\\Xaml\\MainWindow.xaml\u0026#34;) $xmlPage1 = LoadXml(\u0026#34;$PScriptRoot\\Xaml\\Page1.xaml\u0026#34;) $xmlPage2 = LoadXml(\u0026#34;$PScriptRoot\\Xaml\\Page2.xaml\u0026#34;) # Collection storing references to all named WPF controls in the UI $UIControls=[hashtable]::Synchronized(@{}) # Convert Windows and Pages to a XAML object graph $UIControls.MainWindow = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlMainWindow)) $UIControls.Page1 = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlPage1)) $UIControls.Page2 = [Windows.Markup.XamlReader]::Load((New-Object -TypeName System.Xml.XmlNodeReader -ArgumentList $xmlPage2)) # Add each named control to the $UIControls hashtable (repeat for each Window / Page) # This allows key controls to be referenced directly at runtime, rather than through a parent-child hierarchy of Page\u0026gt;Control $XmlMainWindow.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.MainWindow.FindName($_.Name) } $xmlPage1.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.Page1.FindName($_.Name) } $xmlPage2.SelectNodes(\u0026#34;//*[@*[contains(translate(name(.),\u0026#39;n\u0026#39;,\u0026#39;N\u0026#39;),\u0026#39;Name\u0026#39;)]]\u0026#34;) | ForEach-Object -Process { $UIControls.$($_.Name) = $UIControls.Page2.FindName($_.Name) } # Example of loading first Page into the Frame # \u0026#34;frame_Pages\u0026#34; in the name of the Frame defined in the XAML file $UIControls.frame_Pages.Content = $UIControls.Page1 # Show the user interface $UIControls.MainWindow.ShowDialog() Using WPF Styles to simplify XAML XAML styles are analogous to CSS in HTML. The XAML style can apply to all controls of a type, such as all TextBoxes, defining properties such as the colour, font, border etc. If a property needs to be changed, it only needs to be updated in one place. Its also a flexible solution because Style properties can still be over-ridden on an individual control.\nStyles can be defined in a separate XAML file and then imported on each Window / Page as a ResourceDictionary.\n\u0026lt;!--Styles.xaml defines a TextBox style called ModernTextBox--\u0026gt; \u0026lt;ResourceDictionary xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34;\u0026gt; \u0026lt;Style TargetType=\u0026#34;{x:Type TextBox}\u0026#34; x:Key=\u0026#34;ModernTextBox\u0026#34;\u0026gt; \u0026lt;Setter Property=\u0026#34;BorderThickness\u0026#34; Value=\u0026#34;0,0,0,1\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;BorderBrush\u0026#34; Value=\u0026#34;LightGray\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;Background\u0026#34; Value=\u0026#34;Transparent\u0026#34;/\u0026gt; \u0026lt;Setter Property=\u0026#34;FontSize\u0026#34; Value=\u0026#34;18\u0026#34;/\u0026gt; \u0026lt;/Style\u0026gt; \u0026lt;/ResourceDictionary\u0026gt; \u0026lt;!--MainWindow.xaml imports Styles.xml as a ResourceDictionary--\u0026gt; \u0026lt;Window xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Height=\u0026#34;850\u0026#34; Width=\u0026#34;450\u0026#34; Title=\u0026#34;Main Window\u0026#34;\u0026gt; \u0026lt;!--Import a style so it can used by controls in this Window--\u0026gt; \u0026lt;Window.Resources\u0026gt; \u0026lt;ResourceDictionary Source=\u0026#34;Styles.xaml\u0026#34;/\u0026gt; \u0026lt;/Window.Resources\u0026gt; ... \u0026lt;!--Page1.xaml imports Styles.xml as a ResourceDictionary--\u0026gt; \u0026lt;Page xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Background=\u0026#34;Transparent\u0026#34;\u0026gt; \u0026lt;!--Import a style so it can used by controls in this Page--\u0026gt; \u0026lt;Page.Resources\u0026gt; \u0026lt;ResourceDictionary Source=\u0026#34;Styles.xaml\u0026#34;/\u0026gt; \u0026lt;/Page.Resources\u0026gt; ... \u0026lt;!--MainWindow.xaml or Page1.xaml uses the style on a Textbox as shown below--\u0026gt; \u0026lt;TextBox x:Name=\u0026#34;txt_InstallArgs\u0026#34; Width=\u0026#34;330\u0026#34; Height=\u0026#34;30\u0026#34; Style=\u0026#34;{StaticResource ModernTextBox}\u0026#34;/\u0026gt; Define the WPF layout with the Grid control The Grid control overlays invisible rows and columns on a parent container such as a Window or Page. The Controls are then positioned based on the Row and Column number. It is fairly simple to design the Grid layout using a basic text editor. For a more WYSIWYG experience, use Microsoft Visual Studio to view the UI at design time.\nThere are three options for Grid Row/Column height / width:\nSize Meaning [Pixels] A fixed size \u0026ldquo;Auto\u0026rdquo; Expand to fit content \u0026ldquo;*\u0026rdquo; Fit to remaining space in Window / Page \u0026lt;!-- Example positioning a Textbox above a TextBlock using a Grid TextBox is in Grid.Row = \u0026#34;1\u0026#34; and TextBlock is in Grid.Row = \u0026#34;2\u0026#34;--\u0026gt; \u0026lt;Grid\u0026gt; \u0026lt;Grid.ColumnDefinitions\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;Auto\u0026#34;/\u0026gt; \u0026lt;ColumnDefinition Width=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;/Grid.ColumnDefinitions\u0026gt; \u0026lt;Grid.RowDefinitions\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;auto\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;auto\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;*\u0026#34;/\u0026gt; \u0026lt;RowDefinition Height=\u0026#34;5\u0026#34;/\u0026gt; \u0026lt;/Grid.RowDefinitions\u0026gt; \u0026lt;TextBox Width=\u0026#34;350\u0026#34; Grid.Column=\u0026#34;1\u0026#34; Grid.Row=\u0026#34;1\u0026#34; Style=\u0026#34;{StaticResource ModernTextBox}\u0026#34;/\u0026gt; \u0026lt;TextBlock Grid.Column=\u0026#34;1\u0026#34; Grid.Row=\u0026#34;2\u0026#34; Text=\u0026#34;Enter your name\u0026#34; Style=\u0026#34;{StaticResource ModernTextBlock}\u0026#34;/\u0026gt; \u0026lt;/Grid\u0026gt; Use margins to create space between controls The Margin property is available on most controls. The margin creates a buffer of blank space around a control. The margin can either be the same all round, or different for each vector - left, top, right, and bottom\n\u0026lt;!--Example setting a Margin around a control--\u0026gt; \u0026lt;!--Different margin for left, top, right, and bottom --\u0026gt; \u0026lt;ComboBox x:Name=\u0026#34;combo_Supercedence\u0026#34; Grid.Column=\u0026#34;0\u0026#34; Grid.Row=\u0026#34;11\u0026#34; Width=\u0026#34;360\u0026#34; Height=\u0026#34;24\u0026#34; Margin=\u0026#34;15,5,0,0\u0026#34;/\u0026gt; \u0026lt;!--Same margin all round --\u0026gt; \u0026lt;ComboBox x:Name=\u0026#34;combo_Supercedence\u0026#34; Grid.Column=\u0026#34;0\u0026#34; Grid.Row=\u0026#34;11\u0026#34; Width=\u0026#34;360\u0026#34; Height=\u0026#34;24\u0026#34; Margin=\u0026#34;5\u0026#34;/\u0026gt; Defining Event Handlers WPF controls support events that occur based on user input. Some events are common to most controls, such as MouseEnter and MouseLeave while other events are specific to a control type, such as SelectionChanged in a ComboBox.\nPowerShell uses an \u0026ldquo;Add_[Event Name]\u0026rdquo; syntax to define the code to run when the event fires as below. The per-control event list is available in the MS Documentation.\n# Example event handler - code will run when the Combo box selection changes $UIControls.combo_language.Add_SelectionChanged({ $Language = $UIControls.combo_language.SelectedItem }) # Example event handler - code will run when the button is left clicked $UIControls.btn_Next.Add_Click({ $UIControls.frame_Pages.Content = $UIControls.Page2 }) This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/21/PowerShellWPFPt2/","summary":"\u003cp\u003e\u003cstrong\u003eMore things I learned creating a GUI tool with PowerShell and WPF.\u003c/strong\u003e\u003cbr\u003e\nThe snippets in this article are based on the \u003ca href=\"https://github.com/gbdixg/Show-Win32AppUI\" target=\"_blank\"\u003eShow-Win32AppUI tool\u003c/a\u003e available on GitHub.\u003c/p\u003e","title":"Creating a GUI App with PowerShell and WPF - Part 2 Controls, Events and XAML"},{"content":"Things I learned creating a GUI tool with PowerShell and WPF.\nThe snippets in this article are based on the Show-Win32AppUI tool available on GitHub.\nDisclaimer I realise PowerShell isn\u0026rsquo;t suited to creating GUI apps. The main reason to use PowerShell is supportability. Specifically, when working with Colleagues who aren\u0026rsquo;t comfortable maintaining a compiled language like C#. Its far easier to make small changes to variables or paths in a script.\nWhy WPF? WPF is a more modern and flexible choice for a UI than the something like WinForms. There are many newer frameworks available, but most require a runtime on the target platform. WPF is easy to deploy as it\u0026rsquo;s built-into the .NET Framework and available by default on Windows 10/11.\nPart1 - Design the app around PowerShell Runspaces If you try to create a GUI app with a single thread, it will be unresponsive and hang whenever an action takes more than a few seconds. PowerShell runs under a single thread (STA mode) making it unsuitable for a responsive GUI app. However, creating separate Runspaces is a workaround for this problem. Runspaces are analogous to spinning up new PowerShell sessions in the background to execute discrete script blocks.\nWhile Runspaces are effective, they also add a lot of complexity. For example:\nFunctions and variables are not shared between Runspaces by default and have to be imported when the Runspace is started. You should use a thread-safe collection when updating shared variables inside a Runspace. The WPF UI can\u0026rsquo;t be updated directly from a separate Runspace Writing to a single file from multiple Runspaces requires a locking mechanism, such as a Mutex The lifecycle of Runspaces must be managed, capturing output at completion Warning, Verbose and Error streams in the Runspace are not captured by default Warning, Verbose and Error streams do not appear in the console by default. These concepts are covered in more detail below.\nSharing variables, functions and modules with a Runspace Required modules must be specifically loaded into the InitialSessionState of the Runspace. The Runspace won\u0026rsquo;t automatically have access to modules already loaded in the parent PowerShell session.\n### Example of importing modules into a new Runspace ### $modulesToLoad=@(\u0026#39;Microsoft.Graph.Intune\u0026#39;,\u0026#39;MSAL.PS\u0026#39;) # Must be installed on the computer $initialSessionState = [initialsessionstate]::CreateDefault() foreach($module in $modulesToLoad){ $initialSessionState.ImportPSModule($module) } $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) You can also import standalone Functions into a Runspace using the InitialSessionState. The following imports functions already loaded in the parent session, but you could also load directly from a file on disk.\n### Example importing parent session functions into a new Runspace ### $FunctionsToImport = @(\u0026#39;Write-TxtLog\u0026#39;,\u0026#39;Get-APIResults\u0026#39;) foreach($function in $functionsToImport){ $definition = Get-Content \u0026#34;Function:\\$Function\u0026#34; $entry = New-Object System.Management.Automation.Runspaces.SessionStateFunctionEntry -ArgumentList $function, $definition $initialSessionState.Commands.Add($entry) } $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) Variables can be shared with a Runspace using the SetVariable method of the SessionStateProxy class. SetVariable parameters are the variable name (without the \u0026lsquo;$\u0026rsquo;) and the value to set.\n### Example importing parent session variables into a new Runspace ### $VariablesToImport = @(\u0026#39;username\u0026#39;,\u0026#39;password\u0026#39;,\u0026#39;displayName\u0026#39;) # existing variables in the parent session $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() Foreach($var in $VariablesToImport){ $VarValue = Get-Variable -Name $Var | Select -ExpandProperty Value $Runspace.SessionStateProxy.SetVariable($Var,$VarValue) } Runspace output using a thread-safe collection Output from a Runspace can be captured during execution using a thread-safe collection imported from the parent session. When a Runspace updates the collection the updated values are available in the parent session and any concurrent Runspaces that also import the collection. Thread safe collections usually need to be locked during update to prevent conflicts.\nWith a synchronised Arraylist, values added in the Runspace will immediately available to all other Runspaces and the parent session, as in following example:\n### Example using locks on a thread safe collection ### # Parent session code $BackgroundJobs = [system.collections.arraylist]::Synchronized((New-Object System.Collections.ArrayList)) # Thread safe collection $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() $Runspace.SessionStateProxy.SetVariable(\u0026#39;BackgroundJobs\u0026#39;,$BackgroundJobs) # Pass the variable into the RunSpace $PSCode = [powershell]::Create().AddScript({ # This is the Runspace script block try{ [System.Threading.Monitor]::Enter($BackgroundJobs.SyncRoot) # lock $BackgroundJobs.Add(\u0026#34;New item\u0026#34;) # modify ArrayList }finally{ [System.Threading.Monitor]::Exit($BackgroundJobs.SyncRoot) # unlock } },$True) $PSCode.Runspace = $Runspace $null = $PSCode.BeginInvoke() There are also Queues and Stacks in .NET that automatically implement locking and don\u0026rsquo;t need the System.Threading.Monitor code in the above example e.g. a ConcurrentQueue:\n### ConcurrentQueue example - doesn\u0026#39;t need System.Threading.Monitor ### # Parent session code $colQueue = [System.Collections.Concurrent.ConcurrentQueue[psobject]]::new() $colQueue.TryAdd([pscustomobject]@{ givenName = \u0026#39;Bill\u0026#39;;sn=\u0026#39;Gates\u0026#39;}) $colQueue.TryAdd([pscustomobject]@{ givenName = \u0026#39;Steve\u0026#39;;sn=\u0026#39;Jobs\u0026#39;}) $Runspace.SessionStateProxy.SetVariable(\u0026#39;colQueue\u0026#39;,$colQueue) $PSCode = [powershell]::Create().AddScript({ # This is the Runspace script block # No locking required when ConcurrentQueue is modified in the Runspace $Entry = $null if($colQueue.TryDequeue([ref]$Entry)) { Write-Output $Entry } },$True) More information on thread safe collections is available here\nHow to update the WPF user interface from another Runspace If you try to modify the WPF user interface from a separate Runspace, PowerShell will throw an error indicating only the owning thread (Runspace) can update it.\nThe solution is to wrap the update in a Dispatcher.Invoke method as follows:\n### Example updating a WPF control from another thread (Runspace) ### # In the Runspace script block # txt_SetupFile is a WPF text box created in the parent session of the Runspace $UIControls.txt_SetupFile.Dispatcher.Invoke([action]{ $UIControls.txt_SetupFile.Text = \u0026#34;Successfully updated from another Runspace\u0026#34; },\u0026#34;Normal\u0026#34;) Writing to the same log file from separate Runspaces Runspaces also make it more complex to write to a single log file. There is potential for a deadlock or race condition to occur. A Mutex is one way to implement the required locking:\n### Example using a Mutex lock before writing to a log file ### # In the Runspace script block # LogMutex is an arbitrary name but must be the same when used in any Runspace and the parent session $mtx = New-Object System.Threading.Mutex($false, \u0026#34;LogMutex\u0026#34;) If ($mtx.WaitOne()){ # Wait until this Runspace can get a lock on the LogMutex object # Lock obtained. Other Runspaces are now waiting try{ Add-Content -Path $logFile -Value $Message -ErrorAction Stop }finally{ [void]$mtx.ReleaseMutex() # release the lock $mtx.Dispose() } } Managing Runspace lifecycle A Runspace executes its script block asynchronously and output (if any) is available at the end. The parent session must manage Runspaces, checking for completion, processing output and ultimately disposing of them. If you don\u0026rsquo;t dispose of Runspaces they will persist until the parent PowerShell session is closed and could eat-up memory.\nA Timer is a common way to manage Runspaces in an event-driven WPF script. When the Timer event fires, its script blocks checks for Runspace completion as in the example below. A thread safe collection is used to keep track of Runspaces until they are disposed of.\n### Example Timer code to clean-up completed Runspaces ### # Create a collection to track Runspaces $PS = [powershell]::Create().AddScript($codeToRunInRunspace) $handle = $PS.BeginInvoke() # Start the runspace # Add the new Runspace to the RunspaceJobs collection $RunspaceJobs = [system.collections.arraylist]::Synchronized((New-Object System.Collections.ArrayList)) try{ [System.Threading.Monitor]::Enter($RunspaceJobs.SyncRoot) # lock $RunspaceJobs.Add([PSCustomObject]@{ powerShell = $PS # System.Management.Automation.PowerShell object runspace = $handle # System.Management.Automation.PowerShellAsyncResult Object }) | Out-Null }finally{ [System.Threading.Monitor]::Exit($RunspaceJobs.SyncRoot) # unlock } # Timer to manage Runspace lifecycle $RunspaceCleanupTimer = New-Object System.Windows.Forms.Timer $RunspaceCleanupTimer.Enabled = $true $RunspaceCleanupTimer.Interval = 5000 # Timer code runs every 5 seconds $RunspaceCleanupTimer.Add_Tick({ # In the timer code Foreach($job in $Script:RunspaceJobs){ if($job.runspace.IsCompleted -eq $True){ # Capture completed Runspace output and dispose of it to free-up memory try{ $RSOutput = $job.powerShell.EndInvoke($job.runspace) $job.powerShell.runspace.Dispose() # Remove the job from the tracking collection try{ [System.Threading.Monitor]::Enter($Script:RunspaceJobs.SyncRoot) $Script:RunspaceJobs.Remove($job) }finally{ [System.Threading.Monitor]::Exit($Script:RunspaceJobs.SyncRoot) } }catch{ Write-Host \u0026#34;Runspace disposal Failed \u0026#39;$_\u0026#39;\u0026#34; } } }#foreach })#End of timer scriptblock $RunspaceCleanupTimer.Start() Capturing Verbose, Warning and Error streams from a Runspace By default, the Runspace output streams are not displayed in the parent session console and are lost when the Runspace is disposed.\nThe output can be captured at Runspace completion using the Streams object. The modification below to the Timer script block saves the output to a log file.\n#### Modified Timer code to capture additional output streams at clean-up #### Foreach($job in $Global:BackgroundJobs){ if($job.runspace.IsCompleted -eq $True){ Write-TxtLog \u0026#34;Runspace \u0026#39;$($job.powershell.runspace.name)\u0026#39; completed...\u0026#34; # Could also include \u0026#39;DEBUG\u0026#39; and \u0026#39;Information\u0026#39; streams if used in your Runspaces $Streams = @{ \u0026#39;Verbose\u0026#39;=\u0026#39;VERBOSE\u0026#39; \u0026#39;Warning\u0026#39;=\u0026#39;WARN\u0026#39; \u0026#39;Error\u0026#39;=\u0026#39;ERROR\u0026#39; } Foreach($StreamType in $Streams.Keys){ $StreamOutput = $job.powershell.Streams.\u0026#34;$StreamType\u0026#34; # Capture the Runspace output for each stream if($StreamOutput){ $StreamOutput | Foreach-Object { Write-TxtLog $_ -indent 2 -severity $($Streams[$StreamType]) } } Remove-Variable -name \u0026#39;StreamOutput\u0026#39; -force -ErrorAction SilentlyContinue } Write-TxtLog \u0026#34;Disposing of runspace...\u0026#34; -indent 1 try{ $RSOutput = $job.powerShell.EndInvoke($job.runspace) $job.powerShell.runspace.Dispose() # Remove the job from the tracking list try{ [System.Threading.Monitor]::Enter($Global:BackgroundJobs.SyncRoot) $Global:BackgroundJobs.Remove($job) }finally{ [System.Threading.Monitor]::Exit($Global:BackgroundJobs.SyncRoot) } }catch{ Write-TxtLog \u0026#34;Failed \u0026#39;$_\u0026#39;\u0026#34; -indent 2 -severity ERROR } } }#foreach Displaying Verbose, Warning and Error Streams in the console The approach above captures output when the Runspace code has completed. If you want feedback in the console during execution there are a couple of methods.\nFirstly, the simplest option is to use the .NET Console.Writeline() method. Although this doesn\u0026rsquo;t capture the PowerShell streams, it is a simple way to provide real-time console output in the parent session.\nYou will need to implement your own colour-coding to distinguish between warnings or errors if needed.\n### Example Runspace code to write to parent PowerShell console ### [console]::ForegroundColor=\u0026#39;YELLOW\u0026#39; [Console]::WriteLine(\u0026#39;Username was not found\u0026#39;) # Automatically writes to the parent session [console]::ResetColor() Alternatively, you could pass the built-in $Host variable from the parent session into the Runspace and use methods like WriteVerboseLine() as shown below.\n### Example writing to parent console using the $Host variable # Parent session code $Runspace = [runspacefactory]::CreateRunspace($initialSessionState) $Runspace.Open() $Runspace.SessionStateProxy.SetVariable(\u0026#39;ParentHost\u0026#39;,$Host) # Built-in host variable passed into the Runspace as $ParentHost # Write to the parent console from the Runspace script block $ParentHost.ui.WriteVerboseLine(\u0026#34;Realtime verbose output from Runspace in parent console\u0026#34;) $ParentHost.UI.WriteWarningLine(\u0026#34;Realtime warning output from Runspace in parent console\u0026#34;) $ParentHost.UI.WriteErrorLine(\u0026#34;Realtime error output from Runspace in parent console\u0026#34;) See the Show-Win32AppUI tool for an example of using these ideas together in a WPF app.\nSee part2 of this series for information on WPF controls, events and XAML.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/21/PowerShellWPFPt1/","summary":"\u003cp\u003e\u003cstrong\u003eThings I learned creating a GUI tool with PowerShell and WPF.\u003c/strong\u003e\u003cbr\u003e\nThe snippets in this article are based on the \u003ca href=\"https://github.com/gbdixg/Show-Win32AppUI\" target=\"_blank\"\u003eShow-Win32AppUI tool\u003c/a\u003e available on GitHub.\u003c/p\u003e","title":"Creating a GUI App with PowerShell and WPF - Part 1 Runspaces"},{"content":"A step by step guide to registering a custom Azure application for interactive MSGraph PowerShell. The example will create an app for use with the Show-Win32AppUI tool.\nWhy use a custom app? The Microsoft Graph enables access to a wide scope of Azure / Microsoft 365 providers and resources. A compromised Global Administrator account or errant script could cause widespread damage very quickly.\nA custom Azure application can limit MS Graph access to specific requirements of a PowerShell script, reducing the risk.\nDisable user consent The first step is to prevent users granting application consent. User consent is enabled by default and presents a risk of unwanted access to company data.\nIn the Azure AD portal, navigate to Enterprise applications and then Consent and permissions.\nMany companies change the setting to Do not allow user consent. Note that this does create an admin overhead, so you could look into the advanced options of defining low-risk permissions or using conditional access.\nThe following admin roles can then grant application consent:\nCloud App Administrators Global Administrators Clean-up Microsoft Graph PowerShell If you want to implement Custom Apps for access to MSGraph, you should first review and remove excess permissions from the Microsoft Graph PowerShell app. Over time administrators may have consented to more and more permissions.\nIt isn\u0026rsquo;t possible to remove permissions or revoke consent in the admin portal, but it does provide the PowerShell commands.\nIn the Azure AD portal, navigate to Enterprise applications\nIn the All applications view, select Microsoft Graph PowerShell and then Permissions\nClick Review Permissions\nSelect This application has more permissions that I want. The following PowerShell is displayed (uses the AzureAD module):\nConnect-AzureAD # Get Service Principal using objectId $sp = Get-AzureADServicePrincipal -ObjectId \u0026#34;1aded007-dfd1-49cc-8b70-9923a4f53a05\u0026#34; # Get all delegated permissions for the service principal $spOAuth2PermissionsGrants = Get-AzureADOAuth2PermissionGrant -All $true| Where-Object { $_.clientId -eq $sp.ObjectId } # Remove all delegated permissions $spOAuth2PermissionsGrants | ForEach-Object { Remove-AzureADOAuth2PermissionGrant -ObjectId $_.ObjectId } # Get all application permissions for the service principal $spApplicationPermissions = Get-AzureADServiceAppRoleAssignedTo -ObjectId $sp.ObjectId -All $true | Where-Object { $_.PrincipalType -eq \u0026#34;ServicePrincipal\u0026#34; } # Remove all delegated permissions $spApplicationPermissions | ForEach-Object { Remove-AzureADServiceAppRoleAssignment -ObjectId $_.PrincipalId -AppRoleAssignmentId $_.objectId } Custom app Step by Step The following steps create a new application with delegated API access to MSGraph, suitable for use from a PowerShell script. The permissions are specific to the Show-Win32AppUI\nOpen the Azure AD portal and select App registrations\nClick on New Registration to start the process\nEnter a name for the application and select the single tenant option. Do not enter a redirect URI at this stage.\nClick on Register\nThe application is created and the admin center shows the Overview page.\nClick Add a Redirect URI\nThe authentication step is displayed. Click on Add a platform\nSelect Mobile and desktop applications\nEnable the nativeclient URI to support PowerShell 5.1 scripts. Add a custom redirect URI of http://localhost to support PowerShell 7.x scripts.\nClick Configure to save the URIs\nNow back on the Authentication page, ensure the following options are set:\nSupported account types = Accounts in this organizational directory only Allow public client flows = No Click API permissions in the left pane\nUser.Read delegated access is already assigned by default. Click Add a permission\nSelect the Microsoft Graph API\nSelect Delegated permissions\nSearch for each of the following delegated permissions, enable and click Add permissions\nDeviceManagementApps.ReadWrite.All Repeat the process to add permissions for the following:\nGroup.ReadWrite.All GroupMember.ReadWrite.All Directory.AccessAsUser.All The API permissions page will show the list of added permissions with a warning that consent is not granted.\nClick Grant admin consent for The warnings will be replaced by a green tick.\nReturn to the application Overview page and note the Application (client) ID - this will be used to authenticate\nAdd assignment restrictions By default anyone in the tenant can access the application (although they need Role permissions to make changes in Azure AD and Intune). This step will limit access to a specific group.\nView the Properties page of the app and change Assignment required to Yes\nNext click on Users and groups then add user/group. Select the required Azure AD group and Assign. Only members of this group can authenticate using the app.\nAuthenticating using the custom app Delegated consent uses the intersection of application permissions and user permissions to authorise access. i.e. the authenticated user must have the required permissions as well as the application.\nUse interactive authentication to provide credentials with the required role permissions. Interactive auth prompts using the familiar browser page:\nExamples of PowerShell authentication using the app are below:\nMSAL.PS authentication $TenantID = \u0026lt;your tenant ID obtained from the AAD Overview page\u0026gt; $ClientID = \u0026lt;application ID of the custom app\u0026gt; $Token = Get-MSALToken -TenantID $TenantID -ClientID $ClientID -Interactive IntuneWin32App authentication $TenantID = \u0026lt;your tenant ID obtained from the AAD Overview page\u0026gt; $ClientID = \u0026lt;application ID of the custom app\u0026gt; Connect-MSIntuneGraph -TenantID $TenantID -ClientID $ClientID -Interactive The Connect-MSIntuneGraph command in the IntuneWin32App module creates global variables to store the token for later use:\n$Global:AuthenticationHeader $Global:AccessToken This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/19/AzureAppRegistration/","summary":"\u003cp\u003eA step by step guide to registering a custom Azure application for interactive MSGraph PowerShell. The example will create an app for use with the \u003ca href=\"/2023/03/18/Show-Win32AppUI/\"\u003eShow-Win32AppUI tool\u003c/a\u003e.\u003c/p\u003e","title":"A custom Azure app with limited Microsoft Graph consent for PowerShell"},{"content":"A GUI tool for end-to-end creation of Win32 Apps in Microsoft Intune.\nThis post provides details of \u0026ldquo;Show-Win32AppUI\u0026rdquo;, a GUI tool that simplifies the end-to-end process of creating Win32 Apps in Intune.\nThe GitHub source is available here. Instructions for setup and use are below.\nThis isn\u0026rsquo;t a one size fits all community tool. You will likely need to modify it to meet your needs. However, its written in PowerShell and has code comments and blog posts to make editing simpler.\nSETUP Modules Show-Win32AppUI depends on two PowerShell modules. Install these modules if you don\u0026rsquo;t already have them.\nInstall-Module -Name MSAL.PS Install-Module -Name IntuneWin32App The most recent tested versions are listed below:\nInstall-Module -Name MSAL.PS -RequiredVersion 4.37.0.0 Install-Module -Name IntuneWin32App -RequiredVersion 1.4.0 Tenant ID Update the $TenantID on line 4 of Show-Win32AppUI.ps1 to use your required Azure tenant. Your tenant ID is available from the Azure AD portal Overview page.\nAzure Client App An Azure Client App is used with interactive authentication to access the Microsoft Graph. There are two setup steps required:\nSpecify the Azure application\nBy default, the tool will use the built-in Microsoft Graph PowerShell enterprise application. However, I recommend creating a custom Azure app in your own tenant. A step by step guide to creating a custom app is available here. If using a custom app, update Show-Win32AppUI.ps1 to set the $ClientID variable on line 6 to match the client ID (a.k.a Application ID) of your app.\nConsent to the required permissions on behalf of your tenant\nWhether you use a custom app or Microsoft Graph PowerShell, the app must be configured with the required API permissions and consent must be granted. The delegated permissions are listed below. A step by step for setting these permissions can be found in the second part of this article. Directory.AccessAsUser.All DeviceManagementApps.ReadWrite.All Group.ReadWrite.All GroupMember.ReadWrite.All User.Read User permissions Delegated consent uses the intersection of application permissions and user permissions to authorise access. i.e. the authenticated user must have the required permissions as well as the application. When using the app, authenticate using an Azure account with one of the following roles:\nIntune Administrator Global Administrator Workstation permissions The tool does not need administrative access to the client workstation. Internet access is required, to download the Win32 Content Prep tool on first use.\nPowerShell script execution PowerShell script execution is disabled on Windows clients by default. Use one of the methods below to allow script execution on the workstation.\nset-executionpolicy Unrestricted or\npowershell -executionpolicy bypass -file \u0026lt;path to script\u0026gt; Launch the tool Start a PowerShell 5.1 or Pwsh 7.x console and execute the script as follows:\n.\\Show-Win32AppUI.ps1 To show debug information in the console add the WriteHost switch:\n.\\Show-Win32AppUI.ps1 -WriteHost Using the tool Page1 - Package Use the file dialog to select the main Setup File - .msi, .exe or .ps1.\nFor an .msi file, the setup and uninstall automatically uses MSIEXEC, defaulting to a quiet install/uninstall and verbose logging.\nFor an .exe file, the setup parameters default to /S, but you should check the vendor information and replace this as appropriate.\nFor a .ps1 file, setup and uninstall defaults to -noprofile and -executionpolicy bypass.\nThe package source folder is the folder containing the setup file. All the files in this folder are packaged into an .intunewin file in a later step.\nThe tool creates installation wrapper scripts called install.ps1 and uninstall.ps1 in the package source folder. Existing files with these names are overwritten.\nThe Next button is only available when required fields have been completed. Page2 - Deployment The Display Name is built from the Publisher, App Name, Version and Package Number. If the language is changed from the default or the Bitness is changed to x86, these are also included in the Display Name\nFor .msi and .exe files, the fields are populated with information from the setup file, but can be edited as required.\nIf there is already an Intune application with the same Display Name a warning will appear in the status bar. The simplest solution is to increment the Package Number.\nThe Next button is only available when required fields have been completed. Page3 - Assignment Assignment Groups shows the names of three AAD groups for Required Install, Available Install and Uninstall. The group name suffix is based on the App Name from Page 2 and cannot be edited here.\nThe Owner must be a UPN of an AAD user. Start typing a name in the top box to see a list of options. Select a name and click Add. The Owner is set on the properties of the Win32App and the AAD groups.\nThe Dependency and Supercedence lists are populated with existing Win32 Apps. Select from the list if these options are needed. Currently you can only select one of these options due to a limitation in the IntuneWin32App module.\nClick on the Logo box to select a image file for display with the application in the Company Portal.\nThe Next button is only available when required fields have been completed. Page3 - Implement The final page follows a step-by-step approach to creating the Win32 App.\nCreate Wrapper Scripts creates an install.ps1 and uninstall.ps1 file in the package source folder.\nCreate Intunewin Package uses the Win32 Content Prep Tool to build an .intunewin file in the Output Folder.\nCreate App Groups creates three AAD groups for Required Install, Available Install and Uninstall. If the groups already exist they are re-used.\nCreate Win32 App creates the Win32 App in Intune and uploads the .intunewin file. This step can take some time depending on the package size.\nConfigure Dependency / Configure Supercedence modifies the Win32 App in Intune. These steps are skipped if they are set to None.\nConfigure Assignment modifies the Win32 App in Intune to add the assignment groups created in the earlier step. Troubleshooting The tool creates a debug log on every run with detailed information and error messages. The default LogFolder is C:\\Temp, but can be modified on Line 8 of Show-Win32AppUI.ps1.\nThe -WriteHost switch will also show the debug output in the console.\n.\\Show-Win32AppUI.ps1 -writehost Options The variables section in Show-Win32AppUI.ps1 allows default settings to be modified. The following section is at Line 50:\nDetection Method The Win32 app detection is hardcoded to use a file exists method. The Install.ps1 script wrapper creates a \u0026ldquo;.ps1.tag\u0026rdquo; file under the %PROGRAMDATA% folder and Uninstall.ps1 deletes it - a detection method first suggested by Michael Niehaus\nCredits Show-Win32AppUI is a front-end to the excellent IntuneWin32App module. Full credit to the contributors of this project.\nThe MSAL.PS module has simplified the transition from ADAL to MSAL authentication.\nBoe Prox for PowerShell Runspace tips\nSMSAgent for PowerShell WPF tips\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/03/18/Show-Win32AppUI/","summary":"\u003cp\u003eA GUI tool for end-to-end creation of Win32 Apps in Microsoft Intune.\u003c/p\u003e","title":"Create Intune Win32Apps with a PowerShell GUI front-end"},{"content":"You may notice that Windows Firewall events are not available in Defender for Endpoint Advanced Hunting. This is a quick post on the steps required to enable Firewall audit events.\nOlaf Hartong\u0026amp;rsquo;s excellent series on MDE Internals highlights that some MDE telemetry is based on Kernel call-backs or drivers, making those areas independent of client audit policy and enabled by default.\nOther settings, however, do rely on ETW providers and therefore on the MDE client\u0026rsquo;s security audit policy. Firewall audit events will only be available in MDE if the relevant audit subcategory is enabled.\nEnable client-side Firewall auditing The following auditpol command will enable Windows Firewall client-side auditing and start sending the telemetry to MDE.\nauditpol /set /subcategory:\u0026#34;Filtering Platform Connection\u0026#34; /success:disable /failure:enable It\u0026rsquo;s fine to keep this enabled during day-to-day use as it will result in the following\nBlocked connections create an audit event in the Security Event Log Allowed connections do not create an audit event A single audit event is created for each connection attempt, not each packet I don\u0026rsquo;t recommend auditing Filtering Platform Connection success events as this will generate a lot of events. You may be tempted to also enable Filtering Platform Packet Drop, but this would also generate a high volume.\nIntune You could of-course use Intune to apply the same setting to an enrolled Windows client. Create a Device Configuration Profile based on the Settings Catalog. Enter Filtering in the settings search and enable Success auditing for Object Access Audit Filtering Platform Connection\nAdvanced Hunting Query You can view the Firewall events in MDE Avanced Hunting as follows:\nDeviceEvents | where Timestamp \u0026gt; ago(1d) | where ActionType startswith \u0026#34;Firewall\u0026#34; EXAMPLE RESULT:\nTimestamp ComputerName ActionType IPAddress RemoteIPCountry RemoteIPAddr RemotePort Protocol Direction NetworkProfile InterfaceType RuleId RuleName Action Application Service User InitiatingProcess InitiatingProcessPath InitiatingProcessCommandLine 2023-02-02T10:30:12.000Z DESKTOP01 Firewall 192.168.1.100 United States 54.239.29.192 443 TCP Outbound Domain Wi-Fi Allowed Chrome.exe User1 chrome.exe C:\\Program Files (x86)\\Google\\Chrome\\chrome.exe 2023-02-02T09:01:30.000Z DESKTOP02 Firewall 192.168.1.200 United States 91.189.91.26 53 UDP Outbound Public Ethernet Blocked svchost.exe DNS User2 svchost.exe C:\\Windows\\System32\\svchost.exe 2023-02-02T08:45:55.000Z DESKTOP03 Firewall 192.168.1.150 Canada 185.104.10.98 80 TCP Inbound Private Wi-Fi Blocked Example Event in Windows Security Log Log Name: Security Source: Microsoft Windows Security Auditing Date: 5/2/2023 10:31:12 AM Event ID: 5444 Task Category: Filtering Platform Connection Level: Information Keywords: Audit Failure User: N/A Computer: MyComputer Description: A network connection request was blocked. Subject: Security ID: S-1-5-18 Account Name: MyComputer$ Account Domain: MYDOMAIN Logon ID: 0x3E7 Network Information: Direction: Inbound Source Address: 192.168.1.100 Source Port: 54321 Destination Address: 192.168.1.200 Destination Port: 80 Protocol: TCP Filter Information: Filter Run-Time ID: 157314 Layer Name: Transport Layer Run-Time ID: 13 Additional Information: Reason: The rule does not match the traffic. This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2023/02/02/Firewallmde/","summary":"\u003cp\u003eYou may notice that Windows Firewall events are not available in Defender for Endpoint Advanced Hunting. This is a quick post on the steps required to enable Firewall audit events.\u003c/p\u003e","title":"Enable Defender Firewall event forwarding to MDE"},{"content":"Part two of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration, looking at network routing, multi-geo considerations and scheduling issues.\nBE AWARE OF NETWORK ROUTING Although the migration tool uses a source UNC path and destination OneDrive URL, files are actually uploaded to Azure blobs before being transferred to OneDrive\ne.g. *.blob.core.windows.net\nThere is a list of required endpoints, and it\u0026rsquo;s important to determine if the migration traffic will route out through the enterprise proxy servers, or go direct e.g. through an ExpressRoute link. This routing will be specific to your network setup.\nUse \u0026ldquo;Agent Groups\u0026rdquo; in an enterprise network Agent Groups are a logical grouping within Migration Manager, allowing each migration to use a specific agent or agent(s)\nCarefully consider the network location of each on-prem migration agent, to optimises the traffic flow and minimise bandwidth impact. The two key considerations are:\nproximity to the home drive file server proximity to the Azure/Internet egress link. Assign each agent to an Agent Group based on its location. Use the Agent Group option when scheduling a migration to control which agent is used for a specific migration batch.\nSPO Admin Portal \u0026gt; Migration \u0026gt; File Shares \u0026gt; Agents \u0026gt; Select an Agent \u0026gt; Edit \u0026gt; Agent Group Scanning always uses the Default agent group Scanning is a pre-migration activity used to identify home drive data issues such as \u0026lsquo;path too long\u0026rsquo;.\nWhile migration tasks can use agent groups, scanning tasks cannot. Scanning automatically uses the Default Agent group.\nIf the default group has no agents the scanning task will just wait indefinitely for an agent to be added back to the Default group.\nGroup migration batches by location When you use the bulk migration option, the portal imports a CSV file and assigns all entries to the same Migration Agent. If you are carrying out migrations in multiple locations, you need to group them by location and split into separate CSV files to make efficient use of the network..\nRun Satelite-geo migrations separately If you are in a multi-geo tenant, don\u0026rsquo;t mix geolocations in the same migration batch. Migration to a satellite geo needs a modified process with dedicated or modified agent config.\nIf you are signed into the SharePoint Online Admin portal \u0026ldquo;central geo\u0026rdquo;, switch to the relevant \u0026ldquo;satellite geo\u0026rdquo; using the link in the top left. The go to Migrations \u0026gt; File Shares. Initially there will be no agents listed.\nRun the agentsetup.exe on the migration server and pause on the first screen of the setup wizard. If the agent is already installed, re-run setup to modify the configuration.\nWith the setup wizard open, edit the following file:\n%temp%\\SPMigrationAgentSetup\\SPMigrationAgentSetup\\Microsoft.SharePoint.Migration.ClientShared.dll.config Under AppSettings, add the following:\n\u0026lt;add key=GeoLocation value=\u0026#34;FRA\u0026#34; /\u0026gt; The value must be a valid Azure tenant location code that is already enabled in your tenant.\nSave the file and continue the setup wizard. The Migration Manager agent will register in the satellite geo and be visible in the SPO Admin Portal for the relevant Geolocation.\nTIMEZONE HEADACHES Migration tasks can either start immediately or at a specified time. However, the time used in the Migration Manager portal is the time zone tenant home location, not the local time of the administrator accessing the portal.\nTo schedule a migration outside business hours, the administrator must take account of the local time where the agent is based and convert it to the tenant home time zone.\nScanning tasks don\u0026rsquo;t have a scheduling option. They just start immediately, although they use a lot less bandwidth.\nFALSE POSITIVE SCAN WARNINGS Temporary MS Office files create false positive warnings in the scanning tab and scan reports. There is no way to ignore the false positives.\nThe purpose of home drive scanning is to highlight problems such as incompatible file names and long paths. However, it is hard to filter out unimportant issues:\nNumerous files beginning with tilde are flagged as warnings in every report - e.g. \u0026ldquo;~budget 2020.xlsx\u0026rdquo;. These temporary files created by MS Office are often not cleaned-up when a document is closed, but it is safe to skip them during migration.\nIf folder redirection to the home drive is enabled, the scan will flag a warning on every instance of desktop.ini. Again these warnings can be safely ignored as the files are recreated by Windows if needed.\nThis isn\u0026rsquo;t to say that the pre-migration scan is a waste of time. It does have some benefits:\nconfirms the Migration Manager service account has access to the home drive\nprovides statistics such as overall home drive size and number of files\nhighlights long path issues\nCOMPLEX MIGRATION SCHEDULING There is nothing in Migration Manager help with user scheduling. Scheduling is required because there is some user impact:\nUser communciation should explain what is changing and link to more information (FAQs, OneDrive training material etc) Ideally users log off prior to the data copy to prevent issues with files locked in-use Remote users may need to use PLAP VPN to ensure the home drive mapping is removed at next logon Post-migration support may be required to help users with the changes (removal of home drive and new data location) If your IT department does not have any scheduling software, you will most likely end-up using spredasheets.\nJUGGLING CSV FILES An enterprise will need to use the \u0026ldquo;bulk migration\u0026rdquo; option in Migration Manager to migrate large numbers of home drives at a time. This option uses a CSV import to specify source UNC paths and target OneDrive URLs. The CSV must be a set format and a template can be downloaded for this purpose.\nThe pre-migration file share scan step also uses a CSV file, but unfortuately it uses different fields and headers.\nThe additional migration steps are likely to also be driven from a list such as a CSV file e.g.\nPre-provisioning OneDrives that don\u0026rsquo;t already exist Removing Active Directory user homeDirectory and homeDrive Updating folder redirection policies Updating home drive ACLs or file shares to prevent post-migration access With multiple CSV files in-play and last minute changes to schedules, user scheduling can be complex in its own right.\nSUMMARY Microsoft Migration Manager is a free but basic migration tool. By the time you have discovered its limitations, you may wish you had paid for an ISV product instead.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2022/06/11/migration-manager-notes2/","summary":"\u003cp\u003ePart two of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration, looking at network routing, multi-geo considerations and scheduling issues.\u003c/p\u003e","title":"Migrate home drives to OneDrive with Microsoft Migration Manager - Pt2"},{"content":"Part one of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration using Microsoft Migration Manager, covering migration tool considerations, architecture and access requirements.\nTOOL SELECTION Migration Manager comes from the Microsoft purchase of Mover.io in 2019. It is presented as an Enterprise migration tool that puts more structure around the migration process than the more basic SharePoint Migration Tool. However, be prepared for limited configuration and customisation options. As a basic guide consider the following tool choices for file share migration:\nSharePoint Migration Tool : Ad hoc data migrations Migration Manager : Large scale migrations using repetitive process 3rd Party Tool : When you need a lot of control or have a complex process WHAT ARE COMPLEX REQUIREMENTS? Migration Manager focuses on the data migration, but for most companies, this is only part of the story. Consider the following:\nHow to move off folder redirection and offline files Removing the home drive mapping Cleaning-up or restricting access to home drives A 3rd-party tool, could be more suitable if you are looking to automate as much of the process as possible.For example, ShareGate Migration Tool provides a PowerShell interface.\nMigration Manager is still attractive as it\u0026rsquo;s effectively free as part of an enterprise M365 license.\nARCHITECTURE Migration Manager consists of the following components:\nAgents Migration Manager agents run on-premises and perform the data migration, reading data from home drives and copying it to OneDrive for Business.\nThe agent can be installed directly on a file server (assuming it is a Windows Server), but the recommended deployment is on a dedicated server.\nThe agent sends heartbeat information to the SharePoint Online (SPO) Admin Portal and downloads scanning and migration tasks in return.\nSharePoint Online Admin Portal The SPO Admin Portal is used to manage the agents and assign migration tasks to them. The \u0026ldquo;Migration\u0026rdquo; blade has tabs for scanning, agents and migrations.\nFile shares File shares are host the data being migrated. The migration agent service account needs read access to the data and acceses it over SMB.\nProxy server The migration agent communicates with an SPO Admin portal endpoint and the Azure blob service over HTTPs. If the company does not have an Express Route, traffic is likely to go through an enterprise proxy server.\nA LOT OF ELEVATED ACCESS If you work with strict security controls, be prepared for Migraton Manager\u0026rsquo;s privileged access requirements:\nAccount Permission Purpose Cloud service account SharePoint Administrator Agent communication with SPO Admin portal Cloud service account OD4B Site Collection Admin Agent write access to OneDrives On-prem service account Read Access to Home Drives Agent read access to home drives I.T. admin account SharePoint Administrator Scheduling scans and migrations in the SPO Admin portal Why SharePoint Administrator? Under the hood, OneDrive for Business is a personal SharePoint site and from an administrative perspective, Microsoft has done very little to separate OneDrive for Business from SharePoint Online.\nThere is no OneDrive-specific RBAC. To manage OD4B, you need to be a SharePoint Administrator, and Migration Manager is operated through a blade in the SharePoint Online Admin Console. The technicians managing and monitoring the data migrations will need the SharePoint Administrator role and so will the Service Account used by the migration agent.\nSharePoint Administrator role is not enough A SharePoint Administrator cannot copy data to OneDrive by default, but they can grant themselves the additional rights needed\nBy default, only the owner (user) has access to OneDrive for Business. One option is to define a OD4B Secondary Admin group at the tenant level, but this will only apply to new OneDrives as they are provisioned.\nSPO Admin Portal \u0026gt; Advanced \u0026gt; More Features \u0026gt; Setup My Sites \u0026gt; Secondary Owner For already provisioned OneDrives, use a script to grant the SiteCollectionAdmin permission on each OneDrive to a group containing the Migration Manager cloud service account.\nConnect-SPOService -URL $AdminURL Set-SPOUser -Site $UsersOneDriveURL -LoginName $CloudServiceAccountorGroup -IsSiteCollectionAdmin $True NOTE: This command normally expects a user account as the LoginName. If using a group name, it must be in claims encoded format.\nPart two in this series will look at network routing, multi-geo considerations and scheduling issues.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2022/06/04/migration-manager-notes1/","summary":"\u003cp\u003ePart one of \u0026ldquo;Notes from the Field\u0026rdquo; for a home drive to OneDrive for Business migration using Microsoft Migration Manager, covering migration tool considerations, architecture and access requirements.\u003c/p\u003e","title":"Migrate home drives to OneDrive with Microsoft Migration Manager - Pt1"},{"content":"Most of the time in GIT we are working at the file level. It is possible to get more granualar and work with hunks which are parts of a file e.g. a number of lines.\nUsing Hunks The --patch option of the git add command causes GIT to automatically split an updated file into hunks. It then prompts for each hunk and the contributor can decide whether to stage some or all of the hunks.\nGIT actually enters a menu system that allows granular control, such as splitting the hunk into smaller units. After adding some hunks but not all to the index, git status shows the same file is ready to be committed and also not yet staged for commit.\nThere are many commands that can work at the hunk level, including:\ngit checkout git stash git reset \u0026gt;git add --patch file.txt # This command (1/1) Stage this hunk [y,n,q,a,d,s,e,?] ? \u0026gt; y - stage this hunk n - do not stage this hunk q - quit a - stage this hunk d - do not stage this hunk or an of the later hunks in the file s - split the current hunk into smaller hunks e - manually edit the current hunk ? - print help \u0026gt; \u0026gt; git status # After adding some hunks but not all, the status shows the same file is ready to be committed and also not yet staged for commit Changes to be committed: modified: file.txt Changes not staged for commit: modified: file.txt Many ways to reference a commit The git show command provides information about a commit. The most common way to reference a commit is using its hash (or partial hash), but there are also other ways:\n\u0026gt; git show 8d4112 # use the partial hash to refer to the commit commit 8d411239358d55f45747c401c5c2c3fba8652d71 Author: gbdixg \u0026lt;gbdixg@domain.home\u0026gt; Date: Mon Jun 7 17:44:03 +100 ... \u0026gt; \u0026gt; git show HEAD # show information about the latest commit \u0026gt; \u0026gt; git show HEAD^ # show the parent commit of HEAD \u0026gt; \u0026gt; git show HEAD^^ # show the second parent of HEAD \u0026gt; \u0026gt; git show HEAD~2 # show the 2nd commit before HEAD (same as above) \u0026gt; \u0026gt; git show HEAD@{\u0026#34;1 week ago\u0026#34;} # show head 1 week ago This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/07/04/gitedgecases/","summary":"\u003cp\u003eMost of the time in GIT we are working at the file level.  It is possible to get more granualar and work with \u003cem\u003ehunks\u003c/em\u003e which are parts of a file e.g. a number of lines.\u003c/p\u003e","title":"GIT edge cases"},{"content":"GitHub hosts open source projects that have multiple contributors. Only a few maintainers have read-write access to the repository, so how do people contribute suggested changes?\nThe answer is by copying the repo to their own GitHub account, making changes and submitting a pull request to the maintainers of the original project. This post explains the steps.\nCONTRIBUTING ON GitHub Forking a repo A fork is a clone of the repo on the same hosting provider site (i.e. GitHub). You can view all the forks from the repo home page by clicking on Insights \u0026gt; Forks\nAs an example of how forks are used, the Hugo repository on GitHub contains code for a static website generator. The maintainers of the project have write access and can push changes, but other GitHub users have read access.\nAny GitHub contributor can suggest changes by first forking the repo into their own GitHub account. The Fork button at the top-right of the home page clones the Hugo repo to a copy that appears under the contributors GitHub account. The contributor now has write access to the forked copy, but it isn\u0026rsquo;t that convenient to work directly on GitHub. They need to clone the fork to a GIT repo on their development workstation.\nCloning a fork The contributor clones the forked repo to a local GIT repo using git clone \u0026lt;SSH or HTTP address of fork repo\u0026gt;. By default, this clone will track a single remote repo called origin (the fork) that will aceept pushed updates. But how does the local repo keep track of the primary Hugo repo, and how are contributor changes pushed to the original source?\nAdding a second remote While the contributor is working on their local changes, the original Hugo repo on GitHub is also getting updates from other contributors. Any conflicts need to be resolved locally rather than expecting the maintainers of the repo to deal with them. So how does the contributor stay up-to-date while working locally on their changes?\nThe local workstation repo already has a remote (origin, the forked repo on GitHub). Adding the original Hugo repo as a second remote allows the local repo to pull chages from other contributors. This second remote is usually called upstream.\n❯ git remote -v # show current remote repos origin https://github.com/myaccount/myfork.git (fetch) origin https://github.com/myaccount/myfork.git (push) ❯ ❯ git remote add upstream https://github.com/sourcerepo/sourceproject.git # Add a second remote pointing to upstream How to push changes to upstream The final piece of the puzzle, is how to get the contributor\u0026rsquo;s changes into the upstream repo without having write access.\nGIT itself doesn\u0026rsquo;t offer a solution to this, but GitHub does hence it\u0026rsquo;s popularity for open source projects.\nFirst the contributor pushes their local repo changes to the GitHub fork (origin) using standard GIT commands. Next the contributor creates a pull request asking the maintainers of the upstream source repo to pull changes from the fork. The pull request is a messaging system that describes the changes and enables differencing checks.\nThe maintainers of the upstream repo can review and comment on the changes, possibly asking for changes before pulling and merging them into the original repo.\nUsing GIT Diff and Blame DIFF and BLAME are useful when reviewing a pull request or the project history.\ngit blame shows the file history on a line-by-line basis.\nFor each line, it displays the last commit where a line was changed, who made the change and when.\n❯ git blame .\\index.md cf6ff2bb (GD 2021-05-05 22:09:05 +0100 1) --- cf6ff2bb (GD 2021-05-05 22:09:05 +0100 2) author: GD cf6ff2bb (GD 2021-05-05 22:09:05 +0100 3) --- cc0e10d2 (GD 2021-05-11 15:55:25 +0100 4) **This is the first post in a series on creating a graph database CMDB.** cf6ff2bb (GD 2021-05-05 22:09:05 +0100 5) 7c78446d (GD 2021-05-24 19:50:01 +0100 6) Part 1: This article 7c78446d (GD 2021-06-24 21:03:19 +0100 7) Part 2: How to export computer information from Microsoft Active Directory using PowerShell, for use in a Neo4j CMDB git diff shows the differences between commits or between the git areas (working directory, index, repo). For example, to show the differene between the current commit and back two commits:\n\u0026gt; git diff HEAD HEAD~2 # show the difference between the current position of HEAD and 2 commits before HEAD diff --git a/content/blog/gitnotes/index.md b/content/blog/gitnotes/index.md index 10fa938..cc16e9f 100644 --- a/content/blog/gitnotes/index.md +++ b/content/blog/gitnotes/index.md @@ -14,48 +14,31 @@ -The version control system GIT can seem complex, but an understanding of the internal working can help with day-to-day use and is essential to get yourself out of an unexpected state. This article covers the basics of how git works, exploring the files in the object database and laying a foundation for you to explore futher on your own. +An understanding of git internals helps with day-to-day use and is essential if you need to get yourself out of an unexpected state. This article covers the basics of how git works, allowing you to explore further on your own. The first line shows the diff command diff --git...\nThe next line is git metadata that isn\u0026rsquo;t normally needed index 10fa938..cc16e9f 100644\nThe next two lines show assign symbols to the two versions of the index.md file i.e changes from HEAD are marked with \u0026ldquo;\u0026mdash;\u0026rdquo; and tchanges from HEAD~2 are marked with \u0026ldquo;+++\u0026rdquo;\n--- a/content/blog/gitnotes/index.md +++ b/content/blog/gitnotes/index.md The next line represents the header of a hunk (portion) of the file showing the lines that have been modified. In this case, 48 lines were removed at line 14 and 31 lines added at line 14. Realistically, when lines are removed and added at the same location it is a modification of the lines.\n@@ -14,48 +14,31 @@ Finally, the actual lines that are removed and added are displayed. The \u0026ldquo;-\u0026rdquo; at the start of the line means it was removed. The \u0026ldquo;+\u0026rdquo; at the start of the line means it was added. The line was actually edited to change some of the words.\n-The version control system GIT can seem complex... +An understanding of git internals helps... Another way to view the differences is using the --color-words option. This shows the changes in-line using red for removed and green for added i.e.\ngit diff --color-words HEAD HEAD~2\nWhat else can you compare with GIT DIFF? \u0026gt; git diff # show uncommited changes since the last commit \u0026gt; \u0026gt; git diff --cached # compare the repo HEAD to the index \u0026gt; \u0026gt; git diff feature1 main # compare two branches How to see a diff of every commit The git log command has an option to diff every commit. This obviously produces a lot of output.\n\u0026gt; git log --patch How to compare the list of commits between two branches Rather than looking at the changes in the commits. You will sometimes want to just look at the history of commits and understand which are only in one branch compared to another. git log can also help here:\n\u0026gt; git log feature1..main --oneline # Compare the feature1 branch to main showing the commits only in main This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/30/github/","summary":"\u003cp\u003eGitHub hosts open source projects that have multiple contributors. Only a few maintainers have read-write access to the repository, so how do people contribute suggested changes?\u003c/p\u003e\n\u003cp\u003eThe answer is by copying the repo to their own GitHub account, making changes and submitting a \u003cem\u003epull request\u003c/em\u003e to the maintainers of the original project. This post explains the steps.\u003c/p\u003e","title":"GitHub - Contribute to a repo using Fork, Clone, Push and Pull Requests"},{"content":"This article focuses on common GIT actions that affect branches, such as merging, rebasing branches and squashing local commits.\nMERGE Merging brings changes from one branch into another. A feature branch is often created to work on a particular update. When complete, the branch needs to be merged back into \u0026ldquo;main\u0026rdquo;. The process would be as follows:\nFirst switch to the main branch using either git switch main or git checkout main.\nNext merge the featureA branch into main using git merge featureA.\nGIT will create a new commit with the merged changes. This is a special commit as it has two parents - the previous commit on main and the previous commit on the featureA branch. The main branch is updated to point to this new commit and HEAD continues to point to main. FeatureA still points to the last commit on that branch.\nCONFLICTS If a conflict is detected during a merge, GIT will interupt the process and prompt for user action\nGIT will be in a special state where it expects the conflict to be resolved before continuing.\ngit status at this point shows the message \u0026ldquo;you have unmerged paths\u0026hellip;fix conflicts and run git commit\u0026hellip;use git merge --abort to abort the merge\u0026rdquo;.\n❯ git status On branch master You have unmerged paths. (fix conflicts and run \u0026#34;git commit\u0026#34;) Unmerged paths: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to mark resolution) both modified: consolidate.py no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) GIT will display the files that have conflicts. The problem can be resolved at the command line, but a graphical diff tool such as p4merge may be better.\nIf you open the files in a basic text editor you will see GIT has marked the conflicts.\nIn the example below, the file \u0026ldquo;consolidate.py\u0026rdquo; is in conflict. GIT has updated the file with markers showing the lines that need attendtion. The HEAD section shows the lines as they appear in the current branch (main). Then there is a section break and directly below are the same lines as they appear in the featureA branch.\nimport sys \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD name=read \u0026#34;Enter your name\u0026#34; age=read \u0026#34;Enter your age\u0026#34; ======= read \u0026#34;Please enter your name\u0026#34; read \u0026#34;Enter your age\u0026#34; \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; featureA To resolve the conflict, manually edit the file so it reflects the desired state and remove the markers and section break.\nSave the file, add it to the GIT index, then commit the change to complete the merge, as shown below:\n❯ git add consolidate.py ❯ ❯ git commit [main 1aca0e1] Introduce featureA that enables user input In summary:\nSwitch to the branch you are merging into (e.g. git switch main) Merge the required branch into the current branch (i.e. git merge feature1) Fix any conflict by editing the file Add the updated file to the index (git add \u0026lt;file\u0026gt;) Commit to complete the merge (git commit) FAST FORWARD MERGE A fast forward merge occurs automatically when git moves a branch without having to create a new commit. It just re-uses an existing commit.\nThe most common case is when you want to merge a feature branch into main, but then continue working on the Feature branch with the latest updates from main.\nWhen you first merge the branch into main, GIT creates a new commit on main that has two parents - the previous commit on main and the previous commit on the branch.\n❯ git switch main ❯ git merge featureA The status is now:\nmain branch: contains the latest changes and any conflict resolutions featureA branch: contains the working feature before the merge into main To continue developing on featureA, you need to merge main back into the featureA branch so it has the latest updates\n❯ git switch featureA ❯ git merge main At this point, GIT realises that there is already a commit that has the merged contents of featureA and main, so it just re-uses this commit. The merge message shows it performed a \u0026ldquo;fast-forward merge\u0026rdquo;\n❯ git merge main Updating 68a874e..1aca0e1 Fast-forward consolidate.py | 6 +++--- orders.py | 4 ++++ 2 files changed, 7 insertions(+), 3 deletions(-) DETACHED HEAD HEAD is a reference to a branch or a commit. Normally HEAD points to the current branch and thereby indirectly to the latest commit on that branch. Detached head is a state where the HEAD is not referencing a branch, it is pointing to an older commit.\nWhy would this happen? Perhaps you want to do some experimentation without creating a branch. You would checkout a commit rather than creating a branch. At this point HEAD is no longer tracking a branch and so it is detached. It acts like a temporary branch. After making some commits, you could do one of the following:\na) Switch back to a branch\nb) Put a branch on the current commit\nIf you switch back to a branch, any previous commits outside a branch are isolated in the object database and are not referenced by any branch. Eventually they will be removed by the GIT garbage collector.\nAlternatively, git branch \u0026lt;branchname\u0026gt; can be used to put a branch on the current commit. At this point it is like any other branch and HEAD is no longer detached.\n❯ git checkout 460ce0e Note: switching to \u0026#39;460ce0e\u0026#39;. You are in \u0026#39;detached HEAD\u0026#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch. REBASE Rebase is an alternative to a merge. It changes the base of a branch, effectively adding it to the top of another branch as if the changes were sequential rather than created in parallel.\nThe rebase process looks at the first commit that is shared by two branches and uses the next commit as the base of the branch being rebased. It detaches this branch and re-attaches it to the head of the other branch. Under the hood, the commits do not actually move, new commits are created that are copies of the original commits on the branch. GIT moves the branch and the original commits become orphaned and eventually garbage collected.\n\u0026gt; git switch featureA # make featureA the current branch \u0026gt; \u0026gt; git rebase main # rebase the featureA branch onto main Why use Rebase? Rebase can help to simplify the history of a project. If there is a lot of merging it can complicate the history.\nHowver, use with caution. Rebased history is not the true history, so merging is safer.\nSquashing commits Squashing re-writes the GIT history, making two or more commits appear as if they were a single commit. Why would you want to do this? Developers often commit very frequently when working locally on a feature, but don\u0026rsquo;t want to complicate the shared history with all these individual commits. Squashing commits before merging or pushing to an origin repo simplifies the history in a large project.\nThe interactive mode of git rebase is used to squash commits. This is totally different to the basic use of rebase. A starting point commit must be specified as we don\u0026rsquo;t normally want to edit the entire history. The starting point is excluded from the list and the interactive mode starts from the next commit.\n\u0026gt; git rebase --interactive 80f137 pick fd4d8d9 Updates Adsense css pick 8d41123 Adds adsense css pick 9b2aafa Adds article pick 32735fb Fixes error in post pick 7f0063a Fixes typo pick c85a17f Adds article # Rebase 55a0831..c85a17f onto 55a0831 (6 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit\u0026#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # An interactive editor opens as shown above. The commits are listed (in reverse order compared to most git commands). The editor gives the commands - such as pick, reword etc.\nBy default all commits are on a line that starts with \u0026ldquo;pick\u0026rdquo;. Editing these lines will modify the history.\nFor example, you can move entire lines to change the order of commits. Changing \u0026ldquo;pick\u0026rdquo; to \u0026ldquo;squash\u0026rdquo; will cause the commit to be merged with the one above and GIT will prompt to select or edit one of the two commit messages.\nThe golden rule of Rebase Rebasing can lead to problems when sharing a repo across a team. The purpose of the rebase is to simplify history, but it can lead to duplication in the GIT multi-master sharing model. The golden rule is therefore:\nNever rebase shared commits. Only use rebase for commits that have not yet been shared.\nAmmending a commit If you want to update the latest commit to add additional files, add them to the index then use the --ammend option on git commit. This will create a new commit with the additional files and leave the previous commit as an orphan that will eventually get garbage collected. You can only use this option to update the latest commit.\n\u0026gt; git commit --ammend TAGS Tags are labels for a commit. Tags are normally used to mark releases.\n❯ git tag version1_0 -a -m \u0026#34;First version. Basic features\u0026#34; # Create a new annotated tag ❯ ❯ git tag version2_0 # Create a lightweight tag (not annotated) ❯ ❯ git tag # get a list of tags Tags are rederences to a commit, but unlike branches, tags never move.\nCLONE git clone is used to copy an existing repository into an empty local folder. The copy contains the working files and the full GIT history.\nThe existing repo can be local or remote. It can be referenced by an SSH or HTTP address. By default, it will clone the branch HEAD is pointing to, but this can be modified using the -branch option.\nThe command is mostly used to clone a repo from a hosting service such as Github - so it can be edited locally and then pushed back up to Github.\n❯ git clone https://github.com/myaccount/myrepo # clone a repo from Github into the current local folder The source repo is registered as a \u0026ldquo;remote\u0026rdquo; called origin by default. git status will show if the local branch is ahead or behind the remote branch.\nThe information about the remote repo is stored in the /git/config file. The remote branches are tracked by objects in the .git/refs/remotes folder.\n❯ git show-ref main # show all branches that have main in the name and the commit they are pointing to 8195805D refs/heads/main B05DB506 refs/remotes/origin/main If the local repo is in-sync with the remote, they will be pointing to the same commit, i.e. the hashes above would be the same.\nThe git clone -bare option will clone the history but not the working area. It will also not setup the original source as a remote. This can be used to create a central repo that is just a source for cloning and not worked on directly.\nPUSH / FETCH / PULL git push is used to send local changes to the remote origin repo. But what happens if there have been other changes on the remote before we push and we now have a conflict?\nThe answer is to fetch the remote changes and resolve the conflict locally before pushing. There is a comand to get the latest changes from the remote - git fetch, but rather than fetching and then merging in two steps, the git pull command peforms a fetch and merge in one command.\nWhen working with a remote, you should always pull before pushing.\n❯ git pull origin This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/27/usinggit/","summary":"\u003cp\u003eThis article focuses on common GIT actions that affect branches, such as merging, rebasing branches and squashing local commits.\u003c/p\u003e","title":"GIT branches - merging, rebasing and squashing commits"},{"content":"GIT commands move data between four areas of the object database. This article explores the four areas and the common commands that affect them.\nSee the earlier post on How GIT stores information for information on the GIT object database.\nWhat are the four areas GIT uses? The Working Area stores project source files updated directly by a code editor The Index (or Staging Areas) tracks which files from the working area included in the next commit The Repository (or Repo) is the most important area, where GIT stores snapshots of the tracked files, called commits The Stash is a temporary area, a bit like a clipboard, that can store and retrieve a saved copy of the working area and index Understanding git requires understanding how commands move data between these four areas.\nGIT STATUS git status is one of the most important commands. It shows the status of the working area and index. The most common output shows untracked files and modified files.\n❯ git status On branch master nothing to commit, working tree clean New files When a file is created or copied into the project, it only exists in the working area. git status will show the file as untracked. It has not been added to the index (staging) area and will not be part of the next commit until added. The status command shows the list of new files and suggests how to add them to the index.\n❯ git status On branch master Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) consolidate.py nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) Modified files Files added to the index are tracked by GIT. When a tracked file is updated or modified, git status shows the it as modified and not yet staged for commit. The working area version has changed, but GIT is only tracking the previous version in the index. The updated file needs to be added to the index to be included in the next commit. The status command shows the modified files and suggests actions to take.\n❯ git status On branch master Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: consolidate.py no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Changes not committed git status also shows changes in the index but not yet committed to the repo. These could be new and/or modified files.\n❯ git status On branch master Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: consolidate.py Remote A remote is a linked repository, for example, a central source repo that the local repo was originally cloned from. GIT maintains the link to the remote and tracks differences. By default, the remote is named origin. git status will show if the remote repo is ahead or behind the current local commit.\n❯ git status On branch master Your branch is ahead of \u0026#39;origin/master\u0026#39; by 1 commit. (use \u0026#34;git push\u0026#34; to publish your local commits) nothing to commit, working tree clean GIT ADD The git add command updates the index (staging) area to track new files or add new versions of already tracked files. The most common parameters are as follows:\n❯ git add mynewfile.txt # stage a specific file in the index ❯ git add folder1/ # stage all new and modified files in folder1 ❯ git add -A # stage all new and modified files in the entire project ❯ git add . # same as above ❯ git add -i # interactive mode. Prompts for a decision on a file-by-file basis GIT DIFF git diff shows differences between the GIT areas, such as between the working area and index, or between the index and repo. Differences are displayed at the command line. git diff is convenient for quick checks and small changes. Graphical tools may be better suited for complex comparisons.\n❯ git diff # shows differences between the working area and index ❯ git diff --cached # shows differences between the index and repo diff --git a/consolidate.py b/consolidate.py index de10111..97f66ce 100644 --- a/consolidate.py +++ b/consolidate.py @@ -1 +1,2 @@ import sys +print (sys.version) In the example output above, there is a difference between the index and the repo. A line has been added in the index version, indicated by the \u0026ldquo;+\u0026rdquo; symbol.\nGIT COMMIT The git commit command creates a point-in-time snapshot of tracked files in the repo. New and modified files from the index are added to the repo as new objects. Unchanged files are just referenced by a link to the existing object in the repo. The commit itself is an object in the repository, pointing to a tree and any parent commit.\nA commit is named using a SHA1 hash. It can be referenced in commands using just the first few characters of the hash (enough that it is not ambiguous).\n❯ git commit -m \u0026#34;Enabled handling of user input\u0026#34; [main b9b6064] Enabled handling of user input 1 file changed, 1 insertion(+), 1 deletion(-) In the output above, a new commit is created on the main branch with partial SHA1 hash b9b6064\nGIT BRANCH A branch is just a reference to a commit. Creating a branch does not actually change objects in the repo, it just creates a named pointer to an existing commit. If you are on the main branch and create a new branch called dev, GIT adds a new dev object in the .git/refs folder pointing to the current commit. Initially both branches are pointing to the same commit.\nThe git branch command shows the current branches or creates a new branch, but does not switch to it. The asterisk in the output below shows that after creating a new dev branch, main is still the current branch:\n❯ git branch dev # create a new branch. ❯ ❯ git branch # show branches dev * main GIT SWITCH git switch is a relatively new command that switches between branches. It does not make changes to the repo, but it does affect the working area and index. When you switch to another branch, the GIT HEAD reference is updated to point to the selected branch. The branch refers to a commit and the files and folders in the working area and index are replaced by the files in this commit.\n❯ git switch dev Switched to branch \u0026#39;dev\u0026#39; ❯ ❯ git branch * dev main You can create a branch and switch to it with the -c option\n❯ git switch -c feature Switched to a new branch \u0026#39;feature\u0026#39; GIT CHECKOUT When it comes to moving between branches, git checkout is almost identical to git switch. The checkout command has been around for much longer and has options that perform other actions. The variety of uses for the checkout command was deemed confusing and switch was introduced to focus purely on moving between branches.\n❯ git checkout dev # change to the dev branch Switched to branch \u0026#39;dev\u0026#39; ❯ ❯ git branch # show \u0026#39;dev\u0026#39; is now the current branch * dev feature main ❯ ❯ git checkout -b feature2 # create a new branch and switch to it Switched to a new branch \u0026#39;feature2\u0026#39; ❯ ❯ git branch # show \u0026#39;feature2\u0026#39; is not the current branch dev feature * feature2 main GIT LOG git log shows the history of commits and branches. It does not make any changes to the repo, index or working area. Use the \u0026ndash;graph option to see a basic diagram of branches and merges:\n❯ git log --oneline --graph --decorate * a7d531d (HEAD -\u0026gt; main) Merge Feature2, resolve conflict in orders.py |\\ | * d0773eb (feature2) Adds input validation to orders.py * | bba3001 Modifies output in orders.py |/ * 460ce0e Enables user input * b9b6064 Creates orders.py GIT RESET git reset is used to rollback to a previous commit. It can affect the repo, the index and the working area depending on the parameters.\nA common use case is to abandon some changes that have been committed and revert to a previous version of the project. Another is to abandon changes in the working area and revert to the last commit in the repo.\nThe reset command will move the current branch to the specified commit. By default, it will also overwrite the index with the contents of the commit. If you want to completely remove all traces of the unwanted change, you can also overwrite the working area using the \u0026ndash;hard option\n\u0026gt; git log --oneline # show the history of commits 848eae0 (HEAD -\u0026gt; main) Adds search to 404 page 364b33f Changes format of 404 page d4ff0dc Modifies position of search box c55dbb1 Enables comments cccb986 Adds home page, header and footer 4478398 Initial commit \u0026gt; \u0026gt; git reset --hard d4ff0dc # Rollback the repo two commits, overwriting the index and working area HEAD is now at d4ff0dc Modifies position of search box Other options are:\n\u0026gt; git reset --mixed d4ff0dc # Rollback the repo two commits but only overwrite the index. The working area is not touched. This is also the default if no options are specified. \u0026gt; git reset --soft d4ff0dc # Rollback the repo two commits, but don\u0026#39;t touch the index or working area HEAD RESET git reset HEAD is a quick way to return the index/working area to the committed state of the repo. This is an unsual reset as it doesn\u0026rsquo;t actually change anything in the repo.\nHEAD is usually poiting to the latest commit on the current branch. Resetting the repo to HEAD changes nothing in the repo, but by default it will perform a mixed reset, overwriting the index. We can also use the \u0026ndash;hard option to overwrite both the index and working area:\n\u0026gt; git reset --hard HEAD GIT STASH The Stash is a temporary storage area to save the working area and index when you need to working on another feature. You don\u0026rsquo;t want to lose or commit what you are currently working on, so you stash the changes, work on something else and then retrieve them later. Its a bit like saving something in a clipboard.\nWhen you run git stash, GIT copies tracked files in the working area and index that aren\u0026rsquo;t in the current commit and saves them to the stash. It then checks out the current commit (overwriting the working area and index so they match the repo).\nWhen ready to restore the stashed files, run git stash apply.\n\u0026gt; git stash --include-untracked # Save changes to the stash including untracked files \u0026gt; \u0026gt; git stash list # List the contents of the stash. This is an array of saves starting with zero \u0026gt; \u0026gt; git stash apply # Copies the latest entry in the stash to the working area and index. You can also specify an array element number if you don\u0026#39;t want the latest This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/24/gitandthefourareas/","summary":"\u003cp\u003eGIT commands move data between four areas of the object database. This article explores the four areas and the common commands that affect them.\u003c/p\u003e\n\u003cp\u003eSee the earlier post on \u003ca href=\"/2021/06/17/git-cheatnotes/\"\u003eHow GIT stores information\u003c/a\u003e for information on the GIT object database.\u003c/p\u003e","title":"GIT storage - understanding the stash, working, index, repo"},{"content":"GIT has become the de-facto version control system, but it can get complicated quickly. A look under-the-hood can help with day-to-day use and file recovery. This article explores the files in the object database laying a foundation for more advanced use.\nHOW DOES GIT STORE OBJECTS? GIT stores information in the hidden .git folder in the root of the project. The folder is created when a repository is initialized using git init\nCommits, trees and blobs are the fundamental objects in GIT\nThey are stored in the .git/objects folder:\nCommits are a point in time reference to a tree Trees represent folders Blobs represent files* *Blobs can also represent \u0026ldquo;hunks\u0026rdquo; (chunks of a file), but thats a more advanced topic for another article\nWHAT\u0026rsquo;S WITH ALL THE SHA1 HASHES? GIT creates a SHA1 checksum for each object and stores them in files under the.git/objects folder. The files are named after the SHA1 hash which means objects in the database are immutable - they cannot change. Modified files always result in new objects with a new hash, rather than updating the existing.\nGIT uses the hash values to determine which files have been modfied during a commit. New and modified files are added as new blobs. Unchanged files are just referenced, keeping the existing blob.\nTo avoid storing everything in one folder, git creates subfolders under .git/objects. The subfolder folder names are the first two characters of the SHA1 hash and the objects are grouped in these subfolders. The filename in the subfolder is the remaining characters of the hash.\nFor example:\n❯ ls .git/objects 00 00/24a57c6cee77755693e0514f244b1cfa5e645d 00/5b63f2cf1d596fa3f88834b98272a9d1bf9fc3 00/f823e0b5420e1051c80e0b37922409125e9156 01 01/28a8cb2ac88861ec18599c0b05f9481bdd3600 01/8c65d03d8269df96a7da4c3de1a62cd1d1c0ab 02 02/188f346460de1876df7dac2669360396f84a58 In the above example, there are three subfolders under .git/objects, called \u0026ldquo;00\u0026rdquo;, \u0026ldquo;01\u0026rdquo; and \u0026ldquo;02\u0026rdquo;. The full SHA1 hash of an object is constructed by adding the parent folder name to the filename.\nSo the final file listed above has the full hash of 02188f346460de1876df7dac2669360396f84a58\nCAN YOU LOOK INSIDE THE OBJECTS? The objects inside the git database are compressed, but can be viewed with the command git cat-file\nSpecify the object hash and either:\n-t = show the object type\n-p = print the contents\nNOTE: You only need part of the hash when using most GIT commands (and GIT sometimes truncates the hash in its own output)\nWHAT TYPE OF OBJECT IS THIS? ❯ git cat-file 0b4271c56 -t # display the object type commit The above object is a commit.\nWHAT\u0026rsquo;S INSIDE A COMMIT OBJECT? ❯ git cat-file 0b4271c56 -p tree 30b4d42bbe1a39dcc314f7c280b1437a1925585e parent cc0e10d238e78a57115572360a93deba2554d185 author GD \u0026lt;GD@LOCAL.HOME\u0026gt; 1621179226 +0100 committer GD \u0026lt;GD@LOCAL.HOME\u0026gt; 1621179226 +0100 Updated summaries. Added article In the above output:\ntree is a hash reference to the root tree (folder). parent is the hash of the parent commit (unless this is the first commit) Author and committer are the operator who created the commit Finally there is the commit message, Updated summaries\u0026hellip; WHAT\u0026rsquo;S IN A TREE OBJECT? We can view the contents of a tree object in the same way e.g. using the hash of the tree in the commit above:\n❯ git cat-file -p 30b4d42bbe 100644 blob d298be107f27247a24d24f8f78c55d42359007be .gitignore 100644 blob e3720ce5ced245ef02620afca619727c001e85bf 404.html 100644 blob 82b909c8a3de119782d6b66288734f82a4a57d1b about.md 040000 tree 272bc4b082fa15dd84b08712206d2edfe2b41e9a archetypes 040000 tree e305983083fc1872542004d046abdf3a683407e1 config 040000 tree 955f968be02f980640e570874f4c155da51882d4 content The first three items in the output are references to blobs (files) in the root of the tree (e.g. the .gitignore file). The rest are references to child trees, which can be explored further using the cat-file command.\nWHAT\u0026rsquo;S IN A BLOB? ❯ git cat-file -t d298be107 # get the object type blob ❯ ❯ git cat-file -p d298be107 # get the object contents public/ The blob finally contains the actual content, rather than a reference.\nIn this case, it is the .gitignore file that contains a single line to exclude public from the repo\nWHAT ABOUT BRANCHES? Branches are very simple in GIT. They are just references to a commit.\nBranch objects aren\u0026rsquo;t compressed so we can look at the contents of the file directly (without needing git cat-file).\nLocal branches are stored in the .git/refs/heads folder:\n❯ cat .git/refs/heads/main # show the contents of the main file 0b4271c561e6c7ad5dcf788afdc29bebbf11e171 This output is what we expected, the contents of the main branch are a reference to a commit using the SHA1 hash.\nIf we explore the branch using git cat-file, it gives us information about the commit the branch is pointing to:\n❯ git cat-file -t main # show the object type commit ❯ ❯ git cat-file -p main # show the object contents tree 30b4d42bbe1a39dcc314f7c280b1437a1925585e parent cc0e10d238e78a57115572360a93deba2554d185 author gbdixg \u0026lt;user@domain.HOME\u0026gt; 1621179226 +0100 committer gbdixg \u0026lt;user@domain.HOME\u0026gt; 1621179226 +0100 Updated summaries. Added article This is identical to the contents of the commit we looked at earlier, because that was the latest commit on the main branch.\nWHAT\u0026rsquo;S THE HEAD? Head is a special pointer in GIT. It is a reference to the commit that is currently checked-out. Usually the latest commit on the current branch, but not always.\nThe contents of HEAD is not a hash. It contains a pointer to the name of a branch or commit.\n❯ cat .git/HEAD # show contents of the HEAD file ref: refs/heads/main SUMMARY The git object database is all about references.\nHEAD is a reference to the current commit A branch is a reference to a commit A commit is a reference to a tree A tree is a reference to blobs and child trees A blob is the actual content This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/06/17/git-cheatnotes/","summary":"\u003cp\u003eGIT has become the de-facto version control system, but it can get complicated quickly. A look under-the-hood can help with day-to-day use and file recovery. This article explores the files in the object database laying a foundation for more advanced use.\u003c/p\u003e","title":"GIT basics - under-the-hood"},{"content":"Windows Subsystem for Linux (WSL) is a fantastic Dev and Test environment, providing seamless integration for Linux apps and shells running on Windows 10. This post is a quick summary of the manual steps to enable WSL2 in Windows 10.\nWindows Insider Builds At the time of writing, Microsoft is making it much simpler to install and enable WSL2 using a single command. This option is only available in Insider Builds 20262 and higher.\nwsl --install For the stable release versions of Windows 10, the following manual steps are required.\nEnable virtualization support This is separate to the Hyper-V optional feature, but does use the same architecture.\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Enable WSL feature dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart WSL kernel update package This is required because Microsoft removed the previously included Linux kernel from Windows. It now gets updated and patched through Windows Update\nDownload and install the update package\nSet the default version Make sure everything is version 2\nwsl --set-default-version 2 Install the required distro Ubuntu is the most reliable on Windows at the time of writing\nUbuntu 20.04 LTS\nCreate username Start the distro from the Start Menu shortcut. When prompted, specify a username and password.\nThe user is automatically added to the Sudo group.\nUpdate packages in the distro Launch Ubuntu and start a terminal shell\nsudo apt-get update sudo apt-get upgrade Check you are running version 2 Windows 10 PowerShell:\nwsl --list -v NAME STATE VERSION * docker-desktop-data Running 2 Ubuntu-20.04 Running 2 docker-desktop Running 2 Location of the .vhdx file The virtual disk containing the Linux OS is located here:\n%LOCALAPPDATA%\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\\ext4.vhdx\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/05/15/wsl2-quick/","summary":"\u003cp\u003eWindows Subsystem for Linux (WSL) is a fantastic Dev and Test environment, providing seamless integration for Linux apps and shells running on Windows 10. This post is a quick summary of the manual steps to enable WSL2 in Windows 10.\u003c/p\u003e","title":"WSL2 on Windows 10 - quick setup"},{"content":"BACKGROUND VSCode snippets are a productivity feature allowing blocks of code to be inserted with a couple of keystrokes or tab completion. Its simple to add your own Snippets and dramatically boost your productivity. Read on for the details.\nAll paths and keyboard shortcuts in this article assume VSCode is running on Windows\nPredefined Snippets are included with most of the VSCode language extensions (use @category:\u0026ldquo;snippets\u0026rdquo; in the extensions pane to see which ones). However, they may not match your coding style, or have trigger text that suits you. User-defined snippets allow complete customisation. They also support variables that are replaced with required values on insert, making them perfect for boilerplate code.\nEDITING USER-DEFINED SNIPPETS Open a language-specific Snippet file as follows (example using PowerShell):\nFile \u0026gt; Preferences \u0026gt; User Snippets \u0026gt; PowerShell\nA .json file is displayed, empty at first.\nA Snippet is made up of the following elements\nElement Example Description name \u0026ldquo;Function template\u0026rdquo; The name is shown by Intellisense if there is no description prefix [\u0026ldquo;ft\u0026rdquo;,\u0026ldquo;function\u0026rdquo;] One or more trigger words that activate intellisense (uses substring matching) body [\u0026ldquo;function Verb-Noun {\\r\u0026rdquo;,\u0026quot;[cmdletbinding()]\\r\u0026quot;] The template code to be inserted description Advanced function boilerplate Optional description displayed by intellisense placeholder ${1:Verb-Noun} An element within the body that is replaced by the user after insertion. The number represents the tab stop position. The text is the default value that is replaced choices ${1|one,two,three|} This placeholder will prompt to choose one of the options between the pipe characters $0 [\u0026ldquo;while($i -lt 10){\\r\u0026rdquo;,\u0026quot;\\t$0\\r\u0026quot;,\u0026quot;}\u0026quot;] A special placeholder that always comes last and ends insertion mode Snippet example \u0026#34;Advanced function\u0026#34;: { \u0026#34;prefix\u0026#34;: [\u0026#34;fa\u0026#34;,\u0026#34;function\u0026#34;] \u0026#34;body\u0026#34;: [ \u0026#34;Function ${1:Verb-Noun}{\\r\u0026#34;, \u0026#34;[cmdletBinding()]\\r\u0026#34;, \u0026#34;param(\\r\u0026#34;, \u0026#34; \\r\u0026#34;, \u0026#34;)\\r\u0026#34;, \u0026#34;BEGIN{\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;PROCESS{\\r\u0026#34;, \u0026#34;$0\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;END{\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\\r\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;}\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;Advanced function boilerplate\u0026#34; Note the use of a JSON array for the body and control characters for new lines. This is quite laborious to create by hand, but VSCode extensions can make this much easier\u0026hellip;\nMarketplace Snippet Extension There are a number of extensions in the VSCode marketplace that will create a Snippet from highlighted code in the editor.\nFor example, Snippet Creator will automatically detetect the in-use language and then prompt for the Snippet prefix and description. You can then edit the Snippet to fine-tune it.\nSnippet Scope Language-specific\nMost Snippets will be created in a language-specific Snippet file and will only prompt for insertion when using that language e.g.\n%APPDATA%\\Code\\User\\snippetsPowershell.json\nGlobal\nThere is also a global Snippets file that applies to all languages. This file does not exist by default but can be created from File \u0026gt; Preferences \u0026gt; User Snippets \u0026gt; New Global Snippets file. The file can have any name, but always ends in .code-snippets. For example:\n%APPDATA%\\Code\\User\\snippets\\GlobalSnippets.code-snippets\nThe global Snippets can use an additional property called Scope to limit them to a list of languages. If it isn\u0026rsquo;t specified, they are available to all.\nProject-specific\nIf a global Snippets file is placed in the .vscode folder at the root of a project, it is scoped only to that project. It can still use the scope property to further limit Snippets to specific languages.\nKeyboard Shortcut Use File \u0026gt; Preferences \u0026gt; Keyboard Shortcuts \u0026gt; Open Keyboard Shortcuts (JSON) to assign a shortcut to a Snippet. Custom shortcuts are saved in the file %AppData%\\Code\\User\\keybindings.json\nIf the Snippet is not in the Global Snippets file, the langId is used to specify a language specific Snippet:\nKeybinding example { \u0026#34;key\u0026#34;: \u0026#34;cmd+k 1\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;editor.action.insertSnippet\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;langId\u0026#34;: \u0026#34;csharp\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;NewClass\u0026#34; } } Hiding Snippets Hiding Snippets is useful when there is a lot of noise in the Intellisense prompts. This can occur when you create a user snippet with the same trigger as a language extension snippet.\nOpen the insert Snippet dialog using CTRL + ALT + J Start typing the tigger characters to show the Snippet options in the list Hover over each item and click the Hide from Intellisense option on the right hand side Extension Snippets I don\u0026rsquo;t recommend trying to edit or remove extension Snippets. Changes are likely to get overwritten when the extension updates.\nFor information, extension Snippets are stored under %USERPROFILE%\\.vscode\\extensions. For example the Microsoft PowerShell extension Snippets are at:\n%USERPROFILE%\\.vscode\\extensions\\ms-vscode.powershell-2021.2.2\\snippets\\PowerShell.json\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2021/05/07/vscodesnippets/","summary":"\u003ch2 id=\"background\"\u003eBACKGROUND\u003c/h2\u003e\n\u003cp\u003eVSCode snippets are a productivity feature allowing blocks of code to be inserted with a couple of keystrokes or tab completion. Its simple to add your own Snippets and dramatically boost your productivity. Read on for the details.\u003c/p\u003e","title":"Create custom code snippets in VSCode"},{"content":"Read-on for a PowerShell command to get the Active Directory Subnet and Site from the computername or IP Address.\nActive Directory Sites represent locations with good network connectivity. An ADSite is often created for each office or a group of offices in a metropolitan area, to generate the replication topology between Domain Controllers, and to help workstations/servers locate closest services.\nActive Directory Subnets define the IP ranges included in an AD Site. A member workstation/server will have an IP address that should fall within a defined AD subnet, making it part of an AD Site.\nAD Site membership is not fixed. Laptops can move between Sites and Subnets when they roam to another location.\nAn incorrect or undefined AD subnet can lead to slow logon times and slow access to DFS shares. A domain member would use any server that responds when it isn\u0026rsquo;t in a defined subnet.\nPowerShell script The Find-ADSite PowerShell function below will return the AD Site and Subnet for a specified computer name or IP Address.\nIf a company populates subnet descriptions with useful information, it can also identify information such as the specific Office or floor.\nAD Administrators may define a catch-all subnet with a wide address range. By default if the IP address is within more than one subnet, the output will only include the smallest range. Use the -AllMatches parameter to see everything.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/04/13/find-adsite/","summary":"\u003cp\u003eRead-on for a PowerShell command to get the Active Directory Subnet and Site from the computername or IP Address.\u003c/p\u003e","title":"Find a computer's Active Directory Site and Subnet with PowerShell"},{"content":"This article includes a PowerShell Export-Eventlog command to quickly export Windows event logs from a remote computer and copy it to the local machine.\nEvent logs are a cornerstone of troubleshooting, but getting access to them can be difficult across a network.\nIt can be faster to export a Windows event log on a remote computer, copy the .evtx file over the network and then query it locally.\nThe PowerShell Get-Winevent command can work against remote event logs, but it can be painfully slow over the network. Copying an entire exported log (.evtx file) across the same connection is much faster. Get-Winevent can still be used with the -path parameter to query the locally copied .evtx file.\nPowerShell Script wevtutil.exe is Windows .exe that can export event logs. The PowerShell function below uses wevtutil to export one ore more event logs and copy them locally. The computer name is pre-pended to the exported log name.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/04/06/export-eventlog/","summary":"\u003cp\u003eThis article includes a PowerShell \u003cem\u003eExport-Eventlog\u003c/em\u003e command to quickly export Windows event logs from a remote computer and copy it to the local machine.\u003c/p\u003e","title":"Export Remote Eventlog with PowerShell"},{"content":" ADSystemInfo is a built-in COM object in Windows that simplifies lookup of Active Directory user and computer information.\nADSystemInfo can only return information about the local computer and current user. The computer must be joined to a domain and a domain controller must be reachable when the function is called.\nIts simple to instantiate COM objects in PowerShell. The function below shows how to use this object.\nEXAMPLE OUTPUT POWERSHELL SCRIPT Function Get-ADSystemInfo{ \u0026lt;# .Synopsis Used to lookup specific AD user/computer object properties of the current session .Description Uses \u0026#34;ADSystemInfo\u0026#34; COM object to get Active Directory attributes for the current user and computer .Example PS C:\\\u0026gt;Get-ADSystemInfo ComputerDN : CN=EGBLHCNU335BQCG,OU=GBR,OU=Workstations,OU=EU,OU=Regions,DC=mycompany,DC=com SiteName : EULON DomainDNSName : mycompany.com DomainShortName : MYCOMPANY ForestDNSName : mycompany.com IsNativeMode : True PDCRoleOwner : CN=527616-NAADCP01,CN=Servers,CN=Global,CN=Sites,CN=Configuration,DC=mycompany,DC=com SchemaRoleOwner : CN=527616-NAADCP01,CN=Servers,CN=Global,CN=Sites,CN=Configuration,DC=mycompany,DC=com UserDN : CN=gbdixg,OU=Users,OU=GBR,OU=Accounts,OU=EU,OU=Regions,DC=mycompany,DC=com .Notes Version: 1.0 .Link http://msdn.microsoft.com/en-us/library/aa705962(VS.85).aspx #\u0026gt; [CmdletBinding()] Param() Process{ $Output = New-Object -TypeName PSObject | Select ComputerDN,SiteName,DomainDNSName,DomainShortName,ForestDNSName,IsNativeMode,PDCRoleOwner,SchemaRoleOwner,UserDN $obj = new-object -com ADSystemInfo $type = $obj.gettype() $Output.ComputerDN = $type.InvokeMember(\u0026#34;ComputerName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.SiteName = $type.InvokeMember(\u0026#34;sitename\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.DomainDNSName = $type.InvokeMember(\u0026#34;DomainDNSName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.DomainShortName = $type.InvokeMember(\u0026#34;DomainShortName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.ForestDNSName = $type.InvokeMember(\u0026#34;ForestDNSName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.IsNativeMode = $type.InvokeMember(\u0026#34;IsNativeMode\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output.PDCRoleOwner = ($type.InvokeMember(\u0026#34;PDCRoleOwner\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) -replace \u0026#34;CN=NTDS Settings,\u0026#34;,\u0026#34;\u0026#34;) $Output.SchemaRoleOwner = ($type.InvokeMember(\u0026#34;SchemaRoleOwner\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) -replace \u0026#34;CN=NTDS Settings,\u0026#34;,\u0026#34;\u0026#34;) $Output.UserDN = $type.InvokeMember(\u0026#34;UserName\u0026#34;,\u0026#34;GetProperty\u0026#34;,$null,$obj,$null) $Output } } This article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/03/30/adsysteminfo/","summary":"\u003cblockquote\u003e\n\u003cp\u003eADSystemInfo is a built-in COM object in Windows that simplifies lookup of Active Directory user and computer information.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Active Directory ADSystemInfo with PowerShell"},{"content":"This post includes a Get-WLAN function to show information about wireless LAN connections, including the SSID and signal strength. It also demonstrates creating a PowerShell wrapper for a built-in Windows command.\nWhy create a PowerShell exe wrapper PowerShell Tools are re-usable functions that can be used stand-alone or in a pipeline\nSometimes its more convenient to create a wrapper script using the output of a command line tool than try to create the function entirely in PowerShell. The example below creates a PowerShell command to get information about WI-Fi connections on the local computer.\nUsing Regex to parse text output Regular expressions are the ideal way to convert text output from a command line tool into PowerShell objects, making a re-usable pipeline tool. Regex is very powerful, but also intimidating. The solution below uses a handy shortcut to identify boundaries in the output - the not operator - ^.\nFor example, [^:]+ means match one or more characters that are not a colon. In the example below, this is used to split the text on each line in the command output.\nThe netsh output below needs to be split into key value pairs (e.g. SSID = MyWifi) and converted to a PSObject. For each line of output, the colon character is the obvious boundary between the key name and the value.\nNative Command Output C:\\\u0026gt; netsh wlan show interfaces There is 1 interface on the system: Name : Wi-Fi Description : Intel(r) Dual Band Wireless-AC 8260 GUID : 42bce393-237c-4bd4-9d5e-18020ba8bb87 Physical address : b7:8a:60:a5:f7:d8 State : connected SSID : MyWiFi BSSID : 30:d4:2e:50:de:7f Network type : Infrastructure Radio type : 802.11n Authentication : WPA2-Personal Cipher : CCMP Connection mode : Profile Channel : 6 Receive rate (Mbps) : 115.6 Transmit rate (Mbps) : 115.6 Signal : 97% Profile : MyWiFi Hosted network status : Not available The regex explained The PowerShell snippet below shows the regular expression and how the matches are added to a hash table collection as name = value.\n$Properties = @{} $result = netsh wlan show interfaces $Result | ForEach-Object { if ($_ -match \u0026#39;^\\s+(?\u0026lt;name\u0026gt;[^:]+):\\s(?\u0026lt;value\u0026gt;.*)$\u0026#39;) { $name = $Matches[\u0026#39;name\u0026#39;].Trim() $val = $Matches[\u0026#39;value\u0026#39;].Trim() $Properties.Add($name, $val) } } The Foreach-Object loop above processes the NetSH command output line-by-line.\nEach line (the $_ variable) is tested for a match against the RegEx expression using the PowerShell -match operator.\nThe \u0026ldquo;not\u0026rdquo; operator [^:]+ captures all the characters until the colon and saves them in the named capture group \u0026ldquo;name\u0026rdquo; ?\u0026lt;name\u0026gt;. The match then expects a colon followed by a space. Finally, everything until the end of the line is saved to the named capture group \u0026ldquo;value\u0026rdquo; ?\u0026lt;value\u0026gt;.\nMATCHES(0) = the entire line MATCHES(\u0026rsquo;name\u0026rsquo;) = from the start of the line, match any character that is not a colon MATCHES(\u0026lsquo;value\u0026rsquo;) = match everything from colon [space] to the end of the line A complete Get-WLAN PowerShell function is provided below.\nPowerShell Script Regex Links For some practice with Regular Expressions, check out RegEx Golf or Regex Crosswords. There is even a Regular Expressions day.\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2019/01/06/powershell-toolmaking-regex/","summary":"\u003cp\u003eThis post includes a Get-WLAN function to show information about wireless LAN connections, including the SSID and signal strength. It also demonstrates creating a PowerShell wrapper for a built-in Windows command.\u003c/p\u003e","title":"Get-WLAN - PowerShell Toolmaking"},{"content":"This post includes a PowerShell Get-ChromeExtension script to list installed extensions on the local or remote computer.\nBrowser extensions are supposed to be curated and vetted, but there have been many examples of malware. If you don\u0026rsquo;t already have control of extensions through an allow or blocklist, the first step is to find out what is in-use.\nThe code below is a PowerShell function to get the installed Google Chrome browser extensions from a local or remote Windows computer.\nChrome Browser Extensions install into the user profile and do not appear in the Add/Remove Programs list.\nChrome Extensions are a challenge to audit due to the way they install and lack of enumeration options. The PowerShell script below gets the installed extensions using the following method:\nGet the extension IDs from the folders names under %userprofile%\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions Lookup the extension name on the Chrome Web Store using the extension ID Get the extension version from the manifest.json file in the extension folder Example script output C:\\\u0026gt; Get-ChromeExtension | Select Name,Version,Description | ft -AutoSize Name Version Description ---- ------- ----------- Docs 0.10 Create and edit documents Google Drive 14.1 Google Drive: create, share and keep all your stuff in one place. YouTube 4.2.8 The official YouTube website Sheets 1.2 Create and edit spreadsheets Google Docs Offline 1.4 Get things done offline with the Google Docs family of products. Google Wallet 1.0.0.4 Gmail 8.1 Fast, searchable email with less spam. Chrome Cast 6618.312.0.2 Slides 0.10 Create and edit presentations Docs 0.10 Create and edit documents Google Drive 14.2 Google Drive: create, share and keep all your stuff in one place. YouTube 4.2.8 The official YouTube website OneTab 1.18 Save up to 95% memory and reduce tab clutter uBlock Origin 1.20.0 Finally, an efficient blocker. Easy on CPU and memory. Dark Reader 4.7.12 Dark mode for every website. Take care of your eyes, use dark theme for night and daily browsing. Share link via email 3.2.1 Adds a button and context menu item to send the page URL or a link URL via email Sheets 1.2 Create and edit spreadsheets Google Docs Offline 1.7 Get things done offline with the Google Docs family of products. Pinterest Save Button 4.0.82 Save the things you find on the Web. Google Wallet 1.0.0.4 ColorPick Eyedropper 0.0.2.29 An eye-dropper \u0026amp;amp; color-picker tool that allows you to select color values from webpages. Gmail 8.2 Fast, searchable email with less spam. Chrome Cast 7519.422.0.3 PowerShell Script Get-ChromeExtension.ps1 script:\nThis article was originally posted on Write-Verbose.com\n","permalink":"https://write-verbose.com/2018/12/15/audit-google-chrome-extensions/","summary":"\u003cp\u003eThis post includes a PowerShell \u003cem\u003eGet-ChromeExtension\u003c/em\u003e script to list installed extensions on the local or remote computer.\u003c/p\u003e","title":"Audit Chrome Extensions with PowerShell"},{"content":"I live in the U.K. and work remotely on technology projects.\nI\u0026rsquo;ve been self-employed for many years, working as an I.T. consultant for Fortune 500 companies, mainly in the financial sector.\nDisclaimers: Views expressed are solely my own\nProduct reviews are based on real experience\nScripts and guides on this site should be tested in a non-production environment\nThere is no guarantee of accuracy\n","permalink":"https://write-verbose.com/about/","summary":"I live in the U.K. and work remotely on technology projects.\nI\u0026rsquo;ve been self-employed for many years, working as an I.T. consultant for Fortune 500 companies, mainly in the financial sector.\nDisclaimers: Views expressed are solely my own\nProduct reviews are based on real experience\nScripts and guides on this site should be tested in a non-production environment\nThere is no guarantee of accuracy","title":"WhoAmI"}]